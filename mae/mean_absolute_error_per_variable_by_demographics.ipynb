{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypots\n",
    "import os\n",
    "import sys\n",
    "from pypots.utils.metrics import calc_mae\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS, BRITS, MRNN, USGAN, GPVAE\n",
    "import numpy as np\n",
    "import benchpots\n",
    "from pypots.utils.random import set_random_seed\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 23:04:38 [INFO]: Have set the random seed as 2022 for numpy and pytorch.\n",
      "2025-02-10 23:04:38 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2025-02-10 23:04:38 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2025-02-10 23:04:38 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2025-02-10 23:04:38 [INFO]: Loaded successfully!\n",
      "2025-02-10 23:04:53 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-02-10 23:04:53 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-02-10 23:04:54 [INFO]: 68807 values masked out in the val set as ground truth, take 9.97% of the original observed values\n",
      "2025-02-10 23:04:54 [INFO]: 68807 values masked out in the val set as ground truth, take 9.97% of the original observed values\n",
      "2025-02-10 23:04:54 [INFO]: 86319 values masked out in the test set as ground truth, take 9.99% of the original observed values\n",
      "2025-02-10 23:04:54 [INFO]: 86319 values masked out in the test set as ground truth, take 9.99% of the original observed values\n",
      "2025-02-10 23:04:54 [INFO]: Total sample number: 11988\n",
      "2025-02-10 23:04:54 [INFO]: Total sample number: 11988\n",
      "2025-02-10 23:04:54 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-02-10 23:04:54 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-02-10 23:04:54 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-02-10 23:04:54 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-02-10 23:04:54 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-02-10 23:04:54 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-02-10 23:04:54 [INFO]: Number of steps: 48\n",
      "2025-02-10 23:04:54 [INFO]: Number of steps: 48\n",
      "2025-02-10 23:04:54 [INFO]: Number of features: 37\n",
      "2025-02-10 23:04:54 [INFO]: Number of features: 37\n",
      "2025-02-10 23:04:54 [INFO]: Train set missing rate: 79.70%\n",
      "2025-02-10 23:04:54 [INFO]: Train set missing rate: 79.70%\n",
      "2025-02-10 23:04:54 [INFO]: Validating set missing rate: 81.75%\n",
      "2025-02-10 23:04:54 [INFO]: Validating set missing rate: 81.75%\n",
      "2025-02-10 23:04:54 [INFO]: Test set missing rate: 81.75%\n",
      "2025-02-10 23:04:54 [INFO]: Test set missing rate: 81.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['n_classes', 'n_steps', 'n_features', 'scaler', 'train_X', 'train_y', 'train_ICUType', 'val_X', 'val_y', 'val_ICUType', 'test_X', 'test_y', 'test_ICUType', 'female_gender_test_X', 'female_gender_test_y', 'test_ICUType_female_gender', 'male_gender_test_X', 'male_gender_test_y', 'test_ICUType_male_gender', 'undefined_gender_test_X', 'undefined_gender_test_y', 'test_ICUType_undefined_gender', 'more_than_or_equal_to_65_test_X', 'more_than_or_equal_to_65_test_y', 'test_ICUType_more_than_or_equal_to_65', 'less_than_65_test_X', 'less_than_65_test_y', 'test_ICUType_less_than_65', 'ICUType_1_test_X', 'ICUType_1_test_y', 'test_ICUType_1', 'ICUType_2_test_X', 'ICUType_2_test_y', 'test_ICUType_2', 'ICUType_3_test_X', 'ICUType_3_test_y', 'test_ICUType_3', 'ICUType_4_test_X', 'ICUType_4_test_y', 'test_ICUType_4', 'classificacao_undefined_test_X', 'classificacao_undefined_test_y', 'test_ICUType_classificacao_undefined', 'classificacao_baixo_peso_test_X', 'classificacao_baixo_peso_test_y', 'test_ICUType_classificao_baixo_peso', 'classificacao_normal_peso_test_X', 'classificacao_normal_peso_test_y', 'test_ICUType_classificacao_normal_peso', 'classificacao_sobrepeso_test_X', 'classificacao_sobrepeso_test_y', 'test_ICUType_classificacao_sobrepeso', 'classificacao_obesidade_1_test_X', 'classificacao_obesidade_1_test_y', 'test_ICUType_classificacao_obesidade_1', 'classificacao_obesidade_2_test_X', 'classificacao_obesidade_2_test_y', 'test_ICUType_classificacao_obesidade_2', 'classificacao_obesidade_3_test_X', 'classificacao_obesidade_3_test_y', 'test_ICUType_classificacao_obesidade_3', 'val_X_ori', 'test_X_ori', 'female_gender_test_X_ori', 'male_gender_test_X_ori', 'undefined_gender_test_X_ori', 'more_than_or_equal_to_65_test_X_ori', 'less_than_65_test_X_ori', 'ICUType_1_test_X_ori', 'ICUType_2_test_X_ori', 'ICUType_3_test_X_ori', 'ICUType_4_test_X_ori', 'classificacao_undefined_test_X_ori', 'classificacao_baixo_peso_test_X_ori', 'classificacao_normal_peso_test_X_ori', 'classificacao_sobrepeso_test_X_ori', 'classificacao_obesidade_1_test_X_ori', 'classificacao_obesidade_2_test_X_ori', 'classificacao_obesidade_3_test_X_ori'])\n"
     ]
    }
   ],
   "source": [
    "set_random_seed()\n",
    "\n",
    "from pypotsModify.benchpotsMAE.datasets import preprocess_physionet2012\n",
    "\n",
    "# Load the PhysioNet-2012 dataset\n",
    "physionet2012_dataset = preprocess_physionet2012(subset=\"all\", rate=0.1)\n",
    "\n",
    "# Take a look at the generated PhysioNet-2012 dataset, you'll find that everything has been prepared for you,\n",
    "# data splitting, normalization, additional artificially-missing values for evaluation, etc.\n",
    "print(physionet2012_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble the datasets for training\n",
    "dataset_for_training = {\n",
    "    \"X\": physionet2012_dataset['train_X'],\n",
    "}\n",
    "# assemble the datasets for validation\n",
    "dataset_for_validating = {\n",
    "    \"X\": physionet2012_dataset['val_X'],\n",
    "    \"X_ori\": physionet2012_dataset['val_X_ori'],\n",
    "}\n",
    "\n",
    "dataset_for_testing_ori = {\n",
    "    \"X_ori\": physionet2012_dataset['test_X_ori'],\n",
    "    \"female_gender_test_X_ori\": physionet2012_dataset['female_gender_test_X_ori'],\n",
    "    \"male_gender_test_X_ori\": physionet2012_dataset['male_gender_test_X_ori'],\n",
    "    \"undefined_gender_test_X_ori\": physionet2012_dataset['undefined_gender_test_X_ori'],\n",
    "    \"more_than_or_equal_to_65_test_X_ori\":  physionet2012_dataset['more_than_or_equal_to_65_test_X_ori'],\n",
    "    \"less_than_65_test_X_ori\": physionet2012_dataset['less_than_65_test_X_ori'],\n",
    "    \"ICUType_1_test_X_ori\": physionet2012_dataset['ICUType_1_test_X_ori'],\n",
    "    \"ICUType_2_test_X_ori\": physionet2012_dataset['ICUType_2_test_X_ori'],\n",
    "    \"ICUType_3_test_X_ori\": physionet2012_dataset['ICUType_3_test_X_ori'],\n",
    "    \"ICUType_4_test_X_ori\": physionet2012_dataset['ICUType_4_test_X_ori'],\n",
    "    \"classificacao_undefined_test_X_ori\": physionet2012_dataset['classificacao_undefined_test_X_ori'],\n",
    "    \"classificacao_baixo_peso_test_X_ori\": physionet2012_dataset['classificacao_baixo_peso_test_X_ori'],\n",
    "    \"classificacao_normal_peso_test_X_ori\": physionet2012_dataset['classificacao_normal_peso_test_X_ori'],\n",
    "    \"classificacao_sobrepeso_test_X_ori\": physionet2012_dataset['classificacao_sobrepeso_test_X_ori'],\n",
    "    \"classificacao_obesidade_1_test_X_ori\": physionet2012_dataset['classificacao_obesidade_1_test_X_ori'],\n",
    "    \"classificacao_obesidade_2_test_X_ori\": physionet2012_dataset['classificacao_obesidade_2_test_X_ori'],\n",
    "    \"classificacao_obesidade_3_test_X_ori\": physionet2012_dataset['classificacao_obesidade_3_test_X_ori']\n",
    "}\n",
    "\n",
    "# assemble the datasets for test\n",
    "dataset_for_testing = {\n",
    "    \"X\": physionet2012_dataset['test_X'],\n",
    "    \"female_gender_test_X\": physionet2012_dataset['female_gender_test_X'],\n",
    "    \"male_gender_test_X\": physionet2012_dataset['male_gender_test_X'],\n",
    "    \"undefined_gender_test_X\": physionet2012_dataset['undefined_gender_test_X'],\n",
    "    \"more_than_or_equal_to_65_test_X\":  physionet2012_dataset['more_than_or_equal_to_65_test_X'],\n",
    "    \"less_than_65_test_X\": physionet2012_dataset['less_than_65_test_X'],\n",
    "    \"ICUType_1_test_X\": physionet2012_dataset['ICUType_1_test_X'],\n",
    "    \"ICUType_2_test_X\": physionet2012_dataset['ICUType_2_test_X'],\n",
    "    \"ICUType_3_test_X\": physionet2012_dataset['ICUType_3_test_X'],\n",
    "    \"ICUType_4_test_X\": physionet2012_dataset['ICUType_4_test_X'],\n",
    "    \"classificacao_undefined_test_X\": physionet2012_dataset['classificacao_undefined_test_X'],\n",
    "    \"classificacao_baixo_peso_test_X\": physionet2012_dataset['classificacao_baixo_peso_test_X'],\n",
    "    \"classificacao_normal_peso_test_X\": physionet2012_dataset['classificacao_normal_peso_test_X'],\n",
    "    \"classificacao_sobrepeso_test_X\": physionet2012_dataset['classificacao_sobrepeso_test_X'],\n",
    "    \"classificacao_obesidade_1_test_X\": physionet2012_dataset['classificacao_obesidade_1_test_X'],\n",
    "    \"classificacao_obesidade_2_test_X\": physionet2012_dataset['classificacao_obesidade_2_test_X'],\n",
    "    \"classificacao_obesidade_3_test_X\": physionet2012_dataset['classificacao_obesidade_3_test_X']\n",
    "}\n",
    "## calculate the mask to indicate the ground truth positions in test_X_ori, will be used by metric funcs to evaluate models\n",
    "test_X_indicating_mask = []\n",
    "test_X_ori = []\n",
    "for i, j in zip(dataset_for_testing_ori.values(), dataset_for_testing.values()):\n",
    "    test_X_indicating_mask.append(np.isnan(i) ^ np.isnan(j))\n",
    "    test_X_ori.append(np.nan_to_num(i))   # metric functions do not accpet input with NaNs, hence fill NaNs with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialize the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 23:15:52 [INFO]: No given device, using default device: cuda\n",
      "2025-02-10 23:15:52 [INFO]: Model files will be saved to tutorial_results/imputation/saits/20250210_T231552\n",
      "2025-02-10 23:15:52 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits/20250210_T231552/tensorboard\n",
      "2025-02-10 23:15:52 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 720,182\n"
     ]
    }
   ],
   "source": [
    "saits = SAITS(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/saits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 19:12:21 [INFO]: No given device, using default device: cuda\n",
      "2025-01-29 19:12:21 [INFO]: Model files will be saved to tutorial_results/imputation/brits/20250129_T191221\n",
      "2025-01-29 19:12:21 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/brits/20250129_T191221/tensorboard\n",
      "2025-01-29 19:12:21 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 255,344\n"
     ]
    }
   ],
   "source": [
    "brits = BRITS(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/brits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 23:06:16 [INFO]: No given device, using default device: cuda\n",
      "2025-02-10 23:06:16 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-10 23:06:16 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 1,258,517\n"
     ]
    }
   ],
   "source": [
    "us_gan = USGAN(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    rnn_hidden_size=256,\n",
    "    lambda_mse=1,\n",
    "    dropout=0.1,\n",
    "    G_steps=1,\n",
    "    D_steps=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 23:06:18 [INFO]: No given device, using default device: cuda\n",
      "2025-02-10 23:06:18 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-10 23:06:18 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 229,652\n"
     ]
    }
   ],
   "source": [
    "gp_vae = GPVAE(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    latent_size=37,\n",
    "    encoder_sizes=(128,128),\n",
    "    decoder_sizes=(256,256),\n",
    "    kernel=\"cauchy\",\n",
    "    beta=0.2,\n",
    "    M=1,\n",
    "    K=1,\n",
    "    sigma=1.005,\n",
    "    length_scale=7.0,\n",
    "    kernel_scales=1,\n",
    "    window_size=24,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 23:06:21 [INFO]: No given device, using default device: cuda\n",
      "2025-02-10 23:06:21 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-10 23:06:21 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 107,951\n"
     ]
    }
   ],
   "source": [
    "mrnn = MRNN(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 23:16:08 [INFO]: Epoch 001 - training loss: 0.7219, validation loss: 6.7640\n",
      "2025-02-10 23:16:13 [INFO]: Epoch 002 - training loss: 0.5377, validation loss: 6.7444\n",
      "2025-02-10 23:16:17 [INFO]: Epoch 003 - training loss: 0.4944, validation loss: 6.7122\n",
      "2025-02-10 23:16:22 [INFO]: Epoch 004 - training loss: 0.4644, validation loss: 6.6847\n",
      "2025-02-10 23:16:27 [INFO]: Epoch 005 - training loss: 0.4419, validation loss: 6.6802\n",
      "2025-02-10 23:16:32 [INFO]: Epoch 006 - training loss: 0.4235, validation loss: 6.6688\n",
      "2025-02-10 23:16:37 [INFO]: Epoch 007 - training loss: 0.4084, validation loss: 6.6592\n",
      "2025-02-10 23:16:42 [INFO]: Epoch 008 - training loss: 0.3992, validation loss: 6.6596\n",
      "2025-02-10 23:16:47 [INFO]: Epoch 009 - training loss: 0.3911, validation loss: 6.6530\n",
      "2025-02-10 23:16:52 [INFO]: Epoch 010 - training loss: 0.3833, validation loss: 6.6517\n",
      "2025-02-10 23:16:52 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-02-10 23:16:52 [INFO]: Saved the model to tutorial_results/imputation/saits/20250210_T231552/SAITS.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "saits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 19:13:29 [INFO]: Epoch 001 - training loss: 0.8504, validation loss: 0.2889\n",
      "2025-01-29 19:13:29 [INFO]: Saved the model to tutorial_results/imputation/brits/20250129_T191221/BRITS_epoch1_loss0.28890601024031637.pypots\n",
      "2025-01-29 19:14:17 [INFO]: Epoch 002 - training loss: 0.5925, validation loss: 0.2504\n",
      "2025-01-29 19:14:17 [INFO]: Saved the model to tutorial_results/imputation/brits/20250129_T191221/BRITS_epoch2_loss0.25044039537509283.pypots\n",
      "2025-01-29 19:15:06 [INFO]: Epoch 003 - training loss: 0.5444, validation loss: 0.2404\n",
      "2025-01-29 19:15:06 [INFO]: Saved the model to tutorial_results/imputation/brits/20250129_T191221/BRITS_epoch3_loss0.24043585633238157.pypots\n",
      "2025-01-29 19:15:55 [INFO]: Epoch 004 - training loss: 0.5230, validation loss: 0.2346\n",
      "2025-01-29 19:15:55 [INFO]: Saved the model to tutorial_results/imputation/brits/20250129_T191221/BRITS_epoch4_loss0.23458528518676758.pypots\n",
      "2025-01-29 19:16:43 [INFO]: Epoch 005 - training loss: 0.5095, validation loss: 0.2312\n",
      "2025-01-29 19:16:43 [INFO]: Saved the model to tutorial_results/imputation/brits/20250129_T191221/BRITS_epoch5_loss0.231178251405557.pypots\n",
      "2025-01-29 19:17:32 [INFO]: Epoch 006 - training loss: 0.4983, validation loss: 0.2307\n",
      "2025-01-29 19:17:32 [INFO]: Saved the model to tutorial_results/imputation/brits/20250129_T191221/BRITS_epoch6_loss0.23071187660098075.pypots\n",
      "2025-01-29 19:18:21 [INFO]: Epoch 007 - training loss: 0.4895, validation loss: 0.2310\n",
      "2025-01-29 19:19:10 [INFO]: Epoch 008 - training loss: 0.4830, validation loss: 0.2328\n",
      "2025-01-29 19:19:59 [INFO]: Epoch 009 - training loss: 0.4766, validation loss: 0.2355\n",
      "2025-01-29 19:19:59 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-01-29 19:19:59 [INFO]: Finished training. The best model is from epoch#6.\n",
      "2025-01-29 19:19:59 [INFO]: Saved the model to tutorial_results/imputation/brits/20250129_T191221/BRITS.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "brits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pypots/base.py:325: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model = torch.load(path, map_location=self.device)\n",
      "2025-02-10 23:06:35 [INFO]: Model loaded successfully from tutorial_results/imputation/us_gan/20250205_T144707/USGAN.pypots\n"
     ]
    }
   ],
   "source": [
    "us_gan.load(\"tutorial_results/imputation/us_gan/20250205_T144707/USGAN.pypots\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 23:06:46 [INFO]: Model loaded successfully from tutorial_results/imputation/gp_vae/20250205_T160427/GPVAE.pypots\n"
     ]
    }
   ],
   "source": [
    "gp_vae.load(\"tutorial_results/imputation/gp_vae/20250205_T160427/GPVAE.pypots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 23:06:55 [INFO]: Model loaded successfully from tutorial_results/imputation/mrnn/20250205_T165040/MRNN.pypots\n"
     ]
    }
   ],
   "source": [
    "mrnn.load(\"tutorial_results/imputation/mrnn/20250205_T165040/MRNN.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The testing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "saits_imputation = []\n",
    "for value in  dataset_for_testing.values():\n",
    "   _dict = {'X':value}\n",
    "   saits_results = saits.predict(_dict)\n",
    "   saits_imputation.append(saits_results[\"imputation\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "brits_imputation = []\n",
    "for value in dataset_for_testing.values():\n",
    "    _dict = {'X':value}\n",
    "    brits_results = brits.predict(_dict)\n",
    "    brits_imputation.append(brits_results[\"imputation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgan_imputation = []\n",
    "for value in  dataset_for_testing.values():\n",
    "   _dict = {'X':value}\n",
    "   usgan_results = us_gan.predict(_dict)\n",
    "   usgan_imputation.append(usgan_results[\"imputation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpvae_imputation = []\n",
    "for value in  dataset_for_testing.values():\n",
    "   _dict = {'X':value}\n",
    "   gpvae_results = gp_vae.predict(_dict)\n",
    "   gpvae_imputation.append(gpvae_results[\"imputation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_imputation = []\n",
    "for value in  dataset_for_testing.values():\n",
    "   _dict = {'X':value}\n",
    "   mrnn_results = mrnn.predict(_dict)\n",
    "   mrnn_imputation.append(mrnn_results[\"imputation\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_saits = []\n",
    "for i in range(len(saits_imputation)):\n",
    "    testing_mae_saits.append(calc_mae(saits_imputation[i], test_X_ori[i], test_X_indicating_mask[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_brits = []\n",
    "for i in range(len(brits_imputation)):\n",
    "    testing_mae_brits.append(calc_mae(brits_imputation[i], test_X_ori[i], test_X_indicating_mask[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_usgan = []\n",
    "for i in range(len(usgan_imputation)):\n",
    "    testing_mae_usgan.append(calc_mae(usgan_imputation[i], test_X_ori[i], test_X_indicating_mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gpvae_imputation)):\n",
    "    gpvae_imputation[i] = gpvae_imputation[i].reshape(len(gpvae_imputation[i]), 48, 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_gpvae = []\n",
    "for i in range(len(gpvae_imputation)):\n",
    "    testing_mae_gpvae.append(calc_mae(gpvae_imputation[i], test_X_ori[i], test_X_indicating_mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_mrnn = []\n",
    "for i in range(len(mrnn_imputation)):\n",
    "    testing_mae_mrnn.append(calc_mae(mrnn_imputation[i], test_X_ori[i], test_X_indicating_mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = [\"General\", \"Female\", \"Male\", \"Undefined Gender\", \"+65\", \"-65\", \"ICUType 1\", \"ICUType 2\", \"ICUType 3\", \"ICUType 4\", \"Undefined classification\", \"Low Weight\", \"Normal Weight\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAITS - MAE\n",
      "-------------\n",
      "General : 0.2419255043308326\n",
      "Female : 0.242273102224823\n",
      "Male : 0.24712817296439005\n",
      "Undefined Gender : 0.1640770114417619\n",
      "+65 : 0.23916580624094136\n",
      "-65 : 0.25106176086317417\n",
      "ICUType 1 : 0.2623102086255934\n",
      "ICUType 2 : 0.21574660247582167\n",
      "ICUType 3 : 0.2588435848279136\n",
      "ICUType 4 : 0.24981279229779235\n",
      "Undefined classification : 0.254294316114311\n",
      "Low Weight : 0.26693886602376354\n",
      "Normal Weight : 0.22811312944798237\n",
      "Overweight : 0.22872794434980048\n",
      "Obesity 1 : 0.24896863554306903\n",
      "Obesity 2 : 0.23320570515760058\n",
      "Obesity 3 : 0.2450365789175021\n"
     ]
    }
   ],
   "source": [
    "print(\"SAITS - MAE\")\n",
    "print(\"-------------\")\n",
    "for i in range(len(subgroups)):\n",
    "    print(subgroups[i], \":\" ,testing_mae_saits[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRITS - MAE\n",
      "-------------\n",
      "General : 0.20607649494742675\n",
      "Female : 0.20614220432360486\n",
      "Male : 0.21246684469801871\n",
      "Undefined Gender : 0.18272824533494633\n",
      "+65 : 0.20458278834580312\n",
      "-65 : 0.21627359080249212\n",
      "ICUType 1 : 0.220299073686516\n",
      "ICUType 2 : 0.1808477823544893\n",
      "ICUType 3 : 0.22441586013095977\n",
      "ICUType 4 : 0.20966047499437565\n",
      "Undefined classification : 0.21801499897060275\n",
      "Low Weight : 0.24078274437854238\n",
      "Normal Weight : 0.195872946252596\n",
      "Overweight : 0.19804599357933922\n",
      "Obesity 1 : 0.19122453585431926\n",
      "Obesity 2 : 0.2035281786613216\n",
      "Obesity 3 : 0.2143222467284282\n"
     ]
    }
   ],
   "source": [
    "print(\"BRITS - MAE\")\n",
    "print(\"-------------\")\n",
    "for i in range(len(subgroups)):\n",
    "    print(subgroups[i], \":\" ,testing_mae_brits[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USGAN - MAE\n",
      "-------------\n",
      "General : 0.2725404672962992\n",
      "Female : 0.27515345338175035\n",
      "Male : 0.2759156782410606\n",
      "Undefined Gender : 0.22719787038605313\n",
      "+65 : 0.2703022819200602\n",
      "-65 : 0.28051891797636347\n",
      "ICUType 1 : 0.29337843510987965\n",
      "ICUType 2 : 0.23865748003371157\n",
      "ICUType 3 : 0.29412443520709886\n",
      "ICUType 4 : 0.2800859210695823\n",
      "Undefined classification : 0.2877627249325782\n",
      "Low Weight : 0.30363618669583015\n",
      "Normal Weight : 0.2588177315143253\n",
      "Overweight : 0.2544338181108165\n",
      "Obesity 1 : 0.2758376945836133\n",
      "Obesity 2 : 0.2619401389587843\n",
      "Obesity 3 : 0.2693813004105881\n"
     ]
    }
   ],
   "source": [
    "print(\"USGAN - MAE\")\n",
    "print(\"-------------\")\n",
    "for i in range(len(subgroups)):\n",
    "    print(subgroups[i], \":\" ,testing_mae_usgan[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRNN - MAE\n",
      "-------------\n",
      "General : 0.6713145721873255\n",
      "Female : 0.6886168215267683\n",
      "Male : 0.6593698528405196\n",
      "Undefined Gender : 0.5788306694697075\n",
      "+65 : 0.6680155576961296\n",
      "-65 : 0.6851731084848083\n",
      "ICUType 1 : 0.7000602620167663\n",
      "ICUType 2 : 0.5960496586403451\n",
      "ICUType 3 : 0.7305747176870081\n",
      "ICUType 4 : 0.6660171755623768\n",
      "Undefined classification : 0.6998902963878201\n",
      "Low Weight : 0.7413824045826151\n",
      "Normal Weight : 0.6484704845721746\n",
      "Overweight : 0.6258698324694673\n",
      "Obesity 1 : 0.6399939492108968\n",
      "Obesity 2 : 0.6628144867515143\n",
      "Obesity 3 : 0.7372770171765964\n"
     ]
    }
   ],
   "source": [
    "print(\"MRNN - MAE\")\n",
    "print(\"-------------\")\n",
    "for i in range(len(subgroups)):\n",
    "    print(subgroups[i], \":\" ,testing_mae_mrnn[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPVAE - MAE\n",
      "-------------\n",
      "General : 0.454358819819243\n",
      "Female : 0.45964569560182006\n",
      "Male : 0.4539033170525852\n",
      "Undefined Gender : 0.3952713921281636\n",
      "+65 : 0.45158590664180026\n",
      "-65 : 0.4686480177725102\n",
      "ICUType 1 : 0.4759892460652797\n",
      "ICUType 2 : 0.39423516109862916\n",
      "ICUType 3 : 0.49689965362643795\n",
      "ICUType 4 : 0.46005345089771515\n",
      "Undefined classification : 0.47611340106275457\n",
      "Low Weight : 0.5194780203246361\n",
      "Normal Weight : 0.4300186668030726\n",
      "Overweight : 0.4300886671089587\n",
      "Obesity 1 : 0.45186196738172746\n",
      "Obesity 2 : 0.4501944866258905\n",
      "Obesity 3 : 0.4661876578901662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"GPVAE - MAE\")\n",
    "print(\"-------------\")\n",
    "for i in range(len(subgroups)):\n",
    "    print(subgroups[i], \":\" ,testing_mae_gpvae[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
