{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 20:27:30.116930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741735650.169067    1057 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741735650.184632    1057 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-11 20:27:30.317074: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/pypots/nn/functional/cuda.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return autocast(**kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-11 20:27:33 [WARNING]: ‼️ `pypots.utils.metrics` is deprecated. Please import from `pypots.nn.functional` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pypots\n",
    "import os\n",
    "import sys\n",
    "from pypots.utils.metrics import calc_mae\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS, BRITS, USGAN, GPVAE, MRNN\n",
    "import numpy as np\n",
    "import benchpots\n",
    "from pypots.utils.random import set_random_seed\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import pandas as pd    \n",
    "from pypotsModify.benchpotsMAE.datasets import preprocess_physionet2012\n",
    "from missingData.toolkits import toolkits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 22:32:47 [INFO]: Have set the random seed as 2022 for numpy and pytorch.\n",
      "2025-03-10 22:32:47 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2025-03-10 22:32:47 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2025-03-10 22:32:47 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2025-03-10 22:32:48 [INFO]: Loaded successfully!\n",
      "2025-03-10 22:33:02 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-03-10 22:33:02 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-03-10 22:33:02 [INFO]: 68807 values masked out in the val set as ground truth, take 9.97% of the original observed values\n",
      "2025-03-10 22:33:02 [INFO]: 68807 values masked out in the val set as ground truth, take 9.97% of the original observed values\n",
      "2025-03-10 22:33:02 [INFO]: 86319 values masked out in the test set as ground truth, take 9.99% of the original observed values\n",
      "2025-03-10 22:33:02 [INFO]: 86319 values masked out in the test set as ground truth, take 9.99% of the original observed values\n",
      "2025-03-10 22:33:02 [INFO]: Total sample number: 11988\n",
      "2025-03-10 22:33:02 [INFO]: Total sample number: 11988\n",
      "2025-03-10 22:33:02 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-03-10 22:33:02 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-03-10 22:33:02 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-03-10 22:33:02 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-03-10 22:33:02 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-03-10 22:33:02 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-03-10 22:33:02 [INFO]: Number of steps: 48\n",
      "2025-03-10 22:33:02 [INFO]: Number of steps: 48\n",
      "2025-03-10 22:33:02 [INFO]: Number of features: 37\n",
      "2025-03-10 22:33:02 [INFO]: Number of features: 37\n",
      "2025-03-10 22:33:02 [INFO]: Train set missing rate: 79.70%\n",
      "2025-03-10 22:33:02 [INFO]: Train set missing rate: 79.70%\n",
      "2025-03-10 22:33:02 [INFO]: Validating set missing rate: 81.75%\n",
      "2025-03-10 22:33:02 [INFO]: Validating set missing rate: 81.75%\n",
      "2025-03-10 22:33:02 [INFO]: Test set missing rate: 81.75%\n",
      "2025-03-10 22:33:02 [INFO]: Test set missing rate: 81.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['n_classes', 'n_steps', 'n_features', 'scaler', 'train_X', 'train_y', 'train_ICUType', 'val_X', 'val_y', 'val_ICUType', 'test_X', 'test_y', 'test_ICUType', 'female_gender_test_X', 'female_gender_test_y', 'test_ICUType_female_gender', 'male_gender_test_X', 'male_gender_test_y', 'test_ICUType_male_gender', 'undefined_gender_test_X', 'undefined_gender_test_y', 'test_ICUType_undefined_gender', 'more_than_or_equal_to_65_test_X', 'more_than_or_equal_to_65_test_y', 'test_ICUType_more_than_or_equal_to_65', 'less_than_65_test_X', 'less_than_65_test_y', 'test_ICUType_less_than_65', 'ICUType_1_test_X', 'ICUType_1_test_y', 'test_ICUType_1', 'ICUType_2_test_X', 'ICUType_2_test_y', 'test_ICUType_2', 'ICUType_3_test_X', 'ICUType_3_test_y', 'test_ICUType_3', 'ICUType_4_test_X', 'ICUType_4_test_y', 'test_ICUType_4', 'classificacao_undefined_test_X', 'classificacao_undefined_test_y', 'test_ICUType_classificacao_undefined', 'classificacao_baixo_peso_test_X', 'classificacao_baixo_peso_test_y', 'test_ICUType_classificao_baixo_peso', 'classificacao_normal_peso_test_X', 'classificacao_normal_peso_test_y', 'test_ICUType_classificacao_normal_peso', 'classificacao_sobrepeso_test_X', 'classificacao_sobrepeso_test_y', 'test_ICUType_classificacao_sobrepeso', 'classificacao_obesidade_1_test_X', 'classificacao_obesidade_1_test_y', 'test_ICUType_classificacao_obesidade_1', 'classificacao_obesidade_2_test_X', 'classificacao_obesidade_2_test_y', 'test_ICUType_classificacao_obesidade_2', 'classificacao_obesidade_3_test_X', 'classificacao_obesidade_3_test_y', 'test_ICUType_classificacao_obesidade_3', 'val_X_ori', 'test_X_ori', 'female_gender_test_X_ori', 'male_gender_test_X_ori', 'undefined_gender_test_X_ori', 'more_than_or_equal_to_65_test_X_ori', 'less_than_65_test_X_ori', 'ICUType_1_test_X_ori', 'ICUType_2_test_X_ori', 'ICUType_3_test_X_ori', 'ICUType_4_test_X_ori', 'classificacao_undefined_test_X_ori', 'classificacao_baixo_peso_test_X_ori', 'classificacao_normal_peso_test_X_ori', 'classificacao_sobrepeso_test_X_ori', 'classificacao_obesidade_1_test_X_ori', 'classificacao_obesidade_2_test_X_ori', 'classificacao_obesidade_3_test_X_ori'])\n"
     ]
    }
   ],
   "source": [
    "set_random_seed()\n",
    "physionet2012_dataset_standard = preprocess_physionet2012(subset=\"all\", rate=0.1, normalization=1)\n",
    "print(physionet2012_dataset_standard.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_standard = physionet2012_dataset_standard[\"scaler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_training_standard, dataset_for_validating_standard, dataset_for_testing_ori_standard, dataset_for_testing_standard = toolkits.separating_dataset(physionet2012_dataset_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 22:33:10 [INFO]: Have set the random seed as 2022 for numpy and pytorch.\n",
      "2025-03-10 22:33:10 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2025-03-10 22:33:10 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2025-03-10 22:33:10 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2025-03-10 22:33:10 [INFO]: Loaded successfully!\n",
      "2025-03-10 22:33:24 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-03-10 22:33:24 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-03-10 22:33:24 [INFO]: 68807 values masked out in the val set as ground truth, take 9.97% of the original observed values\n",
      "2025-03-10 22:33:24 [INFO]: 68807 values masked out in the val set as ground truth, take 9.97% of the original observed values\n",
      "2025-03-10 22:33:24 [INFO]: 86319 values masked out in the test set as ground truth, take 9.99% of the original observed values\n",
      "2025-03-10 22:33:24 [INFO]: 86319 values masked out in the test set as ground truth, take 9.99% of the original observed values\n",
      "2025-03-10 22:33:24 [INFO]: Total sample number: 11988\n",
      "2025-03-10 22:33:24 [INFO]: Total sample number: 11988\n",
      "2025-03-10 22:33:24 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-03-10 22:33:24 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-03-10 22:33:24 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-03-10 22:33:24 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-03-10 22:33:24 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-03-10 22:33:24 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-03-10 22:33:24 [INFO]: Number of steps: 48\n",
      "2025-03-10 22:33:24 [INFO]: Number of steps: 48\n",
      "2025-03-10 22:33:24 [INFO]: Number of features: 37\n",
      "2025-03-10 22:33:24 [INFO]: Number of features: 37\n",
      "2025-03-10 22:33:24 [INFO]: Train set missing rate: 79.70%\n",
      "2025-03-10 22:33:24 [INFO]: Train set missing rate: 79.70%\n",
      "2025-03-10 22:33:24 [INFO]: Validating set missing rate: 81.75%\n",
      "2025-03-10 22:33:24 [INFO]: Validating set missing rate: 81.75%\n",
      "2025-03-10 22:33:24 [INFO]: Test set missing rate: 81.75%\n",
      "2025-03-10 22:33:24 [INFO]: Test set missing rate: 81.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['n_classes', 'n_steps', 'n_features', 'scaler', 'train_X', 'train_y', 'train_ICUType', 'val_X', 'val_y', 'val_ICUType', 'test_X', 'test_y', 'test_ICUType', 'female_gender_test_X', 'female_gender_test_y', 'test_ICUType_female_gender', 'male_gender_test_X', 'male_gender_test_y', 'test_ICUType_male_gender', 'undefined_gender_test_X', 'undefined_gender_test_y', 'test_ICUType_undefined_gender', 'more_than_or_equal_to_65_test_X', 'more_than_or_equal_to_65_test_y', 'test_ICUType_more_than_or_equal_to_65', 'less_than_65_test_X', 'less_than_65_test_y', 'test_ICUType_less_than_65', 'ICUType_1_test_X', 'ICUType_1_test_y', 'test_ICUType_1', 'ICUType_2_test_X', 'ICUType_2_test_y', 'test_ICUType_2', 'ICUType_3_test_X', 'ICUType_3_test_y', 'test_ICUType_3', 'ICUType_4_test_X', 'ICUType_4_test_y', 'test_ICUType_4', 'classificacao_undefined_test_X', 'classificacao_undefined_test_y', 'test_ICUType_classificacao_undefined', 'classificacao_baixo_peso_test_X', 'classificacao_baixo_peso_test_y', 'test_ICUType_classificao_baixo_peso', 'classificacao_normal_peso_test_X', 'classificacao_normal_peso_test_y', 'test_ICUType_classificacao_normal_peso', 'classificacao_sobrepeso_test_X', 'classificacao_sobrepeso_test_y', 'test_ICUType_classificacao_sobrepeso', 'classificacao_obesidade_1_test_X', 'classificacao_obesidade_1_test_y', 'test_ICUType_classificacao_obesidade_1', 'classificacao_obesidade_2_test_X', 'classificacao_obesidade_2_test_y', 'test_ICUType_classificacao_obesidade_2', 'classificacao_obesidade_3_test_X', 'classificacao_obesidade_3_test_y', 'test_ICUType_classificacao_obesidade_3', 'val_X_ori', 'test_X_ori', 'female_gender_test_X_ori', 'male_gender_test_X_ori', 'undefined_gender_test_X_ori', 'more_than_or_equal_to_65_test_X_ori', 'less_than_65_test_X_ori', 'ICUType_1_test_X_ori', 'ICUType_2_test_X_ori', 'ICUType_3_test_X_ori', 'ICUType_4_test_X_ori', 'classificacao_undefined_test_X_ori', 'classificacao_baixo_peso_test_X_ori', 'classificacao_normal_peso_test_X_ori', 'classificacao_sobrepeso_test_X_ori', 'classificacao_obesidade_1_test_X_ori', 'classificacao_obesidade_2_test_X_ori', 'classificacao_obesidade_3_test_X_ori'])\n"
     ]
    }
   ],
   "source": [
    "set_random_seed()\n",
    "physionet2012_dataset_minmax = preprocess_physionet2012(subset=\"all\", rate=0.1, normalization=2)\n",
    "print(physionet2012_dataset_minmax.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_minmax = physionet2012_dataset_minmax[\"scaler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_training_minmax, dataset_for_validating_minmax, dataset_for_testing_minmax, dataset_for_testing_ori_minmax = toolkits.separating_dataset(physionet2012_dataset_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating indicating mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicating_mask_variable_standard, test_X_ori_variable_standard = toolkits.components_mae(dataset_for_testing_ori_standard, dataset_for_testing_standard)\n",
    "indicating_mask_variable_standard = toolkits.pre_reshape(indicating_mask_variable_standard)\n",
    "test_X_ori_variable_standard = toolkits.pre_reshape(test_X_ori_variable_standard)\n",
    "test_X_ori_variable_standard_ori = toolkits.desnormalization(test_X_ori_variable_standard, scaler_standard)\n",
    "indicating_mask_variable_standard = toolkits.reshape_variable(indicating_mask_variable_standard)\n",
    "test_X_ori_variable_standard = toolkits.reshape_variable(test_X_ori_variable_standard)\n",
    "test_X_ori_variable_standard_ori = toolkits.reshape_variable(test_X_ori_variable_standard_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicating_mask_variable_minmax, test_X_ori_variable_minmax = toolkits.components_mae(dataset_for_testing_ori_minmax, dataset_for_testing_minmax)\n",
    "indicating_mask_variable_minmax = toolkits.pre_reshape(indicating_mask_variable_minmax)\n",
    "test_X_ori_variable_minmax = toolkits.pre_reshape(test_X_ori_variable_minmax)\n",
    "test_X_ori_variable_minmax_ori = toolkits.desnormalization(test_X_ori_variable_minmax, scaler_minmax)\n",
    "indicating_mask_variable_minmax = toolkits.reshape_variable(indicating_mask_variable_minmax)\n",
    "test_X_ori_variable_minmax = toolkits.reshape_variable(test_X_ori_variable_minmax)\n",
    "test_X_ori_variable_minmax_ori = toolkits.reshape_variable(test_X_ori_variable_minmax_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialize the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 22:34:19 [INFO]: No given device, using default device: cuda\n",
      "2025-03-10 22:34:19 [INFO]: Model files will be saved to tutorial_results/imputation/saits/20250310_T223419\n",
      "2025-03-10 22:34:19 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits/20250310_T223419/tensorboard\n",
      "2025-03-10 22:34:19 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 720,182\n"
     ]
    }
   ],
   "source": [
    "saits_standard = SAITS(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/saits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 16:52:24 [INFO]: No given device, using default device: cuda\n",
      "2025-03-04 16:52:24 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-03-04 16:52:24 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 720,182\n"
     ]
    }
   ],
   "source": [
    "saits_standard = SAITS(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 22:34:24 [INFO]: No given device, using default device: cuda\n",
      "2025-03-10 22:34:24 [INFO]: Model files will be saved to tutorial_results/imputation/saits/20250310_T223424\n",
      "2025-03-10 22:34:24 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits/20250310_T223424/tensorboard\n",
      "2025-03-10 22:34:24 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 720,182\n"
     ]
    }
   ],
   "source": [
    "saits_minmax = SAITS(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/saits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits_minmax = SAITS(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 21:15:49 [INFO]: No given device, using default device: cuda\n",
      "2025-02-24 21:15:49 [INFO]: Model files will be saved to tutorial_results/imputation/brits/20250224_T211549\n",
      "2025-02-24 21:15:49 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/brits/20250224_T211549/tensorboard\n",
      "2025-02-24 21:15:49 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 239,344\n"
     ]
    }
   ],
   "source": [
    "brits_standard = BRITS(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/brits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:04:33 [INFO]: No given device, using default device: cpu\n",
      "2025-02-25 21:04:33 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-25 21:04:33 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 239,344\n"
     ]
    }
   ],
   "source": [
    "brits_standard = BRITS(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits_minmax = BRITS(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/brits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits_minmax = BRITS(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 21:26:19 [INFO]: No given device, using default device: cuda\n",
      "2025-02-24 21:26:19 [INFO]: Model files will be saved to tutorial_results/imputation/us_gan/20250224_T212619\n",
      "2025-02-24 21:26:19 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/us_gan/20250224_T212619/tensorboard\n",
      "2025-02-24 21:26:19 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 1,258,517\n"
     ]
    }
   ],
   "source": [
    "us_gan_standard = USGAN(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=256,\n",
    "    lambda_mse=1,\n",
    "    dropout=0.1,\n",
    "    G_steps=1,\n",
    "    D_steps=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/us_gan\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:04:37 [INFO]: No given device, using default device: cpu\n",
      "2025-02-25 21:04:37 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-25 21:04:37 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 1,258,517\n"
     ]
    }
   ],
   "source": [
    "us_gan_standard = USGAN(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=256,\n",
    "    lambda_mse=1,\n",
    "    dropout=0.1,\n",
    "    G_steps=1,\n",
    "    D_steps=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_gan_minmax = USGAN(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=256,\n",
    "    lambda_mse=1,\n",
    "    dropout=0.1,\n",
    "    G_steps=1,\n",
    "    D_steps=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/us_gan\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_gan_minmax = USGAN(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=256,\n",
    "    lambda_mse=1,\n",
    "    dropout=0.1,\n",
    "    G_steps=1,\n",
    "    D_steps=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 21:38:42 [INFO]: No given device, using default device: cuda\n",
      "2025-02-24 21:38:42 [INFO]: Model files will be saved to tutorial_results/imputation/gp_vae/20250224_T213842\n",
      "2025-02-24 21:38:42 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/gp_vae/20250224_T213842/tensorboard\n",
      "2025-02-24 21:38:42 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 229,652\n"
     ]
    }
   ],
   "source": [
    "gp_vae_standard = GPVAE(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    latent_size=37,\n",
    "    encoder_sizes=(128,128),\n",
    "    decoder_sizes=(256,256),\n",
    "    kernel=\"cauchy\",\n",
    "    beta=0.2,\n",
    "    M=1,\n",
    "    K=1,\n",
    "    sigma=1.005,\n",
    "    length_scale=7.0,\n",
    "    kernel_scales=1,\n",
    "    window_size=24,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/gp_vae\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:04:42 [INFO]: No given device, using default device: cpu\n",
      "2025-02-25 21:04:42 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-25 21:04:42 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 229,652\n"
     ]
    }
   ],
   "source": [
    "gp_vae_standard = GPVAE(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    latent_size=37,\n",
    "    encoder_sizes=(128,128),\n",
    "    decoder_sizes=(256,256),\n",
    "    kernel=\"cauchy\",\n",
    "    beta=0.2,\n",
    "    M=1,\n",
    "    K=1,\n",
    "    sigma=1.005,\n",
    "    length_scale=7.0,\n",
    "    kernel_scales=1,\n",
    "    window_size=24,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_vae_minmax = GPVAE(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    latent_size=37,\n",
    "    encoder_sizes=(128,128),\n",
    "    decoder_sizes=(256,256),\n",
    "    kernel=\"cauchy\",\n",
    "    beta=0.2,\n",
    "    M=1,\n",
    "    K=1,\n",
    "    sigma=1.005,\n",
    "    length_scale=7.0,\n",
    "    kernel_scales=1,\n",
    "    window_size=24,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/gp_vae\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_vae_minmax= GPVAE(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    latent_size=37,\n",
    "    encoder_sizes=(128,128),\n",
    "    decoder_sizes=(256,256),\n",
    "    kernel=\"cauchy\",\n",
    "    beta=0.2,\n",
    "    M=1,\n",
    "    K=1,\n",
    "    sigma=1.005,\n",
    "    length_scale=7.0,\n",
    "    kernel_scales=1,\n",
    "    window_size=24,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 21:40:01 [INFO]: No given device, using default device: cuda\n",
      "2025-02-24 21:40:01 [INFO]: Model files will be saved to tutorial_results/imputation/mrnn/20250224_T214001\n",
      "2025-02-24 21:40:01 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/mrnn/20250224_T214001/tensorboard\n",
      "2025-02-24 21:40:01 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 107,951\n"
     ]
    }
   ],
   "source": [
    "mrnn_standard = MRNN(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/mrnn\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:04:45 [INFO]: No given device, using default device: cpu\n",
      "2025-02-25 21:04:45 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-25 21:04:45 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 107,951\n"
     ]
    }
   ],
   "source": [
    "mrnn_standard = MRNN(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_minmax = MRNN(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/mrnn\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_minmax = MRNN(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Load the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - Standard Scaler </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 22:34:50 [INFO]: Epoch 001 - training loss (MSE): 0.7691, validation MSE: 6.7329\n",
      "2025-03-10 22:34:53 [INFO]: Epoch 002 - training loss (MSE): 0.5444, validation MSE: 6.6818\n",
      "2025-03-10 22:34:56 [INFO]: Epoch 003 - training loss (MSE): 0.4779, validation MSE: 6.6747\n",
      "2025-03-10 22:34:59 [INFO]: Epoch 004 - training loss (MSE): 0.4593, validation MSE: 6.6579\n",
      "2025-03-10 22:35:04 [INFO]: Epoch 005 - training loss (MSE): 0.4215, validation MSE: 6.6589\n",
      "2025-03-10 22:35:17 [INFO]: Epoch 006 - training loss (MSE): 0.4239, validation MSE: 6.6434\n",
      "2025-03-10 22:35:30 [INFO]: Epoch 007 - training loss (MSE): 0.4114, validation MSE: 6.7782\n",
      "2025-03-10 22:35:42 [INFO]: Epoch 008 - training loss (MSE): 0.4424, validation MSE: 6.6446\n",
      "2025-03-10 22:35:55 [INFO]: Epoch 009 - training loss (MSE): 0.4374, validation MSE: 6.6392\n",
      "2025-03-10 22:36:07 [INFO]: Epoch 010 - training loss (MSE): 0.3752, validation MSE: 6.6284\n",
      "2025-03-10 22:36:07 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-03-10 22:36:07 [INFO]: Saved the model to tutorial_results/imputation/saits/20250310_T223419/SAITS.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "saits_standard.fit(train_set=dataset_for_training_standard, val_set=dataset_for_validating_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 22:36:24 [INFO]: Epoch 001 - training loss (MSE): 0.0368, validation MSE: 0.0079\n",
      "2025-03-10 22:36:36 [INFO]: Epoch 002 - training loss (MSE): 0.0163, validation MSE: 0.0075\n",
      "2025-03-10 22:36:49 [INFO]: Epoch 003 - training loss (MSE): 0.0123, validation MSE: 0.0055\n",
      "2025-03-10 22:37:02 [INFO]: Epoch 004 - training loss (MSE): 0.0100, validation MSE: 0.0043\n",
      "2025-03-10 22:37:14 [INFO]: Epoch 005 - training loss (MSE): 0.0080, validation MSE: 0.0039\n",
      "2025-03-10 22:37:27 [INFO]: Epoch 006 - training loss (MSE): 0.0069, validation MSE: 0.0036\n",
      "2025-03-10 22:37:39 [INFO]: Epoch 007 - training loss (MSE): 0.0063, validation MSE: 0.0034\n",
      "2025-03-10 22:37:52 [INFO]: Epoch 008 - training loss (MSE): 0.0059, validation MSE: 0.0035\n",
      "2025-03-10 22:38:04 [INFO]: Epoch 009 - training loss (MSE): 0.0056, validation MSE: 0.0032\n",
      "2025-03-10 22:38:17 [INFO]: Epoch 010 - training loss (MSE): 0.0054, validation MSE: 0.0032\n",
      "2025-03-10 22:38:17 [INFO]: Finished training. The best model is from epoch#9.\n",
      "2025-03-10 22:38:17 [INFO]: Saved the model to tutorial_results/imputation/saits/20250310_T223424/SAITS.pypots\n"
     ]
    }
   ],
   "source": [
    "saits_minmax.fit(train_set=dataset_for_training_minmax, val_set=dataset_for_validating_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Load - Standard Scaler </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits_standard.load(\"tutorial_results/imputation/saits/standard_scaler/SAITS.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Load - Min/Max Scaler </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pypots/base.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model = torch.load(path, map_location=self.device)\n",
      "2025-02-26 19:23:02 [INFO]: Model loaded successfully from tutorial_results/imputation/saits/20250226_T191938/SAITS.pypots\n"
     ]
    }
   ],
   "source": [
    "saits_minmax.load(\"tutorial_results/imputation/saits/20250226_T191938/SAITS.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train - Standard Scaler </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 21:17:11 [INFO]: Epoch 001 - training loss: 0.9419, validation loss: 6.7858\n",
      "2025-02-24 21:17:11 [INFO]: Saved the model to tutorial_results/imputation/brits/20250224_T211549/BRITS_epoch1_loss6.78576402142644.pypots\n",
      "2025-02-24 21:17:57 [INFO]: Epoch 002 - training loss: 0.7344, validation loss: 6.7420\n",
      "2025-02-24 21:17:57 [INFO]: Saved the model to tutorial_results/imputation/brits/20250224_T211549/BRITS_epoch2_loss6.742043674240509.pypots\n",
      "2025-02-24 21:18:43 [INFO]: Epoch 003 - training loss: 0.6836, validation loss: 6.7305\n",
      "2025-02-24 21:18:43 [INFO]: Saved the model to tutorial_results/imputation/brits/20250224_T211549/BRITS_epoch3_loss6.730484957744678.pypots\n",
      "2025-02-24 21:19:29 [INFO]: Epoch 004 - training loss: 0.6590, validation loss: 6.7263\n",
      "2025-02-24 21:19:29 [INFO]: Saved the model to tutorial_results/imputation/brits/20250224_T211549/BRITS_epoch4_loss6.726297639558712.pypots\n",
      "2025-02-24 21:20:17 [INFO]: Epoch 005 - training loss: 0.6437, validation loss: 6.7247\n",
      "2025-02-24 21:20:17 [INFO]: Saved the model to tutorial_results/imputation/brits/20250224_T211549/BRITS_epoch5_loss6.724702579776446.pypots\n",
      "2025-02-24 21:21:03 [INFO]: Epoch 006 - training loss: 0.6324, validation loss: 6.7246\n",
      "2025-02-24 21:21:03 [INFO]: Saved the model to tutorial_results/imputation/brits/20250224_T211549/BRITS_epoch6_loss6.724553489933411.pypots\n",
      "2025-02-24 21:21:50 [INFO]: Epoch 007 - training loss: 0.6236, validation loss: 6.7276\n",
      "2025-02-24 21:22:39 [INFO]: Epoch 008 - training loss: 0.6160, validation loss: 6.7288\n",
      "2025-02-24 21:23:25 [INFO]: Epoch 009 - training loss: 0.6095, validation loss: 6.7304\n",
      "2025-02-24 21:23:25 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-02-24 21:23:25 [INFO]: Finished training. The best model is from epoch#6.\n",
      "2025-02-24 21:23:25 [INFO]: Saved the model to tutorial_results/imputation/brits/20250224_T211549/BRITS.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "brits_standard.fit(train_set=dataset_for_training_standard, val_set=dataset_for_validating_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "brits_minmax.fit(train_set=dataset_for_training_minmax, val_set=dataset_for_validating_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:05:09 [INFO]: Model loaded successfully from tutorial_results/imputation/brits/standard_scaler/BRITS.pypots\n"
     ]
    }
   ],
   "source": [
    "brits_standard.load(\"tutorial_results/imputation/brits/standard_scaler/BRITS.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Min/Max Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbrits\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtutorial_results/imputation/brits/20250210_T213459/BRITS.pypots\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'brits' is not defined"
     ]
    }
   ],
   "source": [
    "brits_minmax.load(\"tutorial_results/imputation/brits/20250210_T210425/BRITS.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - Standard Scaler </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 21:27:53 [INFO]: Epoch 001 - generator training loss: 0.4347, discriminator training loss: 0.1896, validation loss: 6.7770\n",
      "2025-02-24 21:27:53 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN_epoch1_loss6.776960478474696.pypots\n",
      "2025-02-24 21:29:00 [INFO]: Epoch 002 - generator training loss: 0.3748, discriminator training loss: 0.0544, validation loss: 6.7267\n",
      "2025-02-24 21:29:00 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN_epoch2_loss6.726737344016631.pypots\n",
      "2025-02-24 21:30:09 [INFO]: Epoch 003 - generator training loss: 0.3420, discriminator training loss: 0.0366, validation loss: 6.7000\n",
      "2025-02-24 21:30:09 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN_epoch3_loss6.699957043429216.pypots\n",
      "2025-02-24 21:31:17 [INFO]: Epoch 004 - generator training loss: 0.3202, discriminator training loss: 0.0309, validation loss: 6.6893\n",
      "2025-02-24 21:31:17 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN_epoch4_loss6.689272005856037.pypots\n",
      "2025-02-24 21:32:24 [INFO]: Epoch 005 - generator training loss: 0.3114, discriminator training loss: 0.0289, validation loss: 6.6805\n",
      "2025-02-24 21:32:24 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN_epoch5_loss6.68046296586593.pypots\n",
      "2025-02-24 21:33:33 [INFO]: Epoch 006 - generator training loss: 0.2998, discriminator training loss: 0.0270, validation loss: 6.6698\n",
      "2025-02-24 21:33:33 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN_epoch6_loss6.669843424111605.pypots\n",
      "2025-02-24 21:34:43 [INFO]: Epoch 007 - generator training loss: 0.2875, discriminator training loss: 0.0255, validation loss: 6.6631\n",
      "2025-02-24 21:34:43 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN_epoch7_loss6.663058541218439.pypots\n",
      "2025-02-24 21:35:54 [INFO]: Epoch 008 - generator training loss: 0.2793, discriminator training loss: 0.0241, validation loss: 6.6590\n",
      "2025-02-24 21:35:54 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN_epoch8_loss6.659013180186351.pypots\n",
      "2025-02-24 21:37:05 [INFO]: Epoch 009 - generator training loss: 0.2728, discriminator training loss: 0.0231, validation loss: 6.6546\n",
      "2025-02-24 21:37:05 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN_epoch9_loss6.654626488188902.pypots\n",
      "2025-02-24 21:38:15 [INFO]: Epoch 010 - generator training loss: 0.2685, discriminator training loss: 0.0225, validation loss: 6.6519\n",
      "2025-02-24 21:38:15 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN_epoch10_loss6.651917580763499.pypots\n",
      "2025-02-24 21:38:15 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-02-24 21:38:15 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250224_T212619/USGAN.pypots\n"
     ]
    }
   ],
   "source": [
    "us_gan_standard.fit(train_set=dataset_for_training_standard, val_set=dataset_for_validating_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_gan_minmax.fit(train_set=dataset_for_training_minmax, val_set=dataset_for_validating_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:05:34 [INFO]: Model loaded successfully from tutorial_results/imputation/us_gan/standard_scaler/USGAN.pypots\n"
     ]
    }
   ],
   "source": [
    "us_gan_standard.load(\"tutorial_results/imputation/us_gan/standard_scaler/USGAN.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Min/Max Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_gan_minmax.load(\"tutorial_results/imputation/us_gan/20250205_T212325/USGAN.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 21:39:04 [INFO]: Epoch 001 - training loss: 25923.7769, validation loss: 7.0152\n",
      "2025-02-24 21:39:04 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250224_T213842/GPVAE_epoch1_loss7.015189884106318.pypots\n",
      "2025-02-24 21:39:07 [INFO]: Epoch 002 - training loss: 22873.1363, validation loss: 6.9989\n",
      "2025-02-24 21:39:07 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250224_T213842/GPVAE_epoch2_loss6.998940398792426.pypots\n",
      "2025-02-24 21:39:10 [INFO]: Epoch 003 - training loss: 22842.5329, validation loss: 6.9768\n",
      "2025-02-24 21:39:10 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250224_T213842/GPVAE_epoch3_loss6.976767674088478.pypots\n",
      "2025-02-24 21:39:13 [INFO]: Epoch 004 - training loss: 22827.7529, validation loss: 6.9652\n",
      "2025-02-24 21:39:13 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250224_T213842/GPVAE_epoch4_loss6.965172406037649.pypots\n",
      "2025-02-24 21:39:16 [INFO]: Epoch 005 - training loss: 22826.0822, validation loss: 6.9331\n",
      "2025-02-24 21:39:16 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250224_T213842/GPVAE_epoch5_loss6.933136768142382.pypots\n",
      "2025-02-24 21:39:19 [INFO]: Epoch 006 - training loss: 22816.0627, validation loss: 6.9306\n",
      "2025-02-24 21:39:19 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250224_T213842/GPVAE_epoch6_loss6.930640709400177.pypots\n",
      "2025-02-24 21:39:22 [INFO]: Epoch 007 - training loss: 22824.0957, validation loss: 6.9383\n",
      "2025-02-24 21:39:25 [INFO]: Epoch 008 - training loss: 22810.7701, validation loss: 6.9179\n",
      "2025-02-24 21:39:25 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250224_T213842/GPVAE_epoch8_loss6.917866353690624.pypots\n",
      "2025-02-24 21:39:28 [INFO]: Epoch 009 - training loss: 22808.8641, validation loss: 6.9069\n",
      "2025-02-24 21:39:28 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250224_T213842/GPVAE_epoch9_loss6.90687709103028.pypots\n",
      "2025-02-24 21:39:31 [INFO]: Epoch 010 - training loss: 22807.0707, validation loss: 6.8994\n",
      "2025-02-24 21:39:31 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250224_T213842/GPVAE_epoch10_loss6.899356806774934.pypots\n",
      "2025-02-24 21:39:31 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-02-24 21:39:31 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250224_T213842/GPVAE.pypots\n"
     ]
    }
   ],
   "source": [
    "gp_vae_standard.fit(train_set=dataset_for_training_standard, val_set=dataset_for_validating_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_vae_minmax.fit(train_set=dataset_for_training_minmax, val_set=dataset_for_validating_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:05:46 [INFO]: Model loaded successfully from tutorial_results/imputation/gp_vae/standard_scaler/GPVAE.pypots\n"
     ]
    }
   ],
   "source": [
    "gp_vae_standard.load(\"tutorial_results/imputation/gp_vae/standard_scaler/GPVAE.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_vae_minmax.load(\"tutorial_results/imputation/gp_vae/20250205_T214223/GPVAE.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 21:40:42 [INFO]: Epoch 001 - training loss: 0.7549, validation loss: 7.3190\n",
      "2025-02-24 21:40:42 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250224_T214001/MRNN_epoch1_loss7.318970077236494.pypots\n",
      "2025-02-24 21:40:56 [INFO]: Epoch 002 - training loss: 0.5305, validation loss: 7.2713\n",
      "2025-02-24 21:40:56 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250224_T214001/MRNN_epoch2_loss7.271315918366114.pypots\n",
      "2025-02-24 21:41:11 [INFO]: Epoch 003 - training loss: 0.4909, validation loss: 7.2550\n",
      "2025-02-24 21:41:11 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250224_T214001/MRNN_epoch3_loss7.254956531524658.pypots\n",
      "2025-02-24 21:41:26 [INFO]: Epoch 004 - training loss: 0.4781, validation loss: 7.2454\n",
      "2025-02-24 21:41:26 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250224_T214001/MRNN_epoch4_loss7.245413625240326.pypots\n",
      "2025-02-24 21:41:41 [INFO]: Epoch 005 - training loss: 0.4575, validation loss: 7.2425\n",
      "2025-02-24 21:41:41 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250224_T214001/MRNN_epoch5_loss7.242501470446586.pypots\n",
      "2025-02-24 21:41:56 [INFO]: Epoch 006 - training loss: 0.4453, validation loss: 7.2405\n",
      "2025-02-24 21:41:56 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250224_T214001/MRNN_epoch6_loss7.240530400474866.pypots\n",
      "2025-02-24 21:42:11 [INFO]: Epoch 007 - training loss: 0.4442, validation loss: 7.2430\n",
      "2025-02-24 21:42:27 [INFO]: Epoch 008 - training loss: 0.4350, validation loss: 7.2452\n",
      "2025-02-24 21:42:42 [INFO]: Epoch 009 - training loss: 0.4246, validation loss: 7.2474\n",
      "2025-02-24 21:42:42 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-02-24 21:42:42 [INFO]: Finished training. The best model is from epoch#6.\n",
      "2025-02-24 21:42:42 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250224_T214001/MRNN.pypots\n"
     ]
    }
   ],
   "source": [
    "mrnn_standard.fit(train_set=dataset_for_training_standard, val_set=dataset_for_validating_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_minmax.fit(train_set=dataset_for_training_minmax, val_set=dataset_for_validating_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:05:52 [INFO]: Model loaded successfully from tutorial_results/imputation/mrnn/standard_scaler/MRNN.pypots\n"
     ]
    }
   ],
   "source": [
    "mrnn_standard.load(\"tutorial_results/imputation/mrnn/standard_scaler/MRNN.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_minmax.load(\"tutorial_results/imputation/mrnn/20250205_T221050/MRNN.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits_imputation_standard = toolkits.model_imputation(dataset_for_testing_standard, saits_standard)\n",
    "saits_imputation_standard = toolkits.pre_reshape(saits_imputation_standard)\n",
    "saits_imputation_standard_ori = toolkits.desnormalization(saits_imputation_standard, scaler_standard)\n",
    "saits_imputation_standard = toolkits.reshape_variable(saits_imputation_standard)\n",
    "saits_imputation_standard_ori = toolkits.reshape_variable(saits_imputation_standard_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits_imputation_minmax = toolkits.model_imputation(dataset_for_testing_minmax, saits_minmax)\n",
    "saits_imputation_minmax = toolkits.pre_reshape(saits_imputation_minmax)\n",
    "saits_imputation_minmax_ori = toolkits.desnormalization(saits_imputation_minmax, scaler_minmax)\n",
    "saits_imputation_minmax = toolkits.reshape_variable(saits_imputation_minmax)\n",
    "saits_imputation_minmax_ori = toolkits.reshape_variable(saits_imputation_minmax_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits_imputation_standard = toolkits.model_imputation(dataset_for_testing_standard, brits_standard)\n",
    "brits_imputation_standard = toolkits.pre_reshape(brits_imputation_standard)\n",
    "brits_imputation_standard_ori = toolkits.desnormalization(brits_imputation_standard, scaler_standard)\n",
    "brits_imputation_standard = toolkits.reshape_variable(brits_imputation_standard)\n",
    "brits_imputation_standard_ori = toolkits.reshape_variable(brits_imputation_standard_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits_imputation_minmax = toolkits.model_imputation(dataset_for_testing_minmax, brits_minmax)\n",
    "brits_imputation_minmax = toolkits.pre_reshape(brits_imputation_minmax)\n",
    "brits_imputation_minmax_ori = toolkits.desnormalization(brits_imputation_minmax, scaler_minmax) \n",
    "brits_imputation_minmax = toolkits.reshape_variable(brits_imputation_minmax)\n",
    "brits_imputation_minmax_ori = toolkits.reshape_variable(brits_imputation_minmax_ori)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgan_imputation_standard = toolkits.model_imputation(dataset_for_testing_standard, us_gan_standard)\n",
    "usgan_imputation_standard = toolkits.pre_reshape(usgan_imputation_standard)\n",
    "usgan_imputation_standard_ori = toolkits.desnormalization(usgan_imputation_standard, scaler_standard)\n",
    "usgan_imputation_standard = toolkits.reshape_variable(usgan_imputation_standard)\n",
    "usgan_imputation_standard_ori = toolkits.reshape_variable(usgan_imputation_standard_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgan_imputation_minmax = toolkits.model_imputation(dataset_for_testing_minmax, us_gan_minmax)\n",
    "usgan_imputation_minmax = toolkits.pre_reshape(usgan_imputation_minmax)\n",
    "usgan_imputation_minmax_ori = toolkits.desnormalization(brits_imputation_minmax, scaler_minmax)\n",
    "usgan_imputation_minmax = toolkits.reshape_variable(usgan_imputation_minmax)\n",
    "usgan_imputation_minmax_ori = toolkits.reshape_variable(usgan_imputation_minmax_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpvae_imputation_standard = toolkits.model_imputation(dataset_for_testing_standard, gp_vae_standard)\n",
    "gpvae_imputation_standard = toolkits.pre_reshape(gpvae_imputation_standard)\n",
    "gpvae_imputation_standard_ori = toolkits.desnormalization(gpvae_imputation_standard, scaler_standard)\n",
    "gpvae_imputation_standard = toolkits.reshape_variable(gpvae_imputation_standard)\n",
    "gpvae_imputation_standard_ori = toolkits.reshape_variable(gpvae_imputation_standard_ori) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpvae_imputation_minmax = toolkits.model_imputation(dataset_for_testing_minmax, gp_vae_minmax)\n",
    "gpvae_imputation_minmax = toolkits.pre_reshape(gpvae_imputation_minmax)\n",
    "gpvae_imputation_minmax_ori = toolkits.desnormalization(gpvae_imputation_minmax, scaler_minmax)\n",
    "gpvae_imputation_minmax = toolkits.reshape_variable(gpvae_imputation_minmax)\n",
    "gpvae_imputation_minmax_ori = toolkits.reshape_variable(gpvae_imputation_minmax_ori) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_imputation_standard = toolkits.model_imputation(dataset_for_testing_standard, mrnn_standard)\n",
    "mrnn_imputation_standard = toolkits.pre_reshape(mrnn_imputation_standard)\n",
    "mrnn_imputation_standard_ori = toolkits.desnormalization(mrnn_imputation_standard, scaler_standard)\n",
    "mrnn_imputation_standard = toolkits.reshape_variable(mrnn_imputation_standard)\n",
    "mrnn_imputation_standard_ori = toolkits.reshape_variable(mrnn_imputation_standard_ori) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_imputation_minmax = toolkits.model_imputation(dataset_for_testing_minmax, mrnn_minmax)\n",
    "mrnn_imputation_minmax = toolkits.pre_reshape(mrnn_imputation_minmax)\n",
    "mrnn_imputation_minmax_ori = toolkits.desnormalization(mrnn_imputation_minmax, scaler_minmax)\n",
    "mrnn_imputation_minmax = toolkits.reshape_variable(mrnn_imputation_minmax)\n",
    "mrnn_imputation_minmax_ori = toolkits.reshape_variable(mrnn_imputation_minmax_ori) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_saits_variables_standard = toolkits.calculate_mae(saits_imputation_standard, test_X_ori_variable_standard, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_saits_variables_standard_ori = toolkits.calculate_mae(saits_imputation_standard_ori, test_X_ori_variable_standard_ori, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_saits_variables_minmax = toolkits.calculate_mae(saits_imputation_minmax, test_X_ori_variable_minmax, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_saits_variables_minmax_ori = toolkits.calculate_mae(saits_imputation_minmax_ori, test_X_ori_variable_minmax_ori, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_brits_variables_standard = toolkits.calculate_mae(brits_imputation_standard, test_X_ori_variable_standard, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_brits_variables_standard_ori = toolkits.calculate_mae(brits_imputation_standard_ori, test_X_ori_variable_standard_ori, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_brits_variables_minmax = toolkits.calculate_mae(brits_imputation_minmax, test_X_ori_variable_minmax, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_brits_variables_minmax_ori = toolkits.calculate_mae(brits_imputation_minmax_ori, test_X_ori_variable_minmax_ori, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_usgan_variables_standard = toolkits.calculate_mae(usgan_imputation_standard, test_X_ori_variable_standard, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_usgan_variables_standard_ori = toolkits.calculate_mae(usgan_imputation_standard_ori, test_X_ori_variable_standard_ori, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_usgan_variables_minmax = toolkits.calculate_mae(usgan_imputation_minmax, test_X_ori_variable_minmax, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_usgan_variables_minmax_ori = toolkits.calculate_mae(usgan_imputation_minmax_ori, test_X_ori_variable_minmax_ori, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_gpvae_variables_standard = toolkits.calculate_mae(gpvae_imputation_standard, test_X_ori_variable_standard, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_gpvae_variables_standard_ori = toolkits.calculate_mae(gpvae_imputation_standard_ori, test_X_ori_variable_standard_ori, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_gpvae_variables_minmax = toolkits.calculate_mae(gpvae_imputation_minmax, test_X_ori_variable_minmax, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_gpvae_variables_minmax_ori = toolkits.calculate_mae(gpvae_imputation_minmax_ori, test_X_ori_variable_minmax_ori, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_mrnn_variables_standard = toolkits.calculate_mae(mrnn_imputation_standard, test_X_ori_variable_standard, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_mrnn_variables_standard_ori = toolkits.calculate_mae(mrnn_imputation_standard_ori, test_X_ori_variable_standard_ori, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_mrnn_variables_minmax = toolkits.calculate_mae(mrnn_imputation_minmax, test_X_ori_variable_minmax, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_mrnn_variables_minmax_ori = toolkits.calculate_mae(mrnn_imputation_minmax_ori, test_X_ori_variable_minmax_ori, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = [\"General\", \"Female\", \"Male\", \"Undefined Gender\", \"+65\", \"-65\", \"ICUType 1\", \"ICUType 2\", \"ICUType 3\", \"ICUType 4\", \"Undefined classification\", \"Low Weight\", \"Normal Weight\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"ALP\", \"ALT\", \"AST\", \"Albumin\", \"BUN\", \"Bilirubin\", \"Cholesterol\", \"Creatinine\", \"DiasABP\", \"FiO2\", \"GCS\", \"Glucose\", \"HCO3\", \"HCT\", \"HR\", \"K\", \"Lactate\", \"MAP\", \"MechVent\", \"Mg\", \"NIDiasABP\", \"NIMAP\", \"NISysABP\", \"Na\", \"PaCO2\", \"PaO2\", \"Platelets\", \"RespRate\", \"SaO2\", \"SysABP\", \"Temp\", \"TroponinI\", \"TroponinT\", \"Urine\", \"WBC\", \"Weight\", \"Ph\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Standard Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAITS - MAE\n",
      "************\n",
      "General\n",
      "-------------\n",
      "ALP : 37.78297266364078\n",
      "ALT : 270.4063470760967\n",
      "AST : 400.73789268472706\n",
      "Albumin : 0.37630310573165954\n",
      "BUN : 5.041922862972568\n",
      "Bilirubin : 1.462853714307099\n",
      "Cholesterol : 40.12622451782026\n",
      "Creatinine : 0.3069420256832965\n",
      "DiasABP : 2.7704566013622594\n",
      "FiO2 : 0.08620956248667794\n",
      "GCS : 0.9567408451844454\n",
      "Glucose : 35.11931412160936\n",
      "HCO3 : 2.0015247896843125\n",
      "HCT : 2.6748668442108534\n",
      "HR : 4.738258661774259\n",
      "K : 0.4027210196933226\n",
      "Lactate : 1.1258680202247207\n",
      "MAP : 3.003517885020544\n",
      "MechVent : 0.012462921181450714\n",
      "Mg : 0.2673795162468418\n",
      "NIDiasABP : 2.679144880002555\n",
      "NIMAP : 2.2011852084165207\n",
      "NISysABP : 5.161661609334132\n",
      "Na : 2.1882702717229923\n",
      "PaCO2 : 4.549572724025382\n",
      "PaO2 : 40.3313324594816\n",
      "Platelets : 37.037300268262754\n",
      "RespRate : 2.5052943566163246\n",
      "SaO2 : 1.3193937473542672\n",
      "SysABP : 5.728127288963052\n",
      "Temp : 0.38344286305542885\n",
      "TroponinI : 8.488083906173282\n",
      "TroponinT : 1.0154387491644747\n",
      "Urine : 58.69247400513298\n",
      "WBC : 5.291650234308072\n",
      "Weight : 2.6566059164306446\n",
      "Ph : 0.14884304164587706\n",
      "Female\n",
      "-------------\n",
      "ALP : 45.90801022150683\n",
      "ALT : 183.76410183436175\n",
      "AST : 395.8701238334127\n",
      "Albumin : 0.4143689898477025\n",
      "BUN : 5.060138316747144\n",
      "Bilirubin : 1.7158839496353115\n",
      "Cholesterol : 57.47481994627756\n",
      "Creatinine : 0.24594034321627314\n",
      "DiasABP : 2.758156649729603\n",
      "FiO2 : 0.08440451246333774\n",
      "GCS : 0.9209658150271853\n",
      "Glucose : 37.63179741949284\n",
      "HCO3 : 1.9573508876279813\n",
      "HCT : 2.7633539258971638\n",
      "HR : 4.914148450234037\n",
      "K : 0.42631756605740817\n",
      "Lactate : 1.0842356902625119\n",
      "MAP : 3.3259711326045793\n",
      "MechVent : 0.01182785045626394\n",
      "Mg : 0.25594399306689614\n",
      "NIDiasABP : 2.8417917438754\n",
      "NIMAP : 2.2527768505354917\n",
      "NISysABP : 5.244811932887657\n",
      "Na : 2.2457757568359313\n",
      "PaCO2 : 4.60622380108139\n",
      "PaO2 : 39.03716874539377\n",
      "Platelets : 39.00449132286891\n",
      "RespRate : 2.5178707732671284\n",
      "SaO2 : 2.015491099839789\n",
      "SysABP : 5.520485803243825\n",
      "Temp : 0.43489881889986015\n",
      "TroponinI : 7.328217387198669\n",
      "TroponinT : 0.9460969434678402\n",
      "Urine : 53.73491961341385\n",
      "WBC : 5.190010370019538\n",
      "Weight : 2.5572251014170955\n",
      "Ph : 0.08406353725209738\n",
      "Male\n",
      "-------------\n",
      "ALP : 44.232028449022806\n",
      "ALT : 176.91085737546138\n",
      "AST : 292.49699600095835\n",
      "Albumin : 0.41883045493102145\n",
      "BUN : 4.804880042054326\n",
      "Bilirubin : 1.3507156259015705\n",
      "Cholesterol : 36.34643730750171\n",
      "Creatinine : 0.2863776194778349\n",
      "DiasABP : 2.7245748803870593\n",
      "FiO2 : 0.09097259859206534\n",
      "GCS : 0.9842186701706863\n",
      "Glucose : 35.64770712503564\n",
      "HCO3 : 1.9404482250213584\n",
      "HCT : 2.7043040742648823\n",
      "HR : 4.762584275178582\n",
      "K : 0.3670619531283295\n",
      "Lactate : 1.2013989996821746\n",
      "MAP : 3.2141414419172594\n",
      "MechVent : 0.013049005193913223\n",
      "Mg : 0.26650014210956996\n",
      "NIDiasABP : 2.9453207688302223\n",
      "NIMAP : 2.216462944201193\n",
      "NISysABP : 5.347620199791449\n",
      "Na : 2.3025885553984233\n",
      "PaCO2 : 4.2915123968708215\n",
      "PaO2 : 37.61968927253425\n",
      "Platelets : 34.583568572997976\n",
      "RespRate : 2.5060472980430912\n",
      "SaO2 : 1.250796426752558\n",
      "SysABP : 5.80673565515112\n",
      "Temp : 0.42170330921973537\n",
      "TroponinI : 10.24883082707552\n",
      "TroponinT : 1.2757686461664963\n",
      "Urine : 62.775168357102835\n",
      "WBC : 5.842827842566371\n",
      "Weight : 2.481717227406003\n",
      "Ph : 0.08523544849834366\n",
      "Undefined Gender\n",
      "-------------\n",
      "ALP : 0.0\n",
      "ALT : 0.0\n",
      "AST : 0.0\n",
      "Albumin : 0.0\n",
      "BUN : 0.0\n",
      "Bilirubin : 0.0\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.0\n",
      "DiasABP : 2.625834147135089\n",
      "FiO2 : 0.0\n",
      "GCS : 0.43523550033558456\n",
      "Glucose : 2.991394042965758\n",
      "HCO3 : 1.062695503234332\n",
      "HCT : 0.0\n",
      "HR : 4.627160072326082\n",
      "K : 0.26083431243870364\n",
      "Lactate : 0.0\n",
      "MAP : 4.170711517332942\n",
      "MechVent : 0.0\n",
      "Mg : 0.10903282165516436\n",
      "NIDiasABP : 4.463973045348005\n",
      "NIMAP : 1.5351785278315189\n",
      "NISysABP : 2.865608215330598\n",
      "Na : 0.0\n",
      "PaCO2 : 0.0\n",
      "PaO2 : 0.0\n",
      "Platelets : 0.0\n",
      "RespRate : 0.8148288726798492\n",
      "SaO2 : 0.0\n",
      "SysABP : 4.187951723734014\n",
      "Temp : 0.49435882568334727\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.0\n",
      "Urine : 57.70387268060635\n",
      "WBC : 0.8858859062190396\n",
      "Weight : 3.159104410806766\n",
      "Ph : 0.04132846832271261\n",
      "+65\n",
      "-------------\n",
      "ALP : 37.58231267664116\n",
      "ALT : 195.89302322890518\n",
      "AST : 231.16054653347015\n",
      "Albumin : 0.32952565950889656\n",
      "BUN : 5.1256476294230815\n",
      "Bilirubin : 1.6627602863104622\n",
      "Cholesterol : 34.27534675597859\n",
      "Creatinine : 0.253128001581153\n",
      "DiasABP : 2.8019461063590736\n",
      "FiO2 : 0.08770886212478182\n",
      "GCS : 0.9685721776753887\n",
      "Glucose : 36.5075550236113\n",
      "HCO3 : 2.089966905528096\n",
      "HCT : 2.616256232579545\n",
      "HR : 4.727897129776535\n",
      "K : 0.3623510098289821\n",
      "Lactate : 0.9925571685461726\n",
      "MAP : 3.2515305195356943\n",
      "MechVent : 0.01248656810672432\n",
      "Mg : 0.2561428501333662\n",
      "NIDiasABP : 2.811189087553255\n",
      "NIMAP : 2.2580729143813847\n",
      "NISysABP : 5.352268846327912\n",
      "Na : 2.0899440577772745\n",
      "PaCO2 : 4.3430915907844945\n",
      "PaO2 : 34.53628166573128\n",
      "Platelets : 38.19324392314768\n",
      "RespRate : 2.4906104773499713\n",
      "SaO2 : 1.3288770410938147\n",
      "SysABP : 5.60197975039624\n",
      "Temp : 0.41338185081499906\n",
      "TroponinI : 10.069288849829789\n",
      "TroponinT : 0.9174992609092552\n",
      "Urine : 52.886833885778415\n",
      "WBC : 5.750450373543619\n",
      "Weight : 2.3062503136502555\n",
      "Ph : 0.08207657400283347\n",
      "-65\n",
      "-------------\n",
      "ALP : 90.36775786990125\n",
      "ALT : 247.7900159608054\n",
      "AST : 371.5504835545984\n",
      "Albumin : 0.42201047476599707\n",
      "BUN : 5.193089256757558\n",
      "Bilirubin : 1.7329451547450232\n",
      "Cholesterol : 51.16094970700567\n",
      "Creatinine : 0.29230473334184565\n",
      "DiasABP : 2.674256230160794\n",
      "FiO2 : 0.08364223361871098\n",
      "GCS : 0.9591653378803174\n",
      "Glucose : 37.553925765188026\n",
      "HCO3 : 2.0818535940987672\n",
      "HCT : 2.6436960920691437\n",
      "HR : 4.801883217460124\n",
      "K : 0.39393472106368355\n",
      "Lactate : 1.1337297905815922\n",
      "MAP : 3.147766829174336\n",
      "MechVent : 0.012519296893875705\n",
      "Mg : 0.25231866424681143\n",
      "NIDiasABP : 2.9142074019079818\n",
      "NIMAP : 2.160745183450147\n",
      "NISysABP : 4.66897457840322\n",
      "Na : 2.450649693160899\n",
      "PaCO2 : 4.591566812710492\n",
      "PaO2 : 39.392599095286464\n",
      "Platelets : 35.455004451838974\n",
      "RespRate : 2.481607985244297\n",
      "SaO2 : 1.924456999473955\n",
      "SysABP : 5.290973273509577\n",
      "Temp : 0.4476782699705033\n",
      "TroponinI : 9.179028165339275\n",
      "TroponinT : 1.3688281882387496\n",
      "Urine : 67.50150207954115\n",
      "WBC : 5.3279573358338315\n",
      "Weight : 2.9135841087161967\n",
      "Ph : 0.08987185132083617\n",
      "ICUType 1\n",
      "-------------\n",
      "ALP : 26.574980605731106\n",
      "ALT : 290.6899559020851\n",
      "AST : 660.6233684539532\n",
      "Albumin : 0.38473770141599645\n",
      "BUN : 4.9190511481706105\n",
      "Bilirubin : 1.693838429093293\n",
      "Cholesterol : 40.131332397457854\n",
      "Creatinine : 0.3494820410505315\n",
      "DiasABP : 3.1914958891400054\n",
      "FiO2 : 0.09476566508317388\n",
      "GCS : 0.8462394520921507\n",
      "Glucose : 41.42535299062697\n",
      "HCO3 : 2.0671917400528717\n",
      "HCT : 2.5472450425889566\n",
      "HR : 4.496198730261589\n",
      "K : 0.3949684967313466\n",
      "Lactate : 1.5529015548767082\n",
      "MAP : 4.181647808249848\n",
      "MechVent : 0.010791735683413642\n",
      "Mg : 0.2207857854967177\n",
      "NIDiasABP : 2.579274255489448\n",
      "NIMAP : 2.137044383168665\n",
      "NISysABP : 4.983266155840321\n",
      "Na : 2.075892761230452\n",
      "PaCO2 : 4.145446461955361\n",
      "PaO2 : 40.291088740030624\n",
      "Platelets : 33.26148786383132\n",
      "RespRate : 2.4937054928568214\n",
      "SaO2 : 1.9246226443519834\n",
      "SysABP : 5.607140528714206\n",
      "Temp : 0.4558324956022057\n",
      "TroponinI : 14.876962232587863\n",
      "TroponinT : 1.488741037828037\n",
      "Urine : 70.16802343966279\n",
      "WBC : 3.9921548687327637\n",
      "Weight : 2.6401605095661815\n",
      "Ph : 0.0877794248827035\n",
      "ICUType 2\n",
      "-------------\n",
      "ALP : 22.82536693572907\n",
      "ALT : 166.7705052024352\n",
      "AST : 213.67784124871028\n",
      "Albumin : 0.48604442732667785\n",
      "BUN : 4.029728350146036\n",
      "Bilirubin : 1.4301827350630465\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.26118708318471734\n",
      "DiasABP : 2.415311428004181\n",
      "FiO2 : 0.08318870053244658\n",
      "GCS : 1.2927043978293604\n",
      "Glucose : 30.978810628254944\n",
      "HCO3 : 1.7425057132791921\n",
      "HCT : 2.6025545063725124\n",
      "HR : 4.114589286564236\n",
      "K : 0.39589649302060165\n",
      "Lactate : 1.1412507679174109\n",
      "MAP : 2.6857393460251107\n",
      "MechVent : 0.01327723206114707\n",
      "Mg : 0.28964251102627736\n",
      "NIDiasABP : 2.4635170322764957\n",
      "NIMAP : 2.063490536073134\n",
      "NISysABP : 5.0809648926004725\n",
      "Na : 2.21512117247649\n",
      "PaCO2 : 4.128036168729655\n",
      "PaO2 : 37.85349037495624\n",
      "Platelets : 33.77196070756844\n",
      "RespRate : 2.180743445812786\n",
      "SaO2 : 1.1829168843288003\n",
      "SysABP : 5.403055774398254\n",
      "Temp : 0.26314605009786535\n",
      "TroponinI : 3.5219585037224403\n",
      "TroponinT : 1.0230060291289145\n",
      "Urine : 56.08892355563659\n",
      "WBC : 4.289541022739691\n",
      "Weight : 2.810956070477913\n",
      "Ph : 0.07963966303379248\n",
      "ICUType 3\n",
      "-------------\n",
      "ALP : 86.87515029166424\n",
      "ALT : 187.66614407506268\n",
      "AST : 392.7643692165898\n",
      "Albumin : 0.4381506804496941\n",
      "BUN : 6.121343749866129\n",
      "Bilirubin : 1.7718448052707056\n",
      "Cholesterol : 41.18793296812935\n",
      "Creatinine : 0.37295183122790976\n",
      "DiasABP : 2.7470724217024354\n",
      "FiO2 : 0.08560839366838205\n",
      "GCS : 0.8941386387813536\n",
      "Glucose : 46.494442687783994\n",
      "HCO3 : 2.371723998160582\n",
      "HCT : 2.8609214281055944\n",
      "HR : 4.962478484868516\n",
      "K : 0.4572738585138742\n",
      "Lactate : 1.3807701775353112\n",
      "MAP : 3.201426160653487\n",
      "MechVent : 0.012368280552701387\n",
      "Mg : 0.28202333077788255\n",
      "NIDiasABP : 2.8455484648559035\n",
      "NIMAP : 2.2646497753718973\n",
      "NISysABP : 5.157939543919466\n",
      "Na : 2.18062673562512\n",
      "PaCO2 : 5.130342728382814\n",
      "PaO2 : 37.34235863485725\n",
      "Platelets : 41.010382328833586\n",
      "RespRate : 2.692867765772526\n",
      "SaO2 : 5.254020843505754\n",
      "SysABP : 5.909495902961238\n",
      "Temp : 0.590800437517797\n",
      "TroponinI : 5.491748778024699\n",
      "TroponinT : 0.7832913192885528\n",
      "Urine : 58.98669116312301\n",
      "WBC : 6.054897498891397\n",
      "Weight : 2.417315190832487\n",
      "Ph : 0.09159655480695168\n",
      "ICUType 4\n",
      "-------------\n",
      "ALP : 56.89257569746452\n",
      "ALT : 199.00896300643845\n",
      "AST : 469.25546223032705\n",
      "Albumin : 0.44695435635586045\n",
      "BUN : 4.1606343999886946\n",
      "Bilirubin : 1.5873492811564753\n",
      "Cholesterol : 29.733723958328376\n",
      "Creatinine : 0.24748621653334932\n",
      "DiasABP : 2.711476109500548\n",
      "FiO2 : 0.08193070614712325\n",
      "GCS : 0.9246070391069117\n",
      "Glucose : 29.069654179005372\n",
      "HCO3 : 1.9045468932057574\n",
      "HCT : 2.60781467674815\n",
      "HR : 5.103670692925516\n",
      "K : 0.32998052358627195\n",
      "Lactate : 1.1741103830509103\n",
      "MAP : 3.166016906577663\n",
      "MechVent : 0.012251276172731964\n",
      "Mg : 0.2610541593551626\n",
      "NIDiasABP : 3.030446541646777\n",
      "NIMAP : 2.2929307890260118\n",
      "NISysABP : 5.279487523739496\n",
      "Na : 2.324009744003681\n",
      "PaCO2 : 4.241135272931682\n",
      "PaO2 : 39.90710458301352\n",
      "Platelets : 34.856355165180275\n",
      "RespRate : 2.5662663039595235\n",
      "SaO2 : 1.7490848359607236\n",
      "SysABP : 5.980944253178294\n",
      "Temp : 0.5049576051642202\n",
      "TroponinI : 12.918011999127021\n",
      "TroponinT : 0.5820139079773556\n",
      "Urine : 61.085766369184974\n",
      "WBC : 4.190956191817902\n",
      "Weight : 2.736775199204113\n",
      "Ph : 0.07933268848455156\n",
      "Undefined classification\n",
      "-------------\n",
      "ALP : 35.112585764252344\n",
      "ALT : 233.5929812419234\n",
      "AST : 346.1022879801262\n",
      "Albumin : 0.44985988843134767\n",
      "BUN : 5.540460280009665\n",
      "Bilirubin : 1.8355630185287448\n",
      "Cholesterol : 29.371134440100903\n",
      "Creatinine : 0.2981092638840877\n",
      "DiasABP : 2.7878389980545144\n",
      "FiO2 : 0.08201023051891411\n",
      "GCS : 0.8269001310711612\n",
      "Glucose : 39.12889126564949\n",
      "HCO3 : 2.1974266479755213\n",
      "HCT : 2.8219767244347183\n",
      "HR : 4.91564167050028\n",
      "K : 0.38927459855874297\n",
      "Lactate : 1.1909267814618265\n",
      "MAP : 3.371506367514423\n",
      "MechVent : 0.01150444749213414\n",
      "Mg : 0.23472453453601874\n",
      "NIDiasABP : 2.8499736766374837\n",
      "NIMAP : 2.2413980699950544\n",
      "NISysABP : 5.224309436871385\n",
      "Na : 2.6156318442335342\n",
      "PaCO2 : 5.417221489599182\n",
      "PaO2 : 41.26753912328852\n",
      "Platelets : 39.68746191438966\n",
      "RespRate : 2.5926951850018085\n",
      "SaO2 : 2.3900887460419264\n",
      "SysABP : 5.931407422282698\n",
      "Temp : 0.4926773200356458\n",
      "TroponinI : 9.96634260813229\n",
      "TroponinT : 0.8920039851611282\n",
      "Urine : 61.26694349394846\n",
      "WBC : 6.259355865456574\n",
      "Weight : 2.5911002028951393\n",
      "Ph : 0.09134564891141982\n",
      "Low Weight\n",
      "-------------\n",
      "ALP : 60.07676506039476\n",
      "ALT : 117.08788604733986\n",
      "AST : 1023.0895042416876\n",
      "Albumin : 0.6520443201063977\n",
      "BUN : 5.120280599593861\n",
      "Bilirubin : 4.056221778391981\n",
      "Cholesterol : 0.5806274414056694\n",
      "Creatinine : 0.2015341237187242\n",
      "DiasABP : 3.1417549821666966\n",
      "FiO2 : 0.08482158961819232\n",
      "GCS : 1.0087426975369296\n",
      "Glucose : 56.65848375403117\n",
      "HCO3 : 2.678804238637065\n",
      "HCT : 2.7630029201506416\n",
      "HR : 5.314517693810162\n",
      "K : 0.43766482809313656\n",
      "Lactate : 0.9350687026976604\n",
      "MAP : 3.2905704484099174\n",
      "MechVent : 0.01291241953449823\n",
      "Mg : 0.258044333457921\n",
      "NIDiasABP : 3.874065741786197\n",
      "NIMAP : 2.858565009971681\n",
      "NISysABP : 4.263396602761038\n",
      "Na : 1.7950454711912267\n",
      "PaCO2 : 4.940311095293685\n",
      "PaO2 : 47.98038362201765\n",
      "Platelets : 42.66221896084484\n",
      "RespRate : 3.8176496596561615\n",
      "SaO2 : 0.7389760698590432\n",
      "SysABP : 5.375160058564854\n",
      "Temp : 0.30158229018702304\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.20344325065592445\n",
      "Urine : 63.73734224882659\n",
      "WBC : 6.064986500372787\n",
      "Weight : 3.280227311991002\n",
      "Ph : 0.08828813413294136\n",
      "Normal Weight\n",
      "-------------\n",
      "ALP : 38.924773185482614\n",
      "ALT : 152.72565505617416\n",
      "AST : 197.998634457582\n",
      "Albumin : 0.5214620678048111\n",
      "BUN : 4.822348313794684\n",
      "Bilirubin : 1.9636073605766933\n",
      "Cholesterol : 36.29257965086076\n",
      "Creatinine : 0.3216497388478136\n",
      "DiasABP : 2.595951942981959\n",
      "FiO2 : 0.08475874801541486\n",
      "GCS : 1.0517967951925158\n",
      "Glucose : 36.0991715360276\n",
      "HCO3 : 2.0130525574539497\n",
      "HCT : 2.426845000652544\n",
      "HR : 4.664726896694224\n",
      "K : 0.3939821773959713\n",
      "Lactate : 0.9000268365328041\n",
      "MAP : 2.929931329625391\n",
      "MechVent : 0.013287598918552006\n",
      "Mg : 0.28398762195090715\n",
      "NIDiasABP : 2.7883484915472905\n",
      "NIMAP : 2.1386014034496497\n",
      "NISysABP : 4.781441450059569\n",
      "Na : 2.4839418199327072\n",
      "PaCO2 : 4.288689076290181\n",
      "PaO2 : 39.58261358843692\n",
      "Platelets : 33.09497317307263\n",
      "RespRate : 2.343235340551307\n",
      "SaO2 : 1.2134137520423187\n",
      "SysABP : 5.304672409141559\n",
      "Temp : 0.36089013358798017\n",
      "TroponinI : 9.10681064393684\n",
      "TroponinT : 0.8382769810017686\n",
      "Urine : 53.68114414134356\n",
      "WBC : 5.259296546980352\n",
      "Weight : 2.2286725238407765\n",
      "Ph : 0.08065878375046892\n",
      "Overweight\n",
      "-------------\n",
      "ALP : 27.00695621967231\n",
      "ALT : 188.66664616266362\n",
      "AST : 522.0102859678475\n",
      "Albumin : 0.5113979615662958\n",
      "BUN : 4.370577674923489\n",
      "Bilirubin : 1.1948480236033265\n",
      "Cholesterol : 48.568814958837926\n",
      "Creatinine : 0.22317998015532362\n",
      "DiasABP : 2.780184372610194\n",
      "FiO2 : 0.08324649634413213\n",
      "GCS : 1.072412354591129\n",
      "Glucose : 36.846976420918146\n",
      "HCO3 : 1.8959981056951585\n",
      "HCT : 2.746380986008676\n",
      "HR : 4.4489803431761805\n",
      "K : 0.39553372653450247\n",
      "Lactate : 1.3272531441477688\n",
      "MAP : 3.180077124582844\n",
      "MechVent : 0.012850226540314485\n",
      "Mg : 0.24944680287287613\n",
      "NIDiasABP : 2.5783521862724537\n",
      "NIMAP : 2.3659360288940343\n",
      "NISysABP : 5.104422624996255\n",
      "Na : 2.1729870902167274\n",
      "PaCO2 : 4.334907681334238\n",
      "PaO2 : 37.94989866726629\n",
      "Platelets : 38.284566808629684\n",
      "RespRate : 2.374905034343298\n",
      "SaO2 : 1.2432743583138437\n",
      "SysABP : 5.663930899560424\n",
      "Temp : 0.2953842020207724\n",
      "TroponinI : 10.654040575025334\n",
      "TroponinT : 1.4031198195768155\n",
      "Urine : 54.22961657490185\n",
      "WBC : 4.549788521719948\n",
      "Weight : 2.0426298809147143\n",
      "Ph : 0.07685126599811348\n",
      "Obesity 1\n",
      "-------------\n",
      "ALP : 21.614137377056213\n",
      "ALT : 197.01251468657463\n",
      "AST : 329.8819475412204\n",
      "Albumin : 0.38511670827862393\n",
      "BUN : 4.105109596252387\n",
      "Bilirubin : 1.6469135570525026\n",
      "Cholesterol : 25.462768554679013\n",
      "Creatinine : 0.2715935959332192\n",
      "DiasABP : 2.6685200551356756\n",
      "FiO2 : 0.08576241496837457\n",
      "GCS : 0.8858744621276825\n",
      "Glucose : 31.635649965519935\n",
      "HCO3 : 1.7638227628624703\n",
      "HCT : 2.597251714978876\n",
      "HR : 4.602974528993501\n",
      "K : 0.37060853429587043\n",
      "Lactate : 1.1234909862941918\n",
      "MAP : 2.989623963850731\n",
      "MechVent : 0.012875867812014346\n",
      "Mg : 0.21986424173627273\n",
      "NIDiasABP : 2.524728567293366\n",
      "NIMAP : 2.1176541256662067\n",
      "NISysABP : 5.049840197036557\n",
      "Na : 2.4099256992339715\n",
      "PaCO2 : 4.251964788725856\n",
      "PaO2 : 35.94716759615145\n",
      "Platelets : 33.327148337112796\n",
      "RespRate : 2.4823727077907827\n",
      "SaO2 : 1.4200907222559962\n",
      "SysABP : 5.211509451015221\n",
      "Temp : 0.29463989568616283\n",
      "TroponinI : 6.433475112911822\n",
      "TroponinT : 0.8940153382040423\n",
      "Urine : 63.74683186904674\n",
      "WBC : 6.866181673768977\n",
      "Weight : 2.2887159407029416\n",
      "Ph : 1.077840609168645\n",
      "Obesity 2\n",
      "-------------\n",
      "ALP : 35.6564610799114\n",
      "ALT : 218.36687033513905\n",
      "AST : 43.18434206643254\n",
      "Albumin : 0.3436090071995479\n",
      "BUN : 6.027658339469469\n",
      "Bilirubin : 1.0728568646641703\n",
      "Cholesterol : 4.860565185542014\n",
      "Creatinine : 0.23406417369841748\n",
      "DiasABP : 2.909209909036012\n",
      "FiO2 : 0.0863455749791229\n",
      "GCS : 1.1467783292134508\n",
      "Glucose : 42.2903474377034\n",
      "HCO3 : 1.5546792984008013\n",
      "HCT : 2.5223673469141246\n",
      "HR : 4.734385670744669\n",
      "K : 0.3475203660818114\n",
      "Lactate : 1.2861948013304745\n",
      "MAP : 2.9985819373625757\n",
      "MechVent : 0.011038146058066765\n",
      "Mg : 0.2942987910023333\n",
      "NIDiasABP : 2.7918599976433436\n",
      "NIMAP : 1.8889718287033441\n",
      "NISysABP : 4.795165688100452\n",
      "Na : 1.7045024525034993\n",
      "PaCO2 : 4.055282534085725\n",
      "PaO2 : 29.575632418118538\n",
      "Platelets : 39.00821216289664\n",
      "RespRate : 2.6126086278394625\n",
      "SaO2 : 1.538899739583248\n",
      "SysABP : 5.401177049509663\n",
      "Temp : 0.36712187803507956\n",
      "TroponinI : 0.0\n",
      "TroponinT : 2.3066109498334186\n",
      "Urine : 57.0015903963946\n",
      "WBC : 4.337441118301744\n",
      "Weight : 2.665546591385537\n",
      "Ph : 0.06203329188482993\n",
      "Obesity 3\n",
      "-------------\n",
      "ALP : 21.327258370137443\n",
      "ALT : 376.07747713712234\n",
      "AST : 137.90593719478974\n",
      "Albumin : 0.26556117057794976\n",
      "BUN : 5.077936581202675\n",
      "Bilirubin : 1.6008287633456106\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.2429011098800089\n",
      "DiasABP : 2.3752263841174814\n",
      "FiO2 : 0.10114214977065175\n",
      "GCS : 1.0898051379156806\n",
      "Glucose : 37.529282064997744\n",
      "HCO3 : 2.216592255760576\n",
      "HCT : 2.874966663267484\n",
      "HR : 4.492783341758685\n",
      "K : 0.3717954587936277\n",
      "Lactate : 1.0522259215513574\n",
      "MAP : 2.582290955423221\n",
      "MechVent : 0.013644342681011359\n",
      "Mg : 0.2679494585309633\n",
      "NIDiasABP : 3.726555076012216\n",
      "NIMAP : 3.0280127619150323\n",
      "NISysABP : 5.978000859218771\n",
      "Na : 1.8388592263925831\n",
      "PaCO2 : 4.932598424512229\n",
      "PaO2 : 30.752425554539485\n",
      "Platelets : 26.51269006729043\n",
      "RespRate : 2.3622169847840744\n",
      "SaO2 : 1.4827295939127056\n",
      "SysABP : 6.0336156951433475\n",
      "Temp : 0.3292028265896774\n",
      "TroponinI : 0.9804100990285605\n",
      "TroponinT : 2.226862775921377\n",
      "Urine : 60.30476829484925\n",
      "WBC : 4.272635539372632\n",
      "Weight : 4.601422664097356\n",
      "Ph : 0.08153553565343111\n"
     ]
    }
   ],
   "source": [
    "print(\"SAITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_saits_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saits_mae_standard = toolkits.create_table(testing_mae_saits_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>General</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Undefined Gender</th>\n",
       "      <th>+65</th>\n",
       "      <th>-65</th>\n",
       "      <th>ICUType 1</th>\n",
       "      <th>ICUType 2</th>\n",
       "      <th>ICUType 3</th>\n",
       "      <th>ICUType 4</th>\n",
       "      <th>Undefined classification</th>\n",
       "      <th>Low Weight</th>\n",
       "      <th>Normal Weight</th>\n",
       "      <th>Overweight</th>\n",
       "      <th>Obesity 1</th>\n",
       "      <th>Obesity 2</th>\n",
       "      <th>Obesity 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALP</td>\n",
       "      <td>37.782973</td>\n",
       "      <td>45.908010</td>\n",
       "      <td>44.232028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.582313</td>\n",
       "      <td>90.367758</td>\n",
       "      <td>26.574981</td>\n",
       "      <td>22.825367</td>\n",
       "      <td>86.875150</td>\n",
       "      <td>56.892576</td>\n",
       "      <td>35.112586</td>\n",
       "      <td>60.076765</td>\n",
       "      <td>38.924773</td>\n",
       "      <td>27.006956</td>\n",
       "      <td>21.614137</td>\n",
       "      <td>35.656461</td>\n",
       "      <td>21.327258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALT</td>\n",
       "      <td>270.406347</td>\n",
       "      <td>183.764102</td>\n",
       "      <td>176.910857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>195.893023</td>\n",
       "      <td>247.790016</td>\n",
       "      <td>290.689956</td>\n",
       "      <td>166.770505</td>\n",
       "      <td>187.666144</td>\n",
       "      <td>199.008963</td>\n",
       "      <td>233.592981</td>\n",
       "      <td>117.087886</td>\n",
       "      <td>152.725655</td>\n",
       "      <td>188.666646</td>\n",
       "      <td>197.012515</td>\n",
       "      <td>218.366870</td>\n",
       "      <td>376.077477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AST</td>\n",
       "      <td>400.737893</td>\n",
       "      <td>395.870124</td>\n",
       "      <td>292.496996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>231.160547</td>\n",
       "      <td>371.550484</td>\n",
       "      <td>660.623368</td>\n",
       "      <td>213.677841</td>\n",
       "      <td>392.764369</td>\n",
       "      <td>469.255462</td>\n",
       "      <td>346.102288</td>\n",
       "      <td>1023.089504</td>\n",
       "      <td>197.998634</td>\n",
       "      <td>522.010286</td>\n",
       "      <td>329.881948</td>\n",
       "      <td>43.184342</td>\n",
       "      <td>137.905937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albumin</td>\n",
       "      <td>0.376303</td>\n",
       "      <td>0.414369</td>\n",
       "      <td>0.418830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329526</td>\n",
       "      <td>0.422010</td>\n",
       "      <td>0.384738</td>\n",
       "      <td>0.486044</td>\n",
       "      <td>0.438151</td>\n",
       "      <td>0.446954</td>\n",
       "      <td>0.449860</td>\n",
       "      <td>0.652044</td>\n",
       "      <td>0.521462</td>\n",
       "      <td>0.511398</td>\n",
       "      <td>0.385117</td>\n",
       "      <td>0.343609</td>\n",
       "      <td>0.265561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUN</td>\n",
       "      <td>5.041923</td>\n",
       "      <td>5.060138</td>\n",
       "      <td>4.804880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.125648</td>\n",
       "      <td>5.193089</td>\n",
       "      <td>4.919051</td>\n",
       "      <td>4.029728</td>\n",
       "      <td>6.121344</td>\n",
       "      <td>4.160634</td>\n",
       "      <td>5.540460</td>\n",
       "      <td>5.120281</td>\n",
       "      <td>4.822348</td>\n",
       "      <td>4.370578</td>\n",
       "      <td>4.105110</td>\n",
       "      <td>6.027658</td>\n",
       "      <td>5.077937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bilirubin</td>\n",
       "      <td>1.462854</td>\n",
       "      <td>1.715884</td>\n",
       "      <td>1.350716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.662760</td>\n",
       "      <td>1.732945</td>\n",
       "      <td>1.693838</td>\n",
       "      <td>1.430183</td>\n",
       "      <td>1.771845</td>\n",
       "      <td>1.587349</td>\n",
       "      <td>1.835563</td>\n",
       "      <td>4.056222</td>\n",
       "      <td>1.963607</td>\n",
       "      <td>1.194848</td>\n",
       "      <td>1.646914</td>\n",
       "      <td>1.072857</td>\n",
       "      <td>1.600829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cholesterol</td>\n",
       "      <td>40.126225</td>\n",
       "      <td>57.474820</td>\n",
       "      <td>36.346437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.275347</td>\n",
       "      <td>51.160950</td>\n",
       "      <td>40.131332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.187933</td>\n",
       "      <td>29.733724</td>\n",
       "      <td>29.371134</td>\n",
       "      <td>0.580627</td>\n",
       "      <td>36.292580</td>\n",
       "      <td>48.568815</td>\n",
       "      <td>25.462769</td>\n",
       "      <td>4.860565</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Creatinine</td>\n",
       "      <td>0.306942</td>\n",
       "      <td>0.245940</td>\n",
       "      <td>0.286378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253128</td>\n",
       "      <td>0.292305</td>\n",
       "      <td>0.349482</td>\n",
       "      <td>0.261187</td>\n",
       "      <td>0.372952</td>\n",
       "      <td>0.247486</td>\n",
       "      <td>0.298109</td>\n",
       "      <td>0.201534</td>\n",
       "      <td>0.321650</td>\n",
       "      <td>0.223180</td>\n",
       "      <td>0.271594</td>\n",
       "      <td>0.234064</td>\n",
       "      <td>0.242901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DiasABP</td>\n",
       "      <td>2.770457</td>\n",
       "      <td>2.758157</td>\n",
       "      <td>2.724575</td>\n",
       "      <td>2.625834</td>\n",
       "      <td>2.801946</td>\n",
       "      <td>2.674256</td>\n",
       "      <td>3.191496</td>\n",
       "      <td>2.415311</td>\n",
       "      <td>2.747072</td>\n",
       "      <td>2.711476</td>\n",
       "      <td>2.787839</td>\n",
       "      <td>3.141755</td>\n",
       "      <td>2.595952</td>\n",
       "      <td>2.780184</td>\n",
       "      <td>2.668520</td>\n",
       "      <td>2.909210</td>\n",
       "      <td>2.375226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FiO2</td>\n",
       "      <td>0.086210</td>\n",
       "      <td>0.084405</td>\n",
       "      <td>0.090973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087709</td>\n",
       "      <td>0.083642</td>\n",
       "      <td>0.094766</td>\n",
       "      <td>0.083189</td>\n",
       "      <td>0.085608</td>\n",
       "      <td>0.081931</td>\n",
       "      <td>0.082010</td>\n",
       "      <td>0.084822</td>\n",
       "      <td>0.084759</td>\n",
       "      <td>0.083246</td>\n",
       "      <td>0.085762</td>\n",
       "      <td>0.086346</td>\n",
       "      <td>0.101142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GCS</td>\n",
       "      <td>0.956741</td>\n",
       "      <td>0.920966</td>\n",
       "      <td>0.984219</td>\n",
       "      <td>0.435236</td>\n",
       "      <td>0.968572</td>\n",
       "      <td>0.959165</td>\n",
       "      <td>0.846239</td>\n",
       "      <td>1.292704</td>\n",
       "      <td>0.894139</td>\n",
       "      <td>0.924607</td>\n",
       "      <td>0.826900</td>\n",
       "      <td>1.008743</td>\n",
       "      <td>1.051797</td>\n",
       "      <td>1.072412</td>\n",
       "      <td>0.885874</td>\n",
       "      <td>1.146778</td>\n",
       "      <td>1.089805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Glucose</td>\n",
       "      <td>35.119314</td>\n",
       "      <td>37.631797</td>\n",
       "      <td>35.647707</td>\n",
       "      <td>2.991394</td>\n",
       "      <td>36.507555</td>\n",
       "      <td>37.553926</td>\n",
       "      <td>41.425353</td>\n",
       "      <td>30.978811</td>\n",
       "      <td>46.494443</td>\n",
       "      <td>29.069654</td>\n",
       "      <td>39.128891</td>\n",
       "      <td>56.658484</td>\n",
       "      <td>36.099172</td>\n",
       "      <td>36.846976</td>\n",
       "      <td>31.635650</td>\n",
       "      <td>42.290347</td>\n",
       "      <td>37.529282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HCO3</td>\n",
       "      <td>2.001525</td>\n",
       "      <td>1.957351</td>\n",
       "      <td>1.940448</td>\n",
       "      <td>1.062696</td>\n",
       "      <td>2.089967</td>\n",
       "      <td>2.081854</td>\n",
       "      <td>2.067192</td>\n",
       "      <td>1.742506</td>\n",
       "      <td>2.371724</td>\n",
       "      <td>1.904547</td>\n",
       "      <td>2.197427</td>\n",
       "      <td>2.678804</td>\n",
       "      <td>2.013053</td>\n",
       "      <td>1.895998</td>\n",
       "      <td>1.763823</td>\n",
       "      <td>1.554679</td>\n",
       "      <td>2.216592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HCT</td>\n",
       "      <td>2.674867</td>\n",
       "      <td>2.763354</td>\n",
       "      <td>2.704304</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.616256</td>\n",
       "      <td>2.643696</td>\n",
       "      <td>2.547245</td>\n",
       "      <td>2.602555</td>\n",
       "      <td>2.860921</td>\n",
       "      <td>2.607815</td>\n",
       "      <td>2.821977</td>\n",
       "      <td>2.763003</td>\n",
       "      <td>2.426845</td>\n",
       "      <td>2.746381</td>\n",
       "      <td>2.597252</td>\n",
       "      <td>2.522367</td>\n",
       "      <td>2.874967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HR</td>\n",
       "      <td>4.738259</td>\n",
       "      <td>4.914148</td>\n",
       "      <td>4.762584</td>\n",
       "      <td>4.627160</td>\n",
       "      <td>4.727897</td>\n",
       "      <td>4.801883</td>\n",
       "      <td>4.496199</td>\n",
       "      <td>4.114589</td>\n",
       "      <td>4.962478</td>\n",
       "      <td>5.103671</td>\n",
       "      <td>4.915642</td>\n",
       "      <td>5.314518</td>\n",
       "      <td>4.664727</td>\n",
       "      <td>4.448980</td>\n",
       "      <td>4.602975</td>\n",
       "      <td>4.734386</td>\n",
       "      <td>4.492783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>K</td>\n",
       "      <td>0.402721</td>\n",
       "      <td>0.426318</td>\n",
       "      <td>0.367062</td>\n",
       "      <td>0.260834</td>\n",
       "      <td>0.362351</td>\n",
       "      <td>0.393935</td>\n",
       "      <td>0.394968</td>\n",
       "      <td>0.395896</td>\n",
       "      <td>0.457274</td>\n",
       "      <td>0.329981</td>\n",
       "      <td>0.389275</td>\n",
       "      <td>0.437665</td>\n",
       "      <td>0.393982</td>\n",
       "      <td>0.395534</td>\n",
       "      <td>0.370609</td>\n",
       "      <td>0.347520</td>\n",
       "      <td>0.371795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lactate</td>\n",
       "      <td>1.125868</td>\n",
       "      <td>1.084236</td>\n",
       "      <td>1.201399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992557</td>\n",
       "      <td>1.133730</td>\n",
       "      <td>1.552902</td>\n",
       "      <td>1.141251</td>\n",
       "      <td>1.380770</td>\n",
       "      <td>1.174110</td>\n",
       "      <td>1.190927</td>\n",
       "      <td>0.935069</td>\n",
       "      <td>0.900027</td>\n",
       "      <td>1.327253</td>\n",
       "      <td>1.123491</td>\n",
       "      <td>1.286195</td>\n",
       "      <td>1.052226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MAP</td>\n",
       "      <td>3.003518</td>\n",
       "      <td>3.325971</td>\n",
       "      <td>3.214141</td>\n",
       "      <td>4.170712</td>\n",
       "      <td>3.251531</td>\n",
       "      <td>3.147767</td>\n",
       "      <td>4.181648</td>\n",
       "      <td>2.685739</td>\n",
       "      <td>3.201426</td>\n",
       "      <td>3.166017</td>\n",
       "      <td>3.371506</td>\n",
       "      <td>3.290570</td>\n",
       "      <td>2.929931</td>\n",
       "      <td>3.180077</td>\n",
       "      <td>2.989624</td>\n",
       "      <td>2.998582</td>\n",
       "      <td>2.582291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MechVent</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>0.011828</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012487</td>\n",
       "      <td>0.012519</td>\n",
       "      <td>0.010792</td>\n",
       "      <td>0.013277</td>\n",
       "      <td>0.012368</td>\n",
       "      <td>0.012251</td>\n",
       "      <td>0.011504</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.013288</td>\n",
       "      <td>0.012850</td>\n",
       "      <td>0.012876</td>\n",
       "      <td>0.011038</td>\n",
       "      <td>0.013644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mg</td>\n",
       "      <td>0.267380</td>\n",
       "      <td>0.255944</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.109033</td>\n",
       "      <td>0.256143</td>\n",
       "      <td>0.252319</td>\n",
       "      <td>0.220786</td>\n",
       "      <td>0.289643</td>\n",
       "      <td>0.282023</td>\n",
       "      <td>0.261054</td>\n",
       "      <td>0.234725</td>\n",
       "      <td>0.258044</td>\n",
       "      <td>0.283988</td>\n",
       "      <td>0.249447</td>\n",
       "      <td>0.219864</td>\n",
       "      <td>0.294299</td>\n",
       "      <td>0.267949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NIDiasABP</td>\n",
       "      <td>2.679145</td>\n",
       "      <td>2.841792</td>\n",
       "      <td>2.945321</td>\n",
       "      <td>4.463973</td>\n",
       "      <td>2.811189</td>\n",
       "      <td>2.914207</td>\n",
       "      <td>2.579274</td>\n",
       "      <td>2.463517</td>\n",
       "      <td>2.845548</td>\n",
       "      <td>3.030447</td>\n",
       "      <td>2.849974</td>\n",
       "      <td>3.874066</td>\n",
       "      <td>2.788348</td>\n",
       "      <td>2.578352</td>\n",
       "      <td>2.524729</td>\n",
       "      <td>2.791860</td>\n",
       "      <td>3.726555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NIMAP</td>\n",
       "      <td>2.201185</td>\n",
       "      <td>2.252777</td>\n",
       "      <td>2.216463</td>\n",
       "      <td>1.535179</td>\n",
       "      <td>2.258073</td>\n",
       "      <td>2.160745</td>\n",
       "      <td>2.137044</td>\n",
       "      <td>2.063491</td>\n",
       "      <td>2.264650</td>\n",
       "      <td>2.292931</td>\n",
       "      <td>2.241398</td>\n",
       "      <td>2.858565</td>\n",
       "      <td>2.138601</td>\n",
       "      <td>2.365936</td>\n",
       "      <td>2.117654</td>\n",
       "      <td>1.888972</td>\n",
       "      <td>3.028013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NISysABP</td>\n",
       "      <td>5.161662</td>\n",
       "      <td>5.244812</td>\n",
       "      <td>5.347620</td>\n",
       "      <td>2.865608</td>\n",
       "      <td>5.352269</td>\n",
       "      <td>4.668975</td>\n",
       "      <td>4.983266</td>\n",
       "      <td>5.080965</td>\n",
       "      <td>5.157940</td>\n",
       "      <td>5.279488</td>\n",
       "      <td>5.224309</td>\n",
       "      <td>4.263397</td>\n",
       "      <td>4.781441</td>\n",
       "      <td>5.104423</td>\n",
       "      <td>5.049840</td>\n",
       "      <td>4.795166</td>\n",
       "      <td>5.978001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Na</td>\n",
       "      <td>2.188270</td>\n",
       "      <td>2.245776</td>\n",
       "      <td>2.302589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.089944</td>\n",
       "      <td>2.450650</td>\n",
       "      <td>2.075893</td>\n",
       "      <td>2.215121</td>\n",
       "      <td>2.180627</td>\n",
       "      <td>2.324010</td>\n",
       "      <td>2.615632</td>\n",
       "      <td>1.795045</td>\n",
       "      <td>2.483942</td>\n",
       "      <td>2.172987</td>\n",
       "      <td>2.409926</td>\n",
       "      <td>1.704502</td>\n",
       "      <td>1.838859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PaCO2</td>\n",
       "      <td>4.549573</td>\n",
       "      <td>4.606224</td>\n",
       "      <td>4.291512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.343092</td>\n",
       "      <td>4.591567</td>\n",
       "      <td>4.145446</td>\n",
       "      <td>4.128036</td>\n",
       "      <td>5.130343</td>\n",
       "      <td>4.241135</td>\n",
       "      <td>5.417221</td>\n",
       "      <td>4.940311</td>\n",
       "      <td>4.288689</td>\n",
       "      <td>4.334908</td>\n",
       "      <td>4.251965</td>\n",
       "      <td>4.055283</td>\n",
       "      <td>4.932598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PaO2</td>\n",
       "      <td>40.331332</td>\n",
       "      <td>39.037169</td>\n",
       "      <td>37.619689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.536282</td>\n",
       "      <td>39.392599</td>\n",
       "      <td>40.291089</td>\n",
       "      <td>37.853490</td>\n",
       "      <td>37.342359</td>\n",
       "      <td>39.907105</td>\n",
       "      <td>41.267539</td>\n",
       "      <td>47.980384</td>\n",
       "      <td>39.582614</td>\n",
       "      <td>37.949899</td>\n",
       "      <td>35.947168</td>\n",
       "      <td>29.575632</td>\n",
       "      <td>30.752426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Platelets</td>\n",
       "      <td>37.037300</td>\n",
       "      <td>39.004491</td>\n",
       "      <td>34.583569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.193244</td>\n",
       "      <td>35.455004</td>\n",
       "      <td>33.261488</td>\n",
       "      <td>33.771961</td>\n",
       "      <td>41.010382</td>\n",
       "      <td>34.856355</td>\n",
       "      <td>39.687462</td>\n",
       "      <td>42.662219</td>\n",
       "      <td>33.094973</td>\n",
       "      <td>38.284567</td>\n",
       "      <td>33.327148</td>\n",
       "      <td>39.008212</td>\n",
       "      <td>26.512690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RespRate</td>\n",
       "      <td>2.505294</td>\n",
       "      <td>2.517871</td>\n",
       "      <td>2.506047</td>\n",
       "      <td>0.814829</td>\n",
       "      <td>2.490610</td>\n",
       "      <td>2.481608</td>\n",
       "      <td>2.493705</td>\n",
       "      <td>2.180743</td>\n",
       "      <td>2.692868</td>\n",
       "      <td>2.566266</td>\n",
       "      <td>2.592695</td>\n",
       "      <td>3.817650</td>\n",
       "      <td>2.343235</td>\n",
       "      <td>2.374905</td>\n",
       "      <td>2.482373</td>\n",
       "      <td>2.612609</td>\n",
       "      <td>2.362217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SaO2</td>\n",
       "      <td>1.319394</td>\n",
       "      <td>2.015491</td>\n",
       "      <td>1.250796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.328877</td>\n",
       "      <td>1.924457</td>\n",
       "      <td>1.924623</td>\n",
       "      <td>1.182917</td>\n",
       "      <td>5.254021</td>\n",
       "      <td>1.749085</td>\n",
       "      <td>2.390089</td>\n",
       "      <td>0.738976</td>\n",
       "      <td>1.213414</td>\n",
       "      <td>1.243274</td>\n",
       "      <td>1.420091</td>\n",
       "      <td>1.538900</td>\n",
       "      <td>1.482730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SysABP</td>\n",
       "      <td>5.728127</td>\n",
       "      <td>5.520486</td>\n",
       "      <td>5.806736</td>\n",
       "      <td>4.187952</td>\n",
       "      <td>5.601980</td>\n",
       "      <td>5.290973</td>\n",
       "      <td>5.607141</td>\n",
       "      <td>5.403056</td>\n",
       "      <td>5.909496</td>\n",
       "      <td>5.980944</td>\n",
       "      <td>5.931407</td>\n",
       "      <td>5.375160</td>\n",
       "      <td>5.304672</td>\n",
       "      <td>5.663931</td>\n",
       "      <td>5.211509</td>\n",
       "      <td>5.401177</td>\n",
       "      <td>6.033616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Temp</td>\n",
       "      <td>0.383443</td>\n",
       "      <td>0.434899</td>\n",
       "      <td>0.421703</td>\n",
       "      <td>0.494359</td>\n",
       "      <td>0.413382</td>\n",
       "      <td>0.447678</td>\n",
       "      <td>0.455832</td>\n",
       "      <td>0.263146</td>\n",
       "      <td>0.590800</td>\n",
       "      <td>0.504958</td>\n",
       "      <td>0.492677</td>\n",
       "      <td>0.301582</td>\n",
       "      <td>0.360890</td>\n",
       "      <td>0.295384</td>\n",
       "      <td>0.294640</td>\n",
       "      <td>0.367122</td>\n",
       "      <td>0.329203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TroponinI</td>\n",
       "      <td>8.488084</td>\n",
       "      <td>7.328217</td>\n",
       "      <td>10.248831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.069289</td>\n",
       "      <td>9.179028</td>\n",
       "      <td>14.876962</td>\n",
       "      <td>3.521959</td>\n",
       "      <td>5.491749</td>\n",
       "      <td>12.918012</td>\n",
       "      <td>9.966343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.106811</td>\n",
       "      <td>10.654041</td>\n",
       "      <td>6.433475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.980410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TroponinT</td>\n",
       "      <td>1.015439</td>\n",
       "      <td>0.946097</td>\n",
       "      <td>1.275769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.917499</td>\n",
       "      <td>1.368828</td>\n",
       "      <td>1.488741</td>\n",
       "      <td>1.023006</td>\n",
       "      <td>0.783291</td>\n",
       "      <td>0.582014</td>\n",
       "      <td>0.892004</td>\n",
       "      <td>0.203443</td>\n",
       "      <td>0.838277</td>\n",
       "      <td>1.403120</td>\n",
       "      <td>0.894015</td>\n",
       "      <td>2.306611</td>\n",
       "      <td>2.226863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Urine</td>\n",
       "      <td>58.692474</td>\n",
       "      <td>53.734920</td>\n",
       "      <td>62.775168</td>\n",
       "      <td>57.703873</td>\n",
       "      <td>52.886834</td>\n",
       "      <td>67.501502</td>\n",
       "      <td>70.168023</td>\n",
       "      <td>56.088924</td>\n",
       "      <td>58.986691</td>\n",
       "      <td>61.085766</td>\n",
       "      <td>61.266943</td>\n",
       "      <td>63.737342</td>\n",
       "      <td>53.681144</td>\n",
       "      <td>54.229617</td>\n",
       "      <td>63.746832</td>\n",
       "      <td>57.001590</td>\n",
       "      <td>60.304768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WBC</td>\n",
       "      <td>5.291650</td>\n",
       "      <td>5.190010</td>\n",
       "      <td>5.842828</td>\n",
       "      <td>0.885886</td>\n",
       "      <td>5.750450</td>\n",
       "      <td>5.327957</td>\n",
       "      <td>3.992155</td>\n",
       "      <td>4.289541</td>\n",
       "      <td>6.054897</td>\n",
       "      <td>4.190956</td>\n",
       "      <td>6.259356</td>\n",
       "      <td>6.064987</td>\n",
       "      <td>5.259297</td>\n",
       "      <td>4.549789</td>\n",
       "      <td>6.866182</td>\n",
       "      <td>4.337441</td>\n",
       "      <td>4.272636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Weight</td>\n",
       "      <td>2.656606</td>\n",
       "      <td>2.557225</td>\n",
       "      <td>2.481717</td>\n",
       "      <td>3.159104</td>\n",
       "      <td>2.306250</td>\n",
       "      <td>2.913584</td>\n",
       "      <td>2.640161</td>\n",
       "      <td>2.810956</td>\n",
       "      <td>2.417315</td>\n",
       "      <td>2.736775</td>\n",
       "      <td>2.591100</td>\n",
       "      <td>3.280227</td>\n",
       "      <td>2.228673</td>\n",
       "      <td>2.042630</td>\n",
       "      <td>2.288716</td>\n",
       "      <td>2.665547</td>\n",
       "      <td>4.601423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ph</td>\n",
       "      <td>0.148843</td>\n",
       "      <td>0.084064</td>\n",
       "      <td>0.085235</td>\n",
       "      <td>0.041328</td>\n",
       "      <td>0.082077</td>\n",
       "      <td>0.089872</td>\n",
       "      <td>0.087779</td>\n",
       "      <td>0.079640</td>\n",
       "      <td>0.091597</td>\n",
       "      <td>0.079333</td>\n",
       "      <td>0.091346</td>\n",
       "      <td>0.088288</td>\n",
       "      <td>0.080659</td>\n",
       "      <td>0.076851</td>\n",
       "      <td>1.077841</td>\n",
       "      <td>0.062033</td>\n",
       "      <td>0.081536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     General      Female        Male  Undefined Gender  \\\n",
       "0           ALP   37.782973   45.908010   44.232028          0.000000   \n",
       "1           ALT  270.406347  183.764102  176.910857          0.000000   \n",
       "2           AST  400.737893  395.870124  292.496996          0.000000   \n",
       "3       Albumin    0.376303    0.414369    0.418830          0.000000   \n",
       "4           BUN    5.041923    5.060138    4.804880          0.000000   \n",
       "5     Bilirubin    1.462854    1.715884    1.350716          0.000000   \n",
       "6   Cholesterol   40.126225   57.474820   36.346437          0.000000   \n",
       "7    Creatinine    0.306942    0.245940    0.286378          0.000000   \n",
       "8       DiasABP    2.770457    2.758157    2.724575          2.625834   \n",
       "9          FiO2    0.086210    0.084405    0.090973          0.000000   \n",
       "10          GCS    0.956741    0.920966    0.984219          0.435236   \n",
       "11      Glucose   35.119314   37.631797   35.647707          2.991394   \n",
       "12         HCO3    2.001525    1.957351    1.940448          1.062696   \n",
       "13          HCT    2.674867    2.763354    2.704304          0.000000   \n",
       "14           HR    4.738259    4.914148    4.762584          4.627160   \n",
       "15            K    0.402721    0.426318    0.367062          0.260834   \n",
       "16      Lactate    1.125868    1.084236    1.201399          0.000000   \n",
       "17          MAP    3.003518    3.325971    3.214141          4.170712   \n",
       "18     MechVent    0.012463    0.011828    0.013049          0.000000   \n",
       "19           Mg    0.267380    0.255944    0.266500          0.109033   \n",
       "20    NIDiasABP    2.679145    2.841792    2.945321          4.463973   \n",
       "21        NIMAP    2.201185    2.252777    2.216463          1.535179   \n",
       "22     NISysABP    5.161662    5.244812    5.347620          2.865608   \n",
       "23           Na    2.188270    2.245776    2.302589          0.000000   \n",
       "24        PaCO2    4.549573    4.606224    4.291512          0.000000   \n",
       "25         PaO2   40.331332   39.037169   37.619689          0.000000   \n",
       "26    Platelets   37.037300   39.004491   34.583569          0.000000   \n",
       "27     RespRate    2.505294    2.517871    2.506047          0.814829   \n",
       "28         SaO2    1.319394    2.015491    1.250796          0.000000   \n",
       "29       SysABP    5.728127    5.520486    5.806736          4.187952   \n",
       "30         Temp    0.383443    0.434899    0.421703          0.494359   \n",
       "31    TroponinI    8.488084    7.328217   10.248831          0.000000   \n",
       "32    TroponinT    1.015439    0.946097    1.275769          0.000000   \n",
       "33        Urine   58.692474   53.734920   62.775168         57.703873   \n",
       "34          WBC    5.291650    5.190010    5.842828          0.885886   \n",
       "35       Weight    2.656606    2.557225    2.481717          3.159104   \n",
       "36           Ph    0.148843    0.084064    0.085235          0.041328   \n",
       "\n",
       "           +65         -65   ICUType 1   ICUType 2   ICUType 3   ICUType 4  \\\n",
       "0    37.582313   90.367758   26.574981   22.825367   86.875150   56.892576   \n",
       "1   195.893023  247.790016  290.689956  166.770505  187.666144  199.008963   \n",
       "2   231.160547  371.550484  660.623368  213.677841  392.764369  469.255462   \n",
       "3     0.329526    0.422010    0.384738    0.486044    0.438151    0.446954   \n",
       "4     5.125648    5.193089    4.919051    4.029728    6.121344    4.160634   \n",
       "5     1.662760    1.732945    1.693838    1.430183    1.771845    1.587349   \n",
       "6    34.275347   51.160950   40.131332    0.000000   41.187933   29.733724   \n",
       "7     0.253128    0.292305    0.349482    0.261187    0.372952    0.247486   \n",
       "8     2.801946    2.674256    3.191496    2.415311    2.747072    2.711476   \n",
       "9     0.087709    0.083642    0.094766    0.083189    0.085608    0.081931   \n",
       "10    0.968572    0.959165    0.846239    1.292704    0.894139    0.924607   \n",
       "11   36.507555   37.553926   41.425353   30.978811   46.494443   29.069654   \n",
       "12    2.089967    2.081854    2.067192    1.742506    2.371724    1.904547   \n",
       "13    2.616256    2.643696    2.547245    2.602555    2.860921    2.607815   \n",
       "14    4.727897    4.801883    4.496199    4.114589    4.962478    5.103671   \n",
       "15    0.362351    0.393935    0.394968    0.395896    0.457274    0.329981   \n",
       "16    0.992557    1.133730    1.552902    1.141251    1.380770    1.174110   \n",
       "17    3.251531    3.147767    4.181648    2.685739    3.201426    3.166017   \n",
       "18    0.012487    0.012519    0.010792    0.013277    0.012368    0.012251   \n",
       "19    0.256143    0.252319    0.220786    0.289643    0.282023    0.261054   \n",
       "20    2.811189    2.914207    2.579274    2.463517    2.845548    3.030447   \n",
       "21    2.258073    2.160745    2.137044    2.063491    2.264650    2.292931   \n",
       "22    5.352269    4.668975    4.983266    5.080965    5.157940    5.279488   \n",
       "23    2.089944    2.450650    2.075893    2.215121    2.180627    2.324010   \n",
       "24    4.343092    4.591567    4.145446    4.128036    5.130343    4.241135   \n",
       "25   34.536282   39.392599   40.291089   37.853490   37.342359   39.907105   \n",
       "26   38.193244   35.455004   33.261488   33.771961   41.010382   34.856355   \n",
       "27    2.490610    2.481608    2.493705    2.180743    2.692868    2.566266   \n",
       "28    1.328877    1.924457    1.924623    1.182917    5.254021    1.749085   \n",
       "29    5.601980    5.290973    5.607141    5.403056    5.909496    5.980944   \n",
       "30    0.413382    0.447678    0.455832    0.263146    0.590800    0.504958   \n",
       "31   10.069289    9.179028   14.876962    3.521959    5.491749   12.918012   \n",
       "32    0.917499    1.368828    1.488741    1.023006    0.783291    0.582014   \n",
       "33   52.886834   67.501502   70.168023   56.088924   58.986691   61.085766   \n",
       "34    5.750450    5.327957    3.992155    4.289541    6.054897    4.190956   \n",
       "35    2.306250    2.913584    2.640161    2.810956    2.417315    2.736775   \n",
       "36    0.082077    0.089872    0.087779    0.079640    0.091597    0.079333   \n",
       "\n",
       "    Undefined classification   Low Weight  Normal Weight  Overweight  \\\n",
       "0                  35.112586    60.076765      38.924773   27.006956   \n",
       "1                 233.592981   117.087886     152.725655  188.666646   \n",
       "2                 346.102288  1023.089504     197.998634  522.010286   \n",
       "3                   0.449860     0.652044       0.521462    0.511398   \n",
       "4                   5.540460     5.120281       4.822348    4.370578   \n",
       "5                   1.835563     4.056222       1.963607    1.194848   \n",
       "6                  29.371134     0.580627      36.292580   48.568815   \n",
       "7                   0.298109     0.201534       0.321650    0.223180   \n",
       "8                   2.787839     3.141755       2.595952    2.780184   \n",
       "9                   0.082010     0.084822       0.084759    0.083246   \n",
       "10                  0.826900     1.008743       1.051797    1.072412   \n",
       "11                 39.128891    56.658484      36.099172   36.846976   \n",
       "12                  2.197427     2.678804       2.013053    1.895998   \n",
       "13                  2.821977     2.763003       2.426845    2.746381   \n",
       "14                  4.915642     5.314518       4.664727    4.448980   \n",
       "15                  0.389275     0.437665       0.393982    0.395534   \n",
       "16                  1.190927     0.935069       0.900027    1.327253   \n",
       "17                  3.371506     3.290570       2.929931    3.180077   \n",
       "18                  0.011504     0.012912       0.013288    0.012850   \n",
       "19                  0.234725     0.258044       0.283988    0.249447   \n",
       "20                  2.849974     3.874066       2.788348    2.578352   \n",
       "21                  2.241398     2.858565       2.138601    2.365936   \n",
       "22                  5.224309     4.263397       4.781441    5.104423   \n",
       "23                  2.615632     1.795045       2.483942    2.172987   \n",
       "24                  5.417221     4.940311       4.288689    4.334908   \n",
       "25                 41.267539    47.980384      39.582614   37.949899   \n",
       "26                 39.687462    42.662219      33.094973   38.284567   \n",
       "27                  2.592695     3.817650       2.343235    2.374905   \n",
       "28                  2.390089     0.738976       1.213414    1.243274   \n",
       "29                  5.931407     5.375160       5.304672    5.663931   \n",
       "30                  0.492677     0.301582       0.360890    0.295384   \n",
       "31                  9.966343     0.000000       9.106811   10.654041   \n",
       "32                  0.892004     0.203443       0.838277    1.403120   \n",
       "33                 61.266943    63.737342      53.681144   54.229617   \n",
       "34                  6.259356     6.064987       5.259297    4.549789   \n",
       "35                  2.591100     3.280227       2.228673    2.042630   \n",
       "36                  0.091346     0.088288       0.080659    0.076851   \n",
       "\n",
       "     Obesity 1   Obesity 2   Obesity 3  \n",
       "0    21.614137   35.656461   21.327258  \n",
       "1   197.012515  218.366870  376.077477  \n",
       "2   329.881948   43.184342  137.905937  \n",
       "3     0.385117    0.343609    0.265561  \n",
       "4     4.105110    6.027658    5.077937  \n",
       "5     1.646914    1.072857    1.600829  \n",
       "6    25.462769    4.860565    0.000000  \n",
       "7     0.271594    0.234064    0.242901  \n",
       "8     2.668520    2.909210    2.375226  \n",
       "9     0.085762    0.086346    0.101142  \n",
       "10    0.885874    1.146778    1.089805  \n",
       "11   31.635650   42.290347   37.529282  \n",
       "12    1.763823    1.554679    2.216592  \n",
       "13    2.597252    2.522367    2.874967  \n",
       "14    4.602975    4.734386    4.492783  \n",
       "15    0.370609    0.347520    0.371795  \n",
       "16    1.123491    1.286195    1.052226  \n",
       "17    2.989624    2.998582    2.582291  \n",
       "18    0.012876    0.011038    0.013644  \n",
       "19    0.219864    0.294299    0.267949  \n",
       "20    2.524729    2.791860    3.726555  \n",
       "21    2.117654    1.888972    3.028013  \n",
       "22    5.049840    4.795166    5.978001  \n",
       "23    2.409926    1.704502    1.838859  \n",
       "24    4.251965    4.055283    4.932598  \n",
       "25   35.947168   29.575632   30.752426  \n",
       "26   33.327148   39.008212   26.512690  \n",
       "27    2.482373    2.612609    2.362217  \n",
       "28    1.420091    1.538900    1.482730  \n",
       "29    5.211509    5.401177    6.033616  \n",
       "30    0.294640    0.367122    0.329203  \n",
       "31    6.433475    0.000000    0.980410  \n",
       "32    0.894015    2.306611    2.226863  \n",
       "33   63.746832   57.001590   60.304768  \n",
       "34    6.866182    4.337441    4.272636  \n",
       "35    2.288716    2.665547    4.601423  \n",
       "36    1.077841    0.062033    0.081536  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saits_mae_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "MechVent\n",
      "0.012462921181450714\n",
      "--------------------\n",
      "Female\n",
      "MechVent\n",
      "0.01182785045626394\n",
      "--------------------\n",
      "Male\n",
      "MechVent\n",
      "0.013049005193913223\n",
      "--------------------\n",
      "Undefined Gender\n",
      "TroponinT\n",
      "0.0\n",
      "--------------------\n",
      "+65\n",
      "MechVent\n",
      "0.01248656810672432\n",
      "--------------------\n",
      "-65\n",
      "MechVent\n",
      "0.012519296893875705\n",
      "--------------------\n",
      "ICUType 1\n",
      "MechVent\n",
      "0.010791735683413642\n",
      "--------------------\n",
      "ICUType 2\n",
      "Cholesterol\n",
      "0.0\n",
      "--------------------\n",
      "ICUType 3\n",
      "MechVent\n",
      "0.012368280552701387\n",
      "--------------------\n",
      "ICUType 4\n",
      "MechVent\n",
      "0.012251276172731964\n",
      "--------------------\n",
      "Undefined classification\n",
      "MechVent\n",
      "0.01150444749213414\n",
      "--------------------\n",
      "Low Weight\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Normal Weight\n",
      "MechVent\n",
      "0.013287598918552006\n",
      "--------------------\n",
      "Overweight\n",
      "MechVent\n",
      "0.012850226540314485\n",
      "--------------------\n",
      "Obesity 1\n",
      "MechVent\n",
      "0.012875867812014346\n",
      "--------------------\n",
      "Obesity 2\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Obesity 3\n",
      "Cholesterol\n",
      "0.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.min_value_in_subgroup(df_saits_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "AST\n",
      "400.73789268472706\n",
      "--------------------\n",
      "Female\n",
      "AST\n",
      "395.8701238334127\n",
      "--------------------\n",
      "Male\n",
      "AST\n",
      "292.49699600095835\n",
      "--------------------\n",
      "Undefined Gender\n",
      "Urine\n",
      "57.70387268060635\n",
      "--------------------\n",
      "+65\n",
      "AST\n",
      "231.16054653347015\n",
      "--------------------\n",
      "-65\n",
      "AST\n",
      "371.5504835545984\n",
      "--------------------\n",
      "ICUType 1\n",
      "AST\n",
      "660.6233684539532\n",
      "--------------------\n",
      "ICUType 2\n",
      "AST\n",
      "213.67784124871028\n",
      "--------------------\n",
      "ICUType 3\n",
      "AST\n",
      "392.7643692165898\n",
      "--------------------\n",
      "ICUType 4\n",
      "AST\n",
      "469.25546223032705\n",
      "--------------------\n",
      "Undefined classification\n",
      "AST\n",
      "346.1022879801262\n",
      "--------------------\n",
      "Low Weight\n",
      "AST\n",
      "1023.0895042416876\n",
      "--------------------\n",
      "Normal Weight\n",
      "AST\n",
      "197.998634457582\n",
      "--------------------\n",
      "Overweight\n",
      "AST\n",
      "522.0102859678475\n",
      "--------------------\n",
      "Obesity 1\n",
      "AST\n",
      "329.8819475412204\n",
      "--------------------\n",
      "Obesity 2\n",
      "ALT\n",
      "218.36687033513905\n",
      "--------------------\n",
      "Obesity 3\n",
      "ALT\n",
      "376.07747713712234\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.max_value_in_subgroup(df_saits_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Standard Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_saits_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saits_mae_standard_ori = toolkits.create_table(testing_mae_saits_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saits_mae_standard_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_saits_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_saits_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAITS - MAE\n",
      "************\n",
      "General\n",
      "-------------\n",
      "ALP : 112.21354016661586\n",
      "ALT : 291.51042278607537\n",
      "AST : 562.9945693689814\n",
      "Albumin : 1.9510791413217858\n",
      "BUN : 24.78843811781201\n",
      "Bilirubin : 2.7703703989979305\n",
      "Cholesterol : 129.29999999999356\n",
      "Creatinine : 1.3927400290337868\n",
      "DiasABP : 60.684729736472974\n",
      "FiO2 : 0.3327910102481898\n",
      "GCS : 8.416553277655188\n",
      "Glucose : 131.70664855678496\n",
      "HCO3 : 17.99873046826588\n",
      "HCT : 25.109207015520656\n",
      "HR : 86.69561557043218\n",
      "K : 2.6361485824391617\n",
      "Lactate : 2.767346140115878\n",
      "MAP : 80.30970073528962\n",
      "MechVent : 0.0\n",
      "Mg : 2.011890264982128\n",
      "NIDiasABP : 59.44657435889726\n",
      "NIMAP : 77.73032076927599\n",
      "NISysABP : 119.4786514127118\n",
      "Na : 40.88944723618086\n",
      "PaCO2 : 40.95278192858044\n",
      "PaO2 : 148.29175956176658\n",
      "Platelets : 185.6867094356714\n",
      "RespRate : 19.431791012219442\n",
      "SaO2 : 96.8508583690985\n",
      "SysABP : 120.12315150566705\n",
      "Temp : 54.89088744652194\n",
      "TroponinI : 8.110000050067496\n",
      "TroponinT : 0.8610493570602916\n",
      "Urine : 116.09896736077816\n",
      "WBC : 12.26488773373189\n",
      "Weight : 83.7309656814315\n",
      "Ph : 7.43894935678957\n",
      "Female\n",
      "-------------\n",
      "ALP : 115.09589082900635\n",
      "ALT : 117.43662048393408\n",
      "AST : 621.6842090706995\n",
      "Albumin : 1.8231884120166904\n",
      "BUN : 23.924858906848215\n",
      "Bilirubin : 2.1516128864179147\n",
      "Cholesterol : 157.99999389645276\n",
      "Creatinine : 1.0732193701457402\n",
      "DiasABP : 60.40330312211117\n",
      "FiO2 : 0.3222841955806467\n",
      "GCS : 8.428689406604585\n",
      "Glucose : 131.76291745774247\n",
      "HCO3 : 18.208913231958533\n",
      "HCT : 24.984399137570826\n",
      "HR : 87.08707636694399\n",
      "K : 2.5514359704508105\n",
      "Lactate : 2.657635525235976\n",
      "MAP : 80.75017183196017\n",
      "MechVent : 0.0\n",
      "Mg : 1.9722071103244072\n",
      "NIDiasABP : 57.85700755225679\n",
      "NIMAP : 75.82649832001862\n",
      "NISysABP : 119.2597793553718\n",
      "Na : 40.9919999999999\n",
      "PaCO2 : 40.32464615074356\n",
      "PaO2 : 146.97227254526067\n",
      "Platelets : 200.68434699587172\n",
      "RespRate : 19.65318653605362\n",
      "SaO2 : 96.17415730337025\n",
      "SysABP : 120.34973163640119\n",
      "Temp : 54.79280200646471\n",
      "TroponinI : 4.880000060796251\n",
      "TroponinT : 1.0306451424679761\n",
      "Urine : 102.45429780167657\n",
      "WBC : 12.421652221355712\n",
      "Weight : 74.86809057753672\n",
      "Ph : 7.373614547519433\n",
      "Male\n",
      "-------------\n",
      "ALP : 119.04629590776122\n",
      "ALT : 280.3000011026836\n",
      "AST : 246.51852046118614\n",
      "Albumin : 1.9353658426098472\n",
      "BUN : 28.04109613318415\n",
      "Bilirubin : 2.242934758167528\n",
      "Cholesterol : 130.23076923075922\n",
      "Creatinine : 1.5857437889191697\n",
      "DiasABP : 61.02959244863548\n",
      "FiO2 : 0.3487641482926762\n",
      "GCS : 8.444232654753355\n",
      "Glucose : 136.02926779956317\n",
      "HCO3 : 18.40399953365322\n",
      "HCT : 25.231574729859318\n",
      "HR : 86.31227045082214\n",
      "K : 2.7116070716153953\n",
      "Lactate : 2.8974815479031206\n",
      "MAP : 80.10412163524481\n",
      "MechVent : 0.0\n",
      "Mg : 2.063424968316233\n",
      "NIDiasABP : 60.82939315657607\n",
      "NIMAP : 78.28881712967546\n",
      "NISysABP : 118.47662729107307\n",
      "Na : 40.774266365688405\n",
      "PaCO2 : 40.7799745433184\n",
      "PaO2 : 148.14498146463632\n",
      "Platelets : 180.03941588975545\n",
      "RespRate : 19.083588507450603\n",
      "SaO2 : 96.75698923910785\n",
      "SysABP : 120.8875130478721\n",
      "Temp : 54.904589557610294\n",
      "TroponinI : 5.433333490291325\n",
      "TroponinT : 1.1349999794677537\n",
      "Urine : 123.36716051185522\n",
      "WBC : 13.113277497046269\n",
      "Weight : 89.53703626779937\n",
      "Ph : 7.3761730448847915\n",
      "Undefined Gender\n",
      "-------------\n",
      "ALP : 0.0\n",
      "ALT : 0.0\n",
      "AST : 0.0\n",
      "Albumin : 0.0\n",
      "BUN : 0.0\n",
      "Bilirubin : 0.0\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.0\n",
      "DiasABP : 62.708333492271215\n",
      "FiO2 : 0.0\n",
      "GCS : 11.250000715252925\n",
      "Glucose : 127.99999999987199\n",
      "HCO3 : 19.499999999990248\n",
      "HCT : 0.0\n",
      "HR : 73.18749999999085\n",
      "K : 3.099999904629468\n",
      "Lactate : 0.0\n",
      "MAP : 75.999999999981\n",
      "MechVent : 0.0\n",
      "Mg : 1.799999952314484\n",
      "NIDiasABP : 63.12499999998422\n",
      "NIMAP : 79.33000183102824\n",
      "NISysABP : 96.49999999995174\n",
      "Na : 0.0\n",
      "PaCO2 : 0.0\n",
      "PaO2 : 0.0\n",
      "Platelets : 0.0\n",
      "RespRate : 19.749999999980247\n",
      "SaO2 : 0.0\n",
      "SysABP : 100.04166603087128\n",
      "Temp : 54.949999618502794\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.0\n",
      "Urine : 139.99999999986\n",
      "WBC : 10.849999904627143\n",
      "Weight : 100.56666310626579\n",
      "Ph : 7.46000051497667\n",
      "+65\n",
      "-------------\n",
      "ALP : 126.58333338631348\n",
      "ALT : 217.86812988218134\n",
      "AST : 235.4480554902678\n",
      "Albumin : 1.8438356281959596\n",
      "BUN : 30.607143059152612\n",
      "Bilirubin : 1.6344444332851122\n",
      "Cholesterol : 121.2499999999899\n",
      "Creatinine : 1.27203790727667\n",
      "DiasABP : 57.92333064940073\n",
      "FiO2 : 0.3409840135837527\n",
      "GCS : 8.52875323817323\n",
      "Glucose : 139.38356123623743\n",
      "HCO3 : 17.977832028431212\n",
      "HCT : 25.34283337593074\n",
      "HR : 83.74005648661858\n",
      "K : 2.6257610482968485\n",
      "Lactate : 2.640000080778473\n",
      "MAP : 78.41003617123543\n",
      "MechVent : 0.0\n",
      "Mg : 2.0671328920306533\n",
      "NIDiasABP : 55.84132360925104\n",
      "NIMAP : 74.70098286153004\n",
      "NISysABP : 119.29339133907601\n",
      "Na : 41.10421547454182\n",
      "PaCO2 : 39.72002827599954\n",
      "PaO2 : 147.65793416600005\n",
      "Platelets : 184.23395111842146\n",
      "RespRate : 19.995240901071856\n",
      "SaO2 : 96.75195727908279\n",
      "SysABP : 120.6963129861179\n",
      "Temp : 54.79190322092852\n",
      "TroponinI : 12.825000001985753\n",
      "TroponinT : 0.9496052598208065\n",
      "Urine : 101.87709531103178\n",
      "WBC : 13.70614814802449\n",
      "Weight : 78.83629935178334\n",
      "Ph : 7.375993523525159\n",
      "-65\n",
      "-------------\n",
      "ALP : 162.84761859348689\n",
      "ALT : 401.7164166987802\n",
      "AST : 426.26339634826826\n",
      "Albumin : 1.9364705899182262\n",
      "BUN : 21.407407635818238\n",
      "Bilirubin : 3.2957983256137395\n",
      "Cholesterol : 79.49999999996025\n",
      "Creatinine : 1.327403837084194\n",
      "DiasABP : 64.24639737374103\n",
      "FiO2 : 0.33402405745214264\n",
      "GCS : 8.468112058820436\n",
      "Glucose : 137.04576622375876\n",
      "HCO3 : 17.57380903107775\n",
      "HCT : 24.89687492325897\n",
      "HR : 89.76194659125372\n",
      "K : 2.6303703066743385\n",
      "Lactate : 2.6484445124202187\n",
      "MAP : 82.55844403135909\n",
      "MechVent : 0.0\n",
      "Mg : 1.9805352983683515\n",
      "NIDiasABP : 63.15181705362211\n",
      "NIMAP : 79.44447386962122\n",
      "NISysABP : 120.10298521469146\n",
      "Na : 40.72517321016157\n",
      "PaCO2 : 40.965354435838044\n",
      "PaO2 : 154.81246861936796\n",
      "Platelets : 190.180658219121\n",
      "RespRate : 18.735708352541018\n",
      "SaO2 : 96.0360824742263\n",
      "SysABP : 120.08354939651238\n",
      "Temp : 54.96286379705038\n",
      "TroponinI : 1.4499999672172643\n",
      "TroponinT : 0.8353571378146666\n",
      "Urine : 138.27340037797455\n",
      "WBC : 12.079177701662926\n",
      "Weight : 87.61735840037137\n",
      "Ph : 7.370806538492749\n",
      "ICUType 1\n",
      "-------------\n",
      "ALP : 81.63636398315059\n",
      "ALT : 318.0000019073328\n",
      "AST : 679.9200496291843\n",
      "Albumin : 2.3399999737738395\n",
      "BUN : 28.48062042118021\n",
      "Bilirubin : 1.1680000156163703\n",
      "Cholesterol : 143.7692290085902\n",
      "Creatinine : 1.7499999867666594\n",
      "DiasABP : 59.33842763481674\n",
      "FiO2 : 0.32821655823166757\n",
      "GCS : 9.705188700612963\n",
      "Glucose : 136.23437499999895\n",
      "HCO3 : 18.309734023777622\n",
      "HCT : 27.246666675143572\n",
      "HR : 81.56440610734238\n",
      "K : 2.662142789363842\n",
      "Lactate : 3.2419355800073872\n",
      "MAP : 80.4727334238057\n",
      "MechVent : 0.0\n",
      "Mg : 2.0463414831859144\n",
      "NIDiasABP : 58.20620430767199\n",
      "NIMAP : 74.22048429601323\n",
      "NISysABP : 113.34630789714456\n",
      "Na : 40.343999999999696\n",
      "PaCO2 : 39.610236235490625\n",
      "PaO2 : 153.58333327553373\n",
      "Platelets : 207.89830217522913\n",
      "RespRate : 19.312523055032656\n",
      "SaO2 : 96.46835443037853\n",
      "SysABP : 114.83695982709325\n",
      "Temp : 54.70374511203673\n",
      "TroponinI : 18.56249945014483\n",
      "TroponinT : 2.4238234945692727\n",
      "Urine : 136.95640062706872\n",
      "WBC : 11.833636316386029\n",
      "Weight : 82.82441105491851\n",
      "Ph : 7.388508066054254\n",
      "ICUType 2\n",
      "-------------\n",
      "ALP : 50.239999847410104\n",
      "ALT : 496.3157886203706\n",
      "AST : 294.00000107805386\n",
      "Albumin : 1.685714295931984\n",
      "BUN : 19.337931251525745\n",
      "Bilirubin : 2.3250000197438934\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 1.123999982476227\n",
      "DiasABP : 56.887238211319065\n",
      "FiO2 : 0.35894281149986823\n",
      "GCS : 8.646399650109842\n",
      "Glucose : 121.68375964042365\n",
      "HCO3 : 18.17391252221517\n",
      "HCT : 23.92999998728425\n",
      "HR : 85.90867378770199\n",
      "K : 2.9758195759819914\n",
      "Lactate : 3.0614286379499416\n",
      "MAP : 75.11228422088696\n",
      "MechVent : 0.0\n",
      "Mg : 2.1540540730630884\n",
      "NIDiasABP : 53.38178922826584\n",
      "NIMAP : 71.19470374340712\n",
      "NISysABP : 111.44726991600109\n",
      "Na : 40.01449275362291\n",
      "PaCO2 : 40.51112254884934\n",
      "PaO2 : 168.76030747195674\n",
      "Platelets : 158.28835696265728\n",
      "RespRate : 20.07981216403772\n",
      "SaO2 : 96.94730394026776\n",
      "SysABP : 114.61523617752768\n",
      "Temp : 55.07901341241117\n",
      "TroponinI : 10.080000114438901\n",
      "TroponinT : 0.37555552810426357\n",
      "Urine : 109.52372798504028\n",
      "WBC : 14.028042288684267\n",
      "Weight : 87.25150206318779\n",
      "Ph : 7.3798095029943\n",
      "ICUType 3\n",
      "-------------\n",
      "ALP : 176.60194148831985\n",
      "ALT : 395.0919550972378\n",
      "AST : 487.0549483351602\n",
      "Albumin : 1.8048387342883403\n",
      "BUN : 32.9982456307661\n",
      "Bilirubin : 2.4300970831542106\n",
      "Cholesterol : 100.24999999997493\n",
      "Creatinine : 1.8934640331011128\n",
      "DiasABP : 61.77350118772809\n",
      "FiO2 : 0.3460876353992164\n",
      "GCS : 8.595744740400564\n",
      "Glucose : 135.58193979933063\n",
      "HCO3 : 17.31845187998948\n",
      "HCT : 25.531780817737246\n",
      "HR : 89.69882572189515\n",
      "K : 2.6334345942210198\n",
      "Lactate : 3.1338158669440164\n",
      "MAP : 81.7872864814867\n",
      "MechVent : 0.0\n",
      "Mg : 2.024652801661021\n",
      "NIDiasABP : 60.09509955157426\n",
      "NIMAP : 77.84404846346212\n",
      "NISysABP : 119.99908820791582\n",
      "Na : 40.84364818361748\n",
      "PaCO2 : 41.83614885484835\n",
      "PaO2 : 122.97904193900979\n",
      "Platelets : 179.28321395553846\n",
      "RespRate : 19.813428369416663\n",
      "SaO2 : 91.89999999999816\n",
      "SysABP : 120.19424387102839\n",
      "Temp : 54.53625548851933\n",
      "TroponinI : 2.59999999470152\n",
      "TroponinT : 0.42593219392884873\n",
      "Urine : 115.98839683532711\n",
      "WBC : 12.33562751651288\n",
      "Weight : 82.20662347609178\n",
      "Ph : 7.353313575834895\n",
      "ICUType 4\n",
      "-------------\n",
      "ALP : 113.0606070431779\n",
      "ALT : 279.6557306227091\n",
      "AST : 626.3636355139878\n",
      "Albumin : 1.7702127517537491\n",
      "BUN : 19.029914857994715\n",
      "Bilirubin : 2.8444444589592264\n",
      "Cholesterol : 118.99999999998016\n",
      "Creatinine : 1.0334661176480102\n",
      "DiasABP : 63.1907915071831\n",
      "FiO2 : 0.31555977550864533\n",
      "GCS : 7.895913207820239\n",
      "Glucose : 126.22672053966394\n",
      "HCO3 : 18.05150167931807\n",
      "HCT : 25.356920980464192\n",
      "HR : 86.93353806854414\n",
      "K : 2.554821360962722\n",
      "Lactate : 2.8838185334989026\n",
      "MAP : 84.9015973258039\n",
      "MechVent : 0.0\n",
      "Mg : 1.9418000202178878\n",
      "NIDiasABP : 60.484403815319865\n",
      "NIMAP : 79.75340070012265\n",
      "NISysABP : 126.00626599873202\n",
      "Na : 41.369003690036756\n",
      "PaCO2 : 39.545340160879036\n",
      "PaO2 : 145.5714285331856\n",
      "Platelets : 185.28759094467665\n",
      "RespRate : 18.940393720266883\n",
      "SaO2 : 96.57142857142703\n",
      "SysABP : 127.63526359476064\n",
      "Temp : 54.84505403655626\n",
      "TroponinI : 7.400000193713245\n",
      "TroponinT : 0.44750001045562954\n",
      "Urine : 122.13026955095468\n",
      "WBC : 12.333124995231577\n",
      "Weight : 81.81641423799681\n",
      "Ph : 7.380082555536936\n",
      "Undefined classification\n",
      "-------------\n",
      "ALP : 103.20224766248953\n",
      "ALT : 290.70886298070616\n",
      "AST : 275.44210944928335\n",
      "Albumin : 1.7397436071664638\n",
      "BUN : 26.15714303993037\n",
      "Bilirubin : 2.7584269657898175\n",
      "Cholesterol : 142.6666649712298\n",
      "Creatinine : 1.4306615559047684\n",
      "DiasABP : 63.04001848988648\n",
      "FiO2 : 0.31534482729038543\n",
      "GCS : 8.622538356864474\n",
      "Glucose : 130.88009568541423\n",
      "HCO3 : 18.105910953042496\n",
      "HCT : 25.53127695678111\n",
      "HR : 86.61615923397423\n",
      "K : 2.6348957702517453\n",
      "Lactate : 2.618356880727496\n",
      "MAP : 84.14977130915793\n",
      "MechVent : 0.0\n",
      "Mg : 1.9515384796338153\n",
      "NIDiasABP : 60.48770409995857\n",
      "NIMAP : 77.79163943956353\n",
      "NISysABP : 121.2010913430792\n",
      "Na : 41.33495141927469\n",
      "PaCO2 : 41.31355941901762\n",
      "PaO2 : 131.05824827661286\n",
      "Platelets : 199.48257088341543\n",
      "RespRate : 19.78130921748146\n",
      "SaO2 : 95.98484848484703\n",
      "SysABP : 124.79178462349446\n",
      "Temp : 54.73883000696939\n",
      "TroponinI : 5.733333333332378\n",
      "TroponinT : 0.5918493108583968\n",
      "Urine : 123.38135347328762\n",
      "WBC : 13.08543102327602\n",
      "Weight : 81.12995947038229\n",
      "Ph : 7.372282332252036\n",
      "Low Weight\n",
      "-------------\n",
      "ALP : 358.4999999998207\n",
      "ALT : 203.19999999995935\n",
      "AST : 885.0000677106552\n",
      "Albumin : 2.100000023841508\n",
      "BUN : 29.850000286100805\n",
      "Bilirubin : 8.280000007150901\n",
      "Cholesterol : 125.99999999987399\n",
      "Creatinine : 0.9142857164143864\n",
      "DiasABP : 59.64975007136021\n",
      "FiO2 : 0.29609756242937435\n",
      "GCS : 8.093750007450454\n",
      "Glucose : 162.34782310153423\n",
      "HCO3 : 20.749999682106832\n",
      "HCT : 24.568749904631545\n",
      "HR : 88.64566976912853\n",
      "K : 2.456521604372001\n",
      "Lactate : 4.56000005602791\n",
      "MAP : 83.81590396165782\n",
      "MechVent : 0.0\n",
      "Mg : 2.1200000286100176\n",
      "NIDiasABP : 58.45740850766419\n",
      "NIMAP : 76.07613635372708\n",
      "NISysABP : 120.98340090899231\n",
      "Na : 37.79999999999623\n",
      "PaCO2 : 39.808823866001525\n",
      "PaO2 : 194.11842105262647\n",
      "Platelets : 164.13636155560474\n",
      "RespRate : 21.47619043077639\n",
      "SaO2 : 96.60714285713595\n",
      "SysABP : 124.1060285635865\n",
      "Temp : 54.78626934398242\n",
      "TroponinI : 0.0\n",
      "TroponinT : 2.779999961850247\n",
      "Urine : 117.98993227145738\n",
      "WBC : 12.10384623820872\n",
      "Weight : 56.794067188844366\n",
      "Ph : 7.372073185152944\n",
      "Normal Weight\n",
      "-------------\n",
      "ALP : 109.83871041574432\n",
      "ALT : 156.09524000257932\n",
      "AST : 173.37499982118064\n",
      "Albumin : 1.636842137888772\n",
      "BUN : 26.64028809567988\n",
      "Bilirubin : 3.4551723982238403\n",
      "Cholesterol : 86.49999237056221\n",
      "Creatinine : 1.5014084398326635\n",
      "DiasABP : 58.76064030732224\n",
      "FiO2 : 0.3270249722019458\n",
      "GCS : 8.016666685070893\n",
      "Glucose : 129.56198290359768\n",
      "HCO3 : 17.909090540625698\n",
      "HCT : 25.228723647746502\n",
      "HR : 86.1620427014138\n",
      "K : 2.6548386273845335\n",
      "Lactate : 2.899519312840214\n",
      "MAP : 77.19315231744962\n",
      "MechVent : 0.0\n",
      "Mg : 2.0536585677929846\n",
      "NIDiasABP : 56.28495372599683\n",
      "NIMAP : 74.73490517552231\n",
      "NISysABP : 114.4056657321788\n",
      "Na : 40.881481481481195\n",
      "PaCO2 : 39.83276454417765\n",
      "PaO2 : 160.31305445101717\n",
      "Platelets : 167.3093499931488\n",
      "RespRate : 18.32343131359484\n",
      "SaO2 : 97.06249999999908\n",
      "SysABP : 115.43367258292807\n",
      "Temp : 54.865208190415636\n",
      "TroponinI : 8.34444455040733\n",
      "TroponinT : 0.8838095269813006\n",
      "Urine : 105.16068367160482\n",
      "WBC : 13.203875901162984\n",
      "Weight : 67.9053476384954\n",
      "Ph : 7.379047604645165\n",
      "Overweight\n",
      "-------------\n",
      "ALP : 90.84375023841574\n",
      "ALT : 640.8055382172088\n",
      "AST : 663.7142724082267\n",
      "Albumin : 2.0210525926789678\n",
      "BUN : 25.492424462780615\n",
      "Bilirubin : 1.5861110765900435\n",
      "Cholesterol : 178.28571210586182\n",
      "Creatinine : 1.2445255269099236\n",
      "DiasABP : 58.9725691945707\n",
      "FiO2 : 0.32623978042472523\n",
      "GCS : 8.548223412379784\n",
      "Glucose : 130.46721258319687\n",
      "HCO3 : 17.406451053003998\n",
      "HCT : 24.82920563778019\n",
      "HR : 85.52348124530441\n",
      "K : 2.882677070737801\n",
      "Lactate : 2.9854651772698344\n",
      "MAP : 78.66941025770825\n",
      "MechVent : 0.0\n",
      "Mg : 2.0132867372952834\n",
      "NIDiasABP : 57.550336975610456\n",
      "NIMAP : 75.65089601458912\n",
      "NISysABP : 115.66145973658907\n",
      "Na : 40.06597222222196\n",
      "PaCO2 : 40.217234301493136\n",
      "PaO2 : 150.2811594424036\n",
      "Platelets : 166.35802219532053\n",
      "RespRate : 19.308421317111165\n",
      "SaO2 : 96.74803149606224\n",
      "SysABP : 117.77767338535995\n",
      "Temp : 54.976217747960845\n",
      "TroponinI : 9.739999675748784\n",
      "TroponinT : 1.55384609037855\n",
      "Urine : 106.28093231747964\n",
      "WBC : 12.234964963439376\n",
      "Weight : 83.8306388319315\n",
      "Ph : 7.376130891697725\n",
      "Obesity 1\n",
      "-------------\n",
      "ALP : 93.57142938885701\n",
      "ALT : 248.9000013351316\n",
      "AST : 384.75000405309663\n",
      "Albumin : 1.983333369095955\n",
      "BUN : 29.306666730244565\n",
      "Bilirubin : 2.5933333277700603\n",
      "Cholesterol : 103.99999999996533\n",
      "Creatinine : 1.263768099442754\n",
      "DiasABP : 60.19672265569193\n",
      "FiO2 : 0.37257575348169936\n",
      "GCS : 8.416949225280215\n",
      "Glucose : 123.98245386491524\n",
      "HCO3 : 18.21739082059972\n",
      "HCT : 25.636160816464876\n",
      "HR : 86.93467938458468\n",
      "K : 2.573493908686779\n",
      "Lactate : 3.297777867317126\n",
      "MAP : 79.07987651984328\n",
      "MechVent : 0.0\n",
      "Mg : 1.9642857432365137\n",
      "NIDiasABP : 59.71149696375257\n",
      "NIMAP : 75.18894930047477\n",
      "NISysABP : 118.64528871669559\n",
      "Na : 39.546874999999396\n",
      "PaCO2 : 39.36060617620271\n",
      "PaO2 : 131.0406977187748\n",
      "Platelets : 188.4999973899415\n",
      "RespRate : 19.633986984202938\n",
      "SaO2 : 96.77868852458857\n",
      "SysABP : 120.06604052649587\n",
      "Temp : 55.086815687410976\n",
      "TroponinI : 7.499999874826496\n",
      "TroponinT : 1.1200000172446058\n",
      "Urine : 119.93684820085748\n",
      "WBC : 16.095082040692674\n",
      "Weight : 98.10293181533095\n",
      "Ph : 7.897253254938348\n",
      "Obesity 2\n",
      "-------------\n",
      "ALP : 55.55555597940464\n",
      "ALT : 771.2857140131257\n",
      "AST : 82.00000127153842\n",
      "Albumin : 1.7499999602632785\n",
      "BUN : 31.096774316602136\n",
      "Bilirubin : 1.6444445004064003\n",
      "Cholesterol : 135.99998474107494\n",
      "Creatinine : 1.3066666533549192\n",
      "DiasABP : 60.65967760512106\n",
      "FiO2 : 0.3507758611440598\n",
      "GCS : 8.775000051657287\n",
      "Glucose : 124.64516196711969\n",
      "HCO3 : 19.849999523161852\n",
      "HCT : 26.09210491180351\n",
      "HR : 89.15027794471129\n",
      "K : 2.623076842381303\n",
      "Lactate : 2.9500000434260305\n",
      "MAP : 79.40157111746333\n",
      "MechVent : 0.0\n",
      "Mg : 1.948148175522061\n",
      "NIDiasABP : 57.99116250240398\n",
      "NIMAP : 75.81827120016492\n",
      "NISysABP : 113.35227284286876\n",
      "Na : 41.060606060604826\n",
      "PaCO2 : 39.86153860825698\n",
      "PaO2 : 150.49230780968068\n",
      "Platelets : 169.65384145882763\n",
      "RespRate : 20.693181991576676\n",
      "SaO2 : 96.27777777777244\n",
      "SysABP : 115.81126669311477\n",
      "Temp : 54.98132122227373\n",
      "TroponinI : 0.0\n",
      "TroponinT : 5.00999968290162\n",
      "Urine : 109.4420520390892\n",
      "WBC : 13.003225834138513\n",
      "Weight : 107.61793074400471\n",
      "Ph : 7.353928514889176\n",
      "Obesity 3\n",
      "-------------\n",
      "ALP : 87.45454268021221\n",
      "ALT : 1687.3333333327707\n",
      "AST : 41.24999952315253\n",
      "Albumin : 1.6799999713894345\n",
      "BUN : 29.428571519396154\n",
      "Bilirubin : 2.2166666885213338\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 1.6838709508218528\n",
      "DiasABP : 60.39697047642279\n",
      "FiO2 : 0.3788278304220515\n",
      "GCS : 8.84836070263965\n",
      "Glucose : 156.94117882672015\n",
      "HCO3 : 16.735293556661674\n",
      "HCT : 25.482926671097296\n",
      "HR : 87.77342518893134\n",
      "K : 2.839999914169217\n",
      "Lactate : 3.423611212107775\n",
      "MAP : 78.63692615194532\n",
      "MechVent : 0.0\n",
      "Mg : 2.037142876216285\n",
      "NIDiasABP : 57.81923150282596\n",
      "NIMAP : 75.5919045499848\n",
      "NISysABP : 114.6807662501471\n",
      "Na : 38.39130434782444\n",
      "PaCO2 : 41.18604651162743\n",
      "PaO2 : 121.22097366847214\n",
      "Platelets : 160.96874713897202\n",
      "RespRate : 19.796296437580647\n",
      "SaO2 : 96.03703703703349\n",
      "SysABP : 117.61257398340582\n",
      "Temp : 55.11973737506749\n",
      "TroponinI : 6.8999999999931\n",
      "TroponinT : 3.876000068484962\n",
      "Urine : 108.46558934383147\n",
      "WBC : 11.903030265460954\n",
      "Weight : 127.39821168354523\n",
      "Ph : 7.3652777539358185\n"
     ]
    }
   ],
   "source": [
    "print(\"SAITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_saits_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saits_mae_minmax = toolkits.create_table(testing_mae_saits_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>General</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Undefined Gender</th>\n",
       "      <th>+65</th>\n",
       "      <th>-65</th>\n",
       "      <th>ICUType 1</th>\n",
       "      <th>ICUType 2</th>\n",
       "      <th>ICUType 3</th>\n",
       "      <th>ICUType 4</th>\n",
       "      <th>Undefined classification</th>\n",
       "      <th>Low Weight</th>\n",
       "      <th>Normal Weight</th>\n",
       "      <th>Overweight</th>\n",
       "      <th>Obesity 1</th>\n",
       "      <th>Obesity 2</th>\n",
       "      <th>Obesity 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALP</td>\n",
       "      <td>93.477045</td>\n",
       "      <td>95.923397</td>\n",
       "      <td>94.204229</td>\n",
       "      <td>90.800756</td>\n",
       "      <td>97.736268</td>\n",
       "      <td>112.724208</td>\n",
       "      <td>85.518225</td>\n",
       "      <td>79.479239</td>\n",
       "      <td>127.019654</td>\n",
       "      <td>90.321470</td>\n",
       "      <td>89.091845</td>\n",
       "      <td>157.806162</td>\n",
       "      <td>98.746588</td>\n",
       "      <td>87.399715</td>\n",
       "      <td>82.485978</td>\n",
       "      <td>64.762371</td>\n",
       "      <td>74.295819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALT</td>\n",
       "      <td>311.856460</td>\n",
       "      <td>146.806237</td>\n",
       "      <td>299.865078</td>\n",
       "      <td>876.780331</td>\n",
       "      <td>241.619627</td>\n",
       "      <td>414.153798</td>\n",
       "      <td>340.540936</td>\n",
       "      <td>521.730284</td>\n",
       "      <td>408.045663</td>\n",
       "      <td>300.914797</td>\n",
       "      <td>314.814130</td>\n",
       "      <td>219.491635</td>\n",
       "      <td>189.224936</td>\n",
       "      <td>648.496920</td>\n",
       "      <td>268.583071</td>\n",
       "      <td>776.440910</td>\n",
       "      <td>1629.534974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AST</td>\n",
       "      <td>580.753358</td>\n",
       "      <td>632.946314</td>\n",
       "      <td>285.729871</td>\n",
       "      <td>663.955860</td>\n",
       "      <td>279.649209</td>\n",
       "      <td>449.902791</td>\n",
       "      <td>685.770612</td>\n",
       "      <td>355.311073</td>\n",
       "      <td>502.757992</td>\n",
       "      <td>641.779854</td>\n",
       "      <td>307.952064</td>\n",
       "      <td>880.059385</td>\n",
       "      <td>209.915174</td>\n",
       "      <td>682.205256</td>\n",
       "      <td>413.126054</td>\n",
       "      <td>193.328673</td>\n",
       "      <td>148.148190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albumin</td>\n",
       "      <td>1.994266</td>\n",
       "      <td>1.977296</td>\n",
       "      <td>2.006398</td>\n",
       "      <td>2.153890</td>\n",
       "      <td>1.981838</td>\n",
       "      <td>2.007729</td>\n",
       "      <td>2.022257</td>\n",
       "      <td>1.914905</td>\n",
       "      <td>2.005524</td>\n",
       "      <td>2.025073</td>\n",
       "      <td>2.051419</td>\n",
       "      <td>1.967156</td>\n",
       "      <td>1.909736</td>\n",
       "      <td>1.948019</td>\n",
       "      <td>1.956438</td>\n",
       "      <td>1.967108</td>\n",
       "      <td>1.943897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUN</td>\n",
       "      <td>24.293075</td>\n",
       "      <td>23.760087</td>\n",
       "      <td>26.193407</td>\n",
       "      <td>24.147696</td>\n",
       "      <td>27.949329</td>\n",
       "      <td>21.970768</td>\n",
       "      <td>27.157634</td>\n",
       "      <td>20.862990</td>\n",
       "      <td>29.679509</td>\n",
       "      <td>20.261092</td>\n",
       "      <td>25.329214</td>\n",
       "      <td>27.332073</td>\n",
       "      <td>25.181038</td>\n",
       "      <td>24.430181</td>\n",
       "      <td>26.966782</td>\n",
       "      <td>28.234565</td>\n",
       "      <td>26.974401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bilirubin</td>\n",
       "      <td>2.770370</td>\n",
       "      <td>2.151613</td>\n",
       "      <td>2.242935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.634444</td>\n",
       "      <td>3.295798</td>\n",
       "      <td>1.168000</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>2.430097</td>\n",
       "      <td>2.844444</td>\n",
       "      <td>2.758427</td>\n",
       "      <td>8.280000</td>\n",
       "      <td>3.455172</td>\n",
       "      <td>1.586111</td>\n",
       "      <td>2.593333</td>\n",
       "      <td>1.644445</td>\n",
       "      <td>2.216667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cholesterol</td>\n",
       "      <td>117.165723</td>\n",
       "      <td>116.194360</td>\n",
       "      <td>117.916085</td>\n",
       "      <td>123.351923</td>\n",
       "      <td>115.404416</td>\n",
       "      <td>119.041461</td>\n",
       "      <td>115.601955</td>\n",
       "      <td>115.020012</td>\n",
       "      <td>116.757318</td>\n",
       "      <td>120.068165</td>\n",
       "      <td>118.743108</td>\n",
       "      <td>118.607903</td>\n",
       "      <td>113.920232</td>\n",
       "      <td>116.190098</td>\n",
       "      <td>115.977272</td>\n",
       "      <td>118.668994</td>\n",
       "      <td>117.289936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Creatinine</td>\n",
       "      <td>1.347340</td>\n",
       "      <td>1.134677</td>\n",
       "      <td>1.479304</td>\n",
       "      <td>1.396137</td>\n",
       "      <td>1.269102</td>\n",
       "      <td>1.310059</td>\n",
       "      <td>1.584878</td>\n",
       "      <td>1.154731</td>\n",
       "      <td>1.681413</td>\n",
       "      <td>1.109242</td>\n",
       "      <td>1.379090</td>\n",
       "      <td>1.009123</td>\n",
       "      <td>1.378736</td>\n",
       "      <td>1.243383</td>\n",
       "      <td>1.278136</td>\n",
       "      <td>1.321840</td>\n",
       "      <td>1.589997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DiasABP</td>\n",
       "      <td>62.718084</td>\n",
       "      <td>62.466381</td>\n",
       "      <td>63.073589</td>\n",
       "      <td>65.099445</td>\n",
       "      <td>59.837164</td>\n",
       "      <td>66.474790</td>\n",
       "      <td>61.324229</td>\n",
       "      <td>58.838258</td>\n",
       "      <td>63.913077</td>\n",
       "      <td>65.301539</td>\n",
       "      <td>65.166037</td>\n",
       "      <td>61.570727</td>\n",
       "      <td>60.634648</td>\n",
       "      <td>60.923928</td>\n",
       "      <td>62.216848</td>\n",
       "      <td>62.799902</td>\n",
       "      <td>62.390182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FiO2</td>\n",
       "      <td>0.300710</td>\n",
       "      <td>0.294553</td>\n",
       "      <td>0.305745</td>\n",
       "      <td>0.324718</td>\n",
       "      <td>0.300769</td>\n",
       "      <td>0.301508</td>\n",
       "      <td>0.324819</td>\n",
       "      <td>0.312221</td>\n",
       "      <td>0.308347</td>\n",
       "      <td>0.273451</td>\n",
       "      <td>0.299435</td>\n",
       "      <td>0.266262</td>\n",
       "      <td>0.290676</td>\n",
       "      <td>0.303029</td>\n",
       "      <td>0.316807</td>\n",
       "      <td>0.312052</td>\n",
       "      <td>0.311929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GCS</td>\n",
       "      <td>6.493009</td>\n",
       "      <td>6.484331</td>\n",
       "      <td>6.514906</td>\n",
       "      <td>6.688443</td>\n",
       "      <td>6.693542</td>\n",
       "      <td>6.333155</td>\n",
       "      <td>7.673461</td>\n",
       "      <td>6.993701</td>\n",
       "      <td>6.788229</td>\n",
       "      <td>5.327492</td>\n",
       "      <td>6.503094</td>\n",
       "      <td>6.374173</td>\n",
       "      <td>6.388281</td>\n",
       "      <td>6.601707</td>\n",
       "      <td>6.359520</td>\n",
       "      <td>6.987679</td>\n",
       "      <td>6.269248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Glucose</td>\n",
       "      <td>126.797350</td>\n",
       "      <td>126.167444</td>\n",
       "      <td>129.474382</td>\n",
       "      <td>125.723023</td>\n",
       "      <td>130.336436</td>\n",
       "      <td>130.839741</td>\n",
       "      <td>129.291353</td>\n",
       "      <td>123.197743</td>\n",
       "      <td>128.780477</td>\n",
       "      <td>123.720415</td>\n",
       "      <td>126.110103</td>\n",
       "      <td>147.982383</td>\n",
       "      <td>125.297818</td>\n",
       "      <td>126.396712</td>\n",
       "      <td>123.643509</td>\n",
       "      <td>124.378041</td>\n",
       "      <td>144.675040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HCO3</td>\n",
       "      <td>17.406941</td>\n",
       "      <td>17.287865</td>\n",
       "      <td>17.561093</td>\n",
       "      <td>18.668094</td>\n",
       "      <td>17.451662</td>\n",
       "      <td>17.339996</td>\n",
       "      <td>17.666147</td>\n",
       "      <td>17.617624</td>\n",
       "      <td>17.216361</td>\n",
       "      <td>17.349596</td>\n",
       "      <td>17.542259</td>\n",
       "      <td>16.886006</td>\n",
       "      <td>17.063045</td>\n",
       "      <td>17.313884</td>\n",
       "      <td>17.460329</td>\n",
       "      <td>17.685992</td>\n",
       "      <td>17.611188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HCT</td>\n",
       "      <td>22.311316</td>\n",
       "      <td>22.328788</td>\n",
       "      <td>22.310200</td>\n",
       "      <td>23.153208</td>\n",
       "      <td>22.204833</td>\n",
       "      <td>22.415030</td>\n",
       "      <td>22.541695</td>\n",
       "      <td>21.290709</td>\n",
       "      <td>22.697641</td>\n",
       "      <td>22.563150</td>\n",
       "      <td>22.743585</td>\n",
       "      <td>22.173624</td>\n",
       "      <td>21.761719</td>\n",
       "      <td>21.737766</td>\n",
       "      <td>22.220204</td>\n",
       "      <td>22.066124</td>\n",
       "      <td>22.389278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HR</td>\n",
       "      <td>86.695616</td>\n",
       "      <td>87.087076</td>\n",
       "      <td>86.312270</td>\n",
       "      <td>73.187500</td>\n",
       "      <td>83.740056</td>\n",
       "      <td>89.761947</td>\n",
       "      <td>81.564406</td>\n",
       "      <td>85.908674</td>\n",
       "      <td>89.698826</td>\n",
       "      <td>86.933538</td>\n",
       "      <td>86.616159</td>\n",
       "      <td>88.645670</td>\n",
       "      <td>86.162043</td>\n",
       "      <td>85.523481</td>\n",
       "      <td>86.934679</td>\n",
       "      <td>89.150278</td>\n",
       "      <td>87.773425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>K</td>\n",
       "      <td>2.565624</td>\n",
       "      <td>2.532001</td>\n",
       "      <td>2.592567</td>\n",
       "      <td>2.573728</td>\n",
       "      <td>2.561794</td>\n",
       "      <td>2.564755</td>\n",
       "      <td>2.559777</td>\n",
       "      <td>2.655631</td>\n",
       "      <td>2.552924</td>\n",
       "      <td>2.525756</td>\n",
       "      <td>2.547907</td>\n",
       "      <td>2.459405</td>\n",
       "      <td>2.534051</td>\n",
       "      <td>2.597519</td>\n",
       "      <td>2.610494</td>\n",
       "      <td>2.668946</td>\n",
       "      <td>2.731433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lactate</td>\n",
       "      <td>2.767346</td>\n",
       "      <td>2.657636</td>\n",
       "      <td>2.897482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.640000</td>\n",
       "      <td>2.648445</td>\n",
       "      <td>3.241936</td>\n",
       "      <td>3.061429</td>\n",
       "      <td>3.133816</td>\n",
       "      <td>2.883819</td>\n",
       "      <td>2.618357</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>2.899519</td>\n",
       "      <td>2.985465</td>\n",
       "      <td>3.297778</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>3.423611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MAP</td>\n",
       "      <td>80.309701</td>\n",
       "      <td>80.750172</td>\n",
       "      <td>80.104122</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>78.410036</td>\n",
       "      <td>82.558444</td>\n",
       "      <td>80.472733</td>\n",
       "      <td>75.112284</td>\n",
       "      <td>81.787286</td>\n",
       "      <td>84.901597</td>\n",
       "      <td>84.149771</td>\n",
       "      <td>83.815904</td>\n",
       "      <td>77.193152</td>\n",
       "      <td>78.669410</td>\n",
       "      <td>79.079877</td>\n",
       "      <td>79.401571</td>\n",
       "      <td>78.636926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MechVent</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.004021</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.003238</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>0.003553</td>\n",
       "      <td>0.003630</td>\n",
       "      <td>0.003485</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>0.003342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mg</td>\n",
       "      <td>2.011890</td>\n",
       "      <td>1.972207</td>\n",
       "      <td>2.063425</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.067133</td>\n",
       "      <td>1.980535</td>\n",
       "      <td>2.046341</td>\n",
       "      <td>2.154054</td>\n",
       "      <td>2.024653</td>\n",
       "      <td>1.941800</td>\n",
       "      <td>1.951538</td>\n",
       "      <td>2.120000</td>\n",
       "      <td>2.053659</td>\n",
       "      <td>2.013287</td>\n",
       "      <td>1.964286</td>\n",
       "      <td>1.948148</td>\n",
       "      <td>2.037143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NIDiasABP</td>\n",
       "      <td>62.984938</td>\n",
       "      <td>61.224616</td>\n",
       "      <td>64.601721</td>\n",
       "      <td>68.150676</td>\n",
       "      <td>59.201504</td>\n",
       "      <td>67.409296</td>\n",
       "      <td>62.002919</td>\n",
       "      <td>56.065658</td>\n",
       "      <td>63.571879</td>\n",
       "      <td>64.033567</td>\n",
       "      <td>63.925443</td>\n",
       "      <td>61.880576</td>\n",
       "      <td>59.495839</td>\n",
       "      <td>61.169317</td>\n",
       "      <td>64.029927</td>\n",
       "      <td>61.478189</td>\n",
       "      <td>61.690531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NIMAP</td>\n",
       "      <td>77.730321</td>\n",
       "      <td>75.826498</td>\n",
       "      <td>78.288817</td>\n",
       "      <td>79.330002</td>\n",
       "      <td>74.700983</td>\n",
       "      <td>79.444474</td>\n",
       "      <td>74.220484</td>\n",
       "      <td>71.194704</td>\n",
       "      <td>77.844048</td>\n",
       "      <td>79.753401</td>\n",
       "      <td>77.791639</td>\n",
       "      <td>76.076136</td>\n",
       "      <td>74.734905</td>\n",
       "      <td>75.650896</td>\n",
       "      <td>75.188949</td>\n",
       "      <td>75.818271</td>\n",
       "      <td>75.591905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NISysABP</td>\n",
       "      <td>119.478651</td>\n",
       "      <td>119.259779</td>\n",
       "      <td>118.476627</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>119.293391</td>\n",
       "      <td>120.102985</td>\n",
       "      <td>113.346308</td>\n",
       "      <td>111.447270</td>\n",
       "      <td>119.999088</td>\n",
       "      <td>126.006266</td>\n",
       "      <td>121.201091</td>\n",
       "      <td>120.983401</td>\n",
       "      <td>114.405666</td>\n",
       "      <td>115.661460</td>\n",
       "      <td>118.645289</td>\n",
       "      <td>113.352273</td>\n",
       "      <td>114.680766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Na</td>\n",
       "      <td>39.757599</td>\n",
       "      <td>39.759120</td>\n",
       "      <td>39.769347</td>\n",
       "      <td>40.111608</td>\n",
       "      <td>39.801245</td>\n",
       "      <td>39.758690</td>\n",
       "      <td>39.461356</td>\n",
       "      <td>40.248345</td>\n",
       "      <td>39.574430</td>\n",
       "      <td>39.802608</td>\n",
       "      <td>39.755696</td>\n",
       "      <td>39.817644</td>\n",
       "      <td>39.894110</td>\n",
       "      <td>39.839113</td>\n",
       "      <td>39.668110</td>\n",
       "      <td>39.647284</td>\n",
       "      <td>39.306893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PaCO2</td>\n",
       "      <td>40.952782</td>\n",
       "      <td>40.324646</td>\n",
       "      <td>40.779975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.720028</td>\n",
       "      <td>40.965354</td>\n",
       "      <td>39.610236</td>\n",
       "      <td>40.511123</td>\n",
       "      <td>41.836149</td>\n",
       "      <td>39.545340</td>\n",
       "      <td>41.313559</td>\n",
       "      <td>39.808824</td>\n",
       "      <td>39.832765</td>\n",
       "      <td>40.217234</td>\n",
       "      <td>39.360606</td>\n",
       "      <td>39.861539</td>\n",
       "      <td>41.186047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PaO2</td>\n",
       "      <td>148.291760</td>\n",
       "      <td>146.972273</td>\n",
       "      <td>148.144981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.657934</td>\n",
       "      <td>154.812469</td>\n",
       "      <td>153.583333</td>\n",
       "      <td>168.760307</td>\n",
       "      <td>122.979042</td>\n",
       "      <td>145.571429</td>\n",
       "      <td>131.058248</td>\n",
       "      <td>194.118421</td>\n",
       "      <td>160.313054</td>\n",
       "      <td>150.281159</td>\n",
       "      <td>131.040698</td>\n",
       "      <td>150.492308</td>\n",
       "      <td>121.220974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Platelets</td>\n",
       "      <td>183.189953</td>\n",
       "      <td>195.089752</td>\n",
       "      <td>178.572231</td>\n",
       "      <td>180.791637</td>\n",
       "      <td>182.110300</td>\n",
       "      <td>186.584315</td>\n",
       "      <td>201.292159</td>\n",
       "      <td>160.211202</td>\n",
       "      <td>179.155845</td>\n",
       "      <td>182.631895</td>\n",
       "      <td>194.720072</td>\n",
       "      <td>165.269997</td>\n",
       "      <td>167.570452</td>\n",
       "      <td>166.780473</td>\n",
       "      <td>184.509999</td>\n",
       "      <td>170.046672</td>\n",
       "      <td>163.075461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RespRate</td>\n",
       "      <td>19.431791</td>\n",
       "      <td>19.653187</td>\n",
       "      <td>19.083589</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>19.995241</td>\n",
       "      <td>18.735708</td>\n",
       "      <td>19.312523</td>\n",
       "      <td>20.079812</td>\n",
       "      <td>19.813428</td>\n",
       "      <td>18.940394</td>\n",
       "      <td>19.781309</td>\n",
       "      <td>21.476190</td>\n",
       "      <td>18.323431</td>\n",
       "      <td>19.308421</td>\n",
       "      <td>19.633987</td>\n",
       "      <td>20.693182</td>\n",
       "      <td>19.796296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SaO2</td>\n",
       "      <td>96.850858</td>\n",
       "      <td>96.174157</td>\n",
       "      <td>96.756989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96.751957</td>\n",
       "      <td>96.036082</td>\n",
       "      <td>96.468354</td>\n",
       "      <td>96.947304</td>\n",
       "      <td>91.900000</td>\n",
       "      <td>96.571429</td>\n",
       "      <td>95.984848</td>\n",
       "      <td>96.607143</td>\n",
       "      <td>97.062500</td>\n",
       "      <td>96.748031</td>\n",
       "      <td>96.778689</td>\n",
       "      <td>96.277778</td>\n",
       "      <td>96.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SysABP</td>\n",
       "      <td>120.123152</td>\n",
       "      <td>120.349732</td>\n",
       "      <td>120.887513</td>\n",
       "      <td>100.041666</td>\n",
       "      <td>120.696313</td>\n",
       "      <td>120.083549</td>\n",
       "      <td>114.836960</td>\n",
       "      <td>114.615236</td>\n",
       "      <td>120.194244</td>\n",
       "      <td>127.635264</td>\n",
       "      <td>124.791785</td>\n",
       "      <td>124.106029</td>\n",
       "      <td>115.433673</td>\n",
       "      <td>117.777673</td>\n",
       "      <td>120.066041</td>\n",
       "      <td>115.811267</td>\n",
       "      <td>117.612574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Temp</td>\n",
       "      <td>34.710162</td>\n",
       "      <td>36.136897</td>\n",
       "      <td>33.576280</td>\n",
       "      <td>48.422422</td>\n",
       "      <td>33.593713</td>\n",
       "      <td>35.933372</td>\n",
       "      <td>39.425467</td>\n",
       "      <td>20.576714</td>\n",
       "      <td>40.251795</td>\n",
       "      <td>35.453349</td>\n",
       "      <td>41.030899</td>\n",
       "      <td>28.656735</td>\n",
       "      <td>29.969903</td>\n",
       "      <td>26.543015</td>\n",
       "      <td>27.848700</td>\n",
       "      <td>28.967912</td>\n",
       "      <td>29.829309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TroponinI</td>\n",
       "      <td>6.738193</td>\n",
       "      <td>6.634211</td>\n",
       "      <td>6.430636</td>\n",
       "      <td>7.609278</td>\n",
       "      <td>7.578575</td>\n",
       "      <td>5.814059</td>\n",
       "      <td>9.434904</td>\n",
       "      <td>7.066274</td>\n",
       "      <td>6.140485</td>\n",
       "      <td>6.407988</td>\n",
       "      <td>6.627917</td>\n",
       "      <td>6.799923</td>\n",
       "      <td>7.238823</td>\n",
       "      <td>6.958370</td>\n",
       "      <td>6.368796</td>\n",
       "      <td>6.004809</td>\n",
       "      <td>5.882983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TroponinT</td>\n",
       "      <td>0.928967</td>\n",
       "      <td>1.076113</td>\n",
       "      <td>1.147564</td>\n",
       "      <td>1.235715</td>\n",
       "      <td>1.017049</td>\n",
       "      <td>0.960499</td>\n",
       "      <td>2.276089</td>\n",
       "      <td>0.860851</td>\n",
       "      <td>0.561091</td>\n",
       "      <td>0.658230</td>\n",
       "      <td>0.705395</td>\n",
       "      <td>2.158369</td>\n",
       "      <td>0.986757</td>\n",
       "      <td>1.469293</td>\n",
       "      <td>1.139178</td>\n",
       "      <td>3.844033</td>\n",
       "      <td>3.242489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Urine</td>\n",
       "      <td>116.098967</td>\n",
       "      <td>102.454298</td>\n",
       "      <td>123.367161</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>101.877095</td>\n",
       "      <td>138.273400</td>\n",
       "      <td>136.956401</td>\n",
       "      <td>109.523728</td>\n",
       "      <td>115.988397</td>\n",
       "      <td>122.130270</td>\n",
       "      <td>123.381353</td>\n",
       "      <td>117.989932</td>\n",
       "      <td>105.160684</td>\n",
       "      <td>106.280932</td>\n",
       "      <td>119.936848</td>\n",
       "      <td>109.442052</td>\n",
       "      <td>108.465589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WBC</td>\n",
       "      <td>12.264888</td>\n",
       "      <td>12.421652</td>\n",
       "      <td>13.113277</td>\n",
       "      <td>10.850000</td>\n",
       "      <td>13.706148</td>\n",
       "      <td>12.079178</td>\n",
       "      <td>11.833636</td>\n",
       "      <td>14.028042</td>\n",
       "      <td>12.335628</td>\n",
       "      <td>12.333125</td>\n",
       "      <td>13.085431</td>\n",
       "      <td>12.103846</td>\n",
       "      <td>13.203876</td>\n",
       "      <td>12.234965</td>\n",
       "      <td>16.095082</td>\n",
       "      <td>13.003226</td>\n",
       "      <td>11.903030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Weight</td>\n",
       "      <td>86.577920</td>\n",
       "      <td>77.264983</td>\n",
       "      <td>92.744695</td>\n",
       "      <td>104.486362</td>\n",
       "      <td>81.392359</td>\n",
       "      <td>90.784658</td>\n",
       "      <td>85.972316</td>\n",
       "      <td>90.150508</td>\n",
       "      <td>84.859222</td>\n",
       "      <td>84.475390</td>\n",
       "      <td>83.910358</td>\n",
       "      <td>58.014297</td>\n",
       "      <td>69.825156</td>\n",
       "      <td>86.868134</td>\n",
       "      <td>101.572599</td>\n",
       "      <td>112.502694</td>\n",
       "      <td>132.875366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ph</td>\n",
       "      <td>7.438949</td>\n",
       "      <td>7.373615</td>\n",
       "      <td>7.376173</td>\n",
       "      <td>7.460001</td>\n",
       "      <td>7.375994</td>\n",
       "      <td>7.370807</td>\n",
       "      <td>7.388508</td>\n",
       "      <td>7.379810</td>\n",
       "      <td>7.353314</td>\n",
       "      <td>7.380083</td>\n",
       "      <td>7.372282</td>\n",
       "      <td>7.372073</td>\n",
       "      <td>7.379048</td>\n",
       "      <td>7.376131</td>\n",
       "      <td>7.897253</td>\n",
       "      <td>7.353929</td>\n",
       "      <td>7.365278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     General      Female        Male  Undefined Gender  \\\n",
       "0           ALP   93.477045   95.923397   94.204229         90.800756   \n",
       "1           ALT  311.856460  146.806237  299.865078        876.780331   \n",
       "2           AST  580.753358  632.946314  285.729871        663.955860   \n",
       "3       Albumin    1.994266    1.977296    2.006398          2.153890   \n",
       "4           BUN   24.293075   23.760087   26.193407         24.147696   \n",
       "5     Bilirubin    2.770370    2.151613    2.242935          0.000000   \n",
       "6   Cholesterol  117.165723  116.194360  117.916085        123.351923   \n",
       "7    Creatinine    1.347340    1.134677    1.479304          1.396137   \n",
       "8       DiasABP   62.718084   62.466381   63.073589         65.099445   \n",
       "9          FiO2    0.300710    0.294553    0.305745          0.324718   \n",
       "10          GCS    6.493009    6.484331    6.514906          6.688443   \n",
       "11      Glucose  126.797350  126.167444  129.474382        125.723023   \n",
       "12         HCO3   17.406941   17.287865   17.561093         18.668094   \n",
       "13          HCT   22.311316   22.328788   22.310200         23.153208   \n",
       "14           HR   86.695616   87.087076   86.312270         73.187500   \n",
       "15            K    2.565624    2.532001    2.592567          2.573728   \n",
       "16      Lactate    2.767346    2.657636    2.897482          0.000000   \n",
       "17          MAP   80.309701   80.750172   80.104122         76.000000   \n",
       "18     MechVent    0.003631    0.003696    0.003580          0.007679   \n",
       "19           Mg    2.011890    1.972207    2.063425          1.800000   \n",
       "20    NIDiasABP   62.984938   61.224616   64.601721         68.150676   \n",
       "21        NIMAP   77.730321   75.826498   78.288817         79.330002   \n",
       "22     NISysABP  119.478651  119.259779  118.476627         96.500000   \n",
       "23           Na   39.757599   39.759120   39.769347         40.111608   \n",
       "24        PaCO2   40.952782   40.324646   40.779975          0.000000   \n",
       "25         PaO2  148.291760  146.972273  148.144981          0.000000   \n",
       "26    Platelets  183.189953  195.089752  178.572231        180.791637   \n",
       "27     RespRate   19.431791   19.653187   19.083589         19.750000   \n",
       "28         SaO2   96.850858   96.174157   96.756989          0.000000   \n",
       "29       SysABP  120.123152  120.349732  120.887513        100.041666   \n",
       "30         Temp   34.710162   36.136897   33.576280         48.422422   \n",
       "31    TroponinI    6.738193    6.634211    6.430636          7.609278   \n",
       "32    TroponinT    0.928967    1.076113    1.147564          1.235715   \n",
       "33        Urine  116.098967  102.454298  123.367161        140.000000   \n",
       "34          WBC   12.264888   12.421652   13.113277         10.850000   \n",
       "35       Weight   86.577920   77.264983   92.744695        104.486362   \n",
       "36           Ph    7.438949    7.373615    7.376173          7.460001   \n",
       "\n",
       "           +65         -65   ICUType 1   ICUType 2   ICUType 3   ICUType 4  \\\n",
       "0    97.736268  112.724208   85.518225   79.479239  127.019654   90.321470   \n",
       "1   241.619627  414.153798  340.540936  521.730284  408.045663  300.914797   \n",
       "2   279.649209  449.902791  685.770612  355.311073  502.757992  641.779854   \n",
       "3     1.981838    2.007729    2.022257    1.914905    2.005524    2.025073   \n",
       "4    27.949329   21.970768   27.157634   20.862990   29.679509   20.261092   \n",
       "5     1.634444    3.295798    1.168000    2.325000    2.430097    2.844444   \n",
       "6   115.404416  119.041461  115.601955  115.020012  116.757318  120.068165   \n",
       "7     1.269102    1.310059    1.584878    1.154731    1.681413    1.109242   \n",
       "8    59.837164   66.474790   61.324229   58.838258   63.913077   65.301539   \n",
       "9     0.300769    0.301508    0.324819    0.312221    0.308347    0.273451   \n",
       "10    6.693542    6.333155    7.673461    6.993701    6.788229    5.327492   \n",
       "11  130.336436  130.839741  129.291353  123.197743  128.780477  123.720415   \n",
       "12   17.451662   17.339996   17.666147   17.617624   17.216361   17.349596   \n",
       "13   22.204833   22.415030   22.541695   21.290709   22.697641   22.563150   \n",
       "14   83.740056   89.761947   81.564406   85.908674   89.698826   86.933538   \n",
       "15    2.561794    2.564755    2.559777    2.655631    2.552924    2.525756   \n",
       "16    2.640000    2.648445    3.241936    3.061429    3.133816    2.883819   \n",
       "17   78.410036   82.558444   80.472733   75.112284   81.787286   84.901597   \n",
       "18    0.003640    0.003627    0.004139    0.004021    0.003518    0.003238   \n",
       "19    2.067133    1.980535    2.046341    2.154054    2.024653    1.941800   \n",
       "20   59.201504   67.409296   62.002919   56.065658   63.571879   64.033567   \n",
       "21   74.700983   79.444474   74.220484   71.194704   77.844048   79.753401   \n",
       "22  119.293391  120.102985  113.346308  111.447270  119.999088  126.006266   \n",
       "23   39.801245   39.758690   39.461356   40.248345   39.574430   39.802608   \n",
       "24   39.720028   40.965354   39.610236   40.511123   41.836149   39.545340   \n",
       "25  147.657934  154.812469  153.583333  168.760307  122.979042  145.571429   \n",
       "26  182.110300  186.584315  201.292159  160.211202  179.155845  182.631895   \n",
       "27   19.995241   18.735708   19.312523   20.079812   19.813428   18.940394   \n",
       "28   96.751957   96.036082   96.468354   96.947304   91.900000   96.571429   \n",
       "29  120.696313  120.083549  114.836960  114.615236  120.194244  127.635264   \n",
       "30   33.593713   35.933372   39.425467   20.576714   40.251795   35.453349   \n",
       "31    7.578575    5.814059    9.434904    7.066274    6.140485    6.407988   \n",
       "32    1.017049    0.960499    2.276089    0.860851    0.561091    0.658230   \n",
       "33  101.877095  138.273400  136.956401  109.523728  115.988397  122.130270   \n",
       "34   13.706148   12.079178   11.833636   14.028042   12.335628   12.333125   \n",
       "35   81.392359   90.784658   85.972316   90.150508   84.859222   84.475390   \n",
       "36    7.375994    7.370807    7.388508    7.379810    7.353314    7.380083   \n",
       "\n",
       "    Undefined classification  Low Weight  Normal Weight  Overweight  \\\n",
       "0                  89.091845  157.806162      98.746588   87.399715   \n",
       "1                 314.814130  219.491635     189.224936  648.496920   \n",
       "2                 307.952064  880.059385     209.915174  682.205256   \n",
       "3                   2.051419    1.967156       1.909736    1.948019   \n",
       "4                  25.329214   27.332073      25.181038   24.430181   \n",
       "5                   2.758427    8.280000       3.455172    1.586111   \n",
       "6                 118.743108  118.607903     113.920232  116.190098   \n",
       "7                   1.379090    1.009123       1.378736    1.243383   \n",
       "8                  65.166037   61.570727      60.634648   60.923928   \n",
       "9                   0.299435    0.266262       0.290676    0.303029   \n",
       "10                  6.503094    6.374173       6.388281    6.601707   \n",
       "11                126.110103  147.982383     125.297818  126.396712   \n",
       "12                 17.542259   16.886006      17.063045   17.313884   \n",
       "13                 22.743585   22.173624      21.761719   21.737766   \n",
       "14                 86.616159   88.645670      86.162043   85.523481   \n",
       "15                  2.547907    2.459405       2.534051    2.597519   \n",
       "16                  2.618357    4.560000       2.899519    2.985465   \n",
       "17                 84.149771   83.815904      77.193152   78.669410   \n",
       "18                  0.003729    0.003232       0.003553    0.003630   \n",
       "19                  1.951538    2.120000       2.053659    2.013287   \n",
       "20                 63.925443   61.880576      59.495839   61.169317   \n",
       "21                 77.791639   76.076136      74.734905   75.650896   \n",
       "22                121.201091  120.983401     114.405666  115.661460   \n",
       "23                 39.755696   39.817644      39.894110   39.839113   \n",
       "24                 41.313559   39.808824      39.832765   40.217234   \n",
       "25                131.058248  194.118421     160.313054  150.281159   \n",
       "26                194.720072  165.269997     167.570452  166.780473   \n",
       "27                 19.781309   21.476190      18.323431   19.308421   \n",
       "28                 95.984848   96.607143      97.062500   96.748031   \n",
       "29                124.791785  124.106029     115.433673  117.777673   \n",
       "30                 41.030899   28.656735      29.969903   26.543015   \n",
       "31                  6.627917    6.799923       7.238823    6.958370   \n",
       "32                  0.705395    2.158369       0.986757    1.469293   \n",
       "33                123.381353  117.989932     105.160684  106.280932   \n",
       "34                 13.085431   12.103846      13.203876   12.234965   \n",
       "35                 83.910358   58.014297      69.825156   86.868134   \n",
       "36                  7.372282    7.372073       7.379048    7.376131   \n",
       "\n",
       "     Obesity 1   Obesity 2    Obesity 3  \n",
       "0    82.485978   64.762371    74.295819  \n",
       "1   268.583071  776.440910  1629.534974  \n",
       "2   413.126054  193.328673   148.148190  \n",
       "3     1.956438    1.967108     1.943897  \n",
       "4    26.966782   28.234565    26.974401  \n",
       "5     2.593333    1.644445     2.216667  \n",
       "6   115.977272  118.668994   117.289936  \n",
       "7     1.278136    1.321840     1.589997  \n",
       "8    62.216848   62.799902    62.390182  \n",
       "9     0.316807    0.312052     0.311929  \n",
       "10    6.359520    6.987679     6.269248  \n",
       "11  123.643509  124.378041   144.675040  \n",
       "12   17.460329   17.685992    17.611188  \n",
       "13   22.220204   22.066124    22.389278  \n",
       "14   86.934679   89.150278    87.773425  \n",
       "15    2.610494    2.668946     2.731433  \n",
       "16    3.297778    2.950000     3.423611  \n",
       "17   79.079877   79.401571    78.636926  \n",
       "18    0.003485    0.003618     0.003342  \n",
       "19    1.964286    1.948148     2.037143  \n",
       "20   64.029927   61.478189    61.690531  \n",
       "21   75.188949   75.818271    75.591905  \n",
       "22  118.645289  113.352273   114.680766  \n",
       "23   39.668110   39.647284    39.306893  \n",
       "24   39.360606   39.861539    41.186047  \n",
       "25  131.040698  150.492308   121.220974  \n",
       "26  184.509999  170.046672   163.075461  \n",
       "27   19.633987   20.693182    19.796296  \n",
       "28   96.778689   96.277778    96.037037  \n",
       "29  120.066041  115.811267   117.612574  \n",
       "30   27.848700   28.967912    29.829309  \n",
       "31    6.368796    6.004809     5.882983  \n",
       "32    1.139178    3.844033     3.242489  \n",
       "33  119.936848  109.442052   108.465589  \n",
       "34   16.095082   13.003226    11.903030  \n",
       "35  101.572599  112.502694   132.875366  \n",
       "36    7.897253    7.353929     7.365278  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saits_mae_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "FiO2\n",
      "0.5427910102481897\n",
      "--------------------\n",
      "Female\n",
      "FiO2\n",
      "0.5322841955806464\n",
      "--------------------\n",
      "Male\n",
      "FiO2\n",
      "0.558764148292676\n",
      "--------------------\n",
      "Undefined Gender\n",
      "TroponinT\n",
      "0.0\n",
      "--------------------\n",
      "+65\n",
      "FiO2\n",
      "0.5509840135837525\n",
      "--------------------\n",
      "-65\n",
      "FiO2\n",
      "0.5440240574521423\n",
      "--------------------\n",
      "ICUType 1\n",
      "FiO2\n",
      "0.5382165582316663\n",
      "--------------------\n",
      "ICUType 2\n",
      "Cholesterol\n",
      "0.0\n",
      "--------------------\n",
      "ICUType 3\n",
      "TroponinT\n",
      "0.4359321938985411\n",
      "--------------------\n",
      "ICUType 4\n",
      "TroponinT\n",
      "0.4575000104183762\n",
      "--------------------\n",
      "Undefined classification\n",
      "FiO2\n",
      "0.5253448272903851\n",
      "--------------------\n",
      "Low Weight\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Normal Weight\n",
      "FiO2\n",
      "0.5370249722019451\n",
      "--------------------\n",
      "Overweight\n",
      "FiO2\n",
      "0.5362397804247246\n",
      "--------------------\n",
      "Obesity 1\n",
      "FiO2\n",
      "0.5825757534816982\n",
      "--------------------\n",
      "Obesity 2\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Obesity 3\n",
      "Cholesterol\n",
      "0.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.min_value_in_subgroup(df_saits_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Maximum MAE value in each subgroup </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "AST\n",
      "566.9945693689813\n",
      "--------------------\n",
      "Female\n",
      "AST\n",
      "625.6842090706995\n",
      "--------------------\n",
      "Male\n",
      "ALT\n",
      "281.3000011026836\n",
      "--------------------\n",
      "Undefined Gender\n",
      "Urine\n",
      "139.99999999986\n",
      "--------------------\n",
      "+65\n",
      "AST\n",
      "239.44805549026773\n",
      "--------------------\n",
      "-65\n",
      "AST\n",
      "430.2633963482682\n",
      "--------------------\n",
      "ICUType 1\n",
      "AST\n",
      "683.9200496291841\n",
      "--------------------\n",
      "ICUType 2\n",
      "ALT\n",
      "497.31578862037054\n",
      "--------------------\n",
      "ICUType 3\n",
      "AST\n",
      "491.05494833516013\n",
      "--------------------\n",
      "ICUType 4\n",
      "AST\n",
      "630.3636355139878\n",
      "--------------------\n",
      "Undefined classification\n",
      "ALT\n",
      "291.7088629807061\n",
      "--------------------\n",
      "Low Weight\n",
      "AST\n",
      "889.0000677106542\n",
      "--------------------\n",
      "Normal Weight\n",
      "AST\n",
      "177.37499982118052\n",
      "--------------------\n",
      "Overweight\n",
      "AST\n",
      "667.7142724082266\n",
      "--------------------\n",
      "Obesity 1\n",
      "AST\n",
      "388.75000405309646\n",
      "--------------------\n",
      "Obesity 2\n",
      "ALT\n",
      "772.2857140131256\n",
      "--------------------\n",
      "Obesity 3\n",
      "ALT\n",
      "1688.3333333327705\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.max_value_in_subgroup(df_saits_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SAITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_saits_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saits_mae_minmax_ori = toolkits.create_table(testing_mae_saits_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saits_mae_minmax_ori "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_saits_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_saits_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BRITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BRITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_brits_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_standard = toolkits.create_table(testing_mae_brits_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_standard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_brits_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_brits_mae_standard, subgroups, variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BRITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_brits_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_standard_ori = toolkits.create_table(testing_mae_brits_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_standard_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_brits_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_brits_mae_standard_ori, subgroups, variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> MinMax Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BRITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_brits_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_minmax = toolkits.create_table(testing_mae_brits_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_minmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_brits_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_brits_mae_minmax, subgroups, variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> MinMax Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BRITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_brits_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_minmax_ori = toolkits.create_table(testing_mae_brits_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_minmax_ori "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_brits_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_brits_mae_minmax_ori, subgroups, variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USGAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"USGAN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_usgan_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_standard = toolkits.create_table(testing_mae_saits_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_standard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_usgan_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_usgan_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"USGAN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_usgan_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_standard_ori = toolkits.create_table(testing_mae_usgan_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_standard_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_usgan_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_usgan_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"USGAN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_usgan_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_minmax = toolkits.create_table(testing_mae_usgan_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_usgan_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_usgan_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"USGAN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_usgan_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_minmax_ori = toolkits.create_table(testing_mae_usgan_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_minmax_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_usgan_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_usgan_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPVAE - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_gpvae_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_standard = toolkits.create_table(testing_mae_gpvae_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_standard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_gpvae_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_gpvae_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPVAE - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_gpvae_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_standard_ori = toolkits.create_table(testing_mae_gpvae_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_standard_ori "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_gpvae_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_gpvae_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPVAE - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_gpvae_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_minmax = toolkits.create_table(testing_mae_gpvae_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_gpvae_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_gpvae_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPVAE - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_gpvae_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_minmax_ori = toolkits.create_table(testing_mae_gpvae_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_minmax_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_gpvae_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_gpvae_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MRNN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_mrnn_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_standard = toolkits.create_table(testing_mae_mrnn_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_standard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_mrnn_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_mrnn_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MRNN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_mrnn_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_standard_ori = toolkits.create_table(testing_mae_mrnn_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_standard_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_mrnn_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_mrnn_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MRNN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_mrnn_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_minmax = toolkits.create_table(testing_mae_mrnn_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_mrnn_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_mrnn_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MRNN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_mrnn_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_minmax_ori = toolkits.create_table(testing_mae_mrnn_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_minmax_ori "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_mrnn_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_mrnn_mae_minmax_ori, subgroups, variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
