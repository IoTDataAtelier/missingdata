{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 15:06:59.005858: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742148419.021109  242464 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742148419.026036  242464 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-16 15:06:59.043039: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/pypots/nn/functional/cuda.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  return autocast(**kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-16 15:07:01 [WARNING]: ‼️ `pypots.utils.metrics` is deprecated. Please import from `pypots.nn.functional` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "████████╗██╗███╗   ███╗███████╗    ███████╗███████╗██████╗ ██╗███████╗███████╗    █████╗ ██╗\n",
      "╚══██╔══╝██║████╗ ████║██╔════╝    ██╔════╝██╔════╝██╔══██╗██║██╔════╝██╔════╝   ██╔══██╗██║\n",
      "   ██║   ██║██╔████╔██║█████╗█████╗███████╗█████╗  ██████╔╝██║█████╗  ███████╗   ███████║██║\n",
      "   ██║   ██║██║╚██╔╝██║██╔══╝╚════╝╚════██║██╔══╝  ██╔══██╗██║██╔══╝  ╚════██║   ██╔══██║██║\n",
      "   ██║   ██║██║ ╚═╝ ██║███████╗    ███████║███████╗██║  ██║██║███████╗███████║██╗██║  ██║██║\n",
      "   ╚═╝   ╚═╝╚═╝     ╚═╝╚══════╝    ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝╚═╝╚═╝  ╚═╝╚═╝\n",
      "ai4ts v0.0.3 - building AI for unified time-series analysis, https://time-series.ai \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pypots\n",
    "import os\n",
    "import sys\n",
    "from pypots.nn.functional import calc_mae\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS, BRITS, USGAN, GPVAE, MRNN\n",
    "import numpy as np\n",
    "import benchpots\n",
    "from pypots.utils.random import set_random_seed\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import pandas as pd    \n",
    "from pypotsModify.benchpotsMAE.datasets import preprocess_physionet2012\n",
    "from missingData.toolkits import toolkits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 15:07:15 [INFO]: Have set the random seed as 2022 for numpy and pytorch.\n",
      "2025-03-16 15:07:15 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2025-03-16 15:07:15 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2025-03-16 15:07:15 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2025-03-16 15:07:15 [INFO]: Loaded successfully!\n",
      "2025-03-16 15:07:29 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-03-16 15:07:29 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-03-16 15:07:29 [INFO]: 68807 values masked out in the val set as ground truth, take 9.97% of the original observed values\n",
      "2025-03-16 15:07:29 [INFO]: 68807 values masked out in the val set as ground truth, take 9.97% of the original observed values\n",
      "2025-03-16 15:07:29 [INFO]: 86319 values masked out in the test set as ground truth, take 9.99% of the original observed values\n",
      "2025-03-16 15:07:29 [INFO]: 86319 values masked out in the test set as ground truth, take 9.99% of the original observed values\n",
      "2025-03-16 15:07:29 [INFO]: Total sample number: 11988\n",
      "2025-03-16 15:07:29 [INFO]: Total sample number: 11988\n",
      "2025-03-16 15:07:29 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-03-16 15:07:29 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-03-16 15:07:29 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-03-16 15:07:29 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-03-16 15:07:29 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-03-16 15:07:29 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-03-16 15:07:29 [INFO]: Number of steps: 48\n",
      "2025-03-16 15:07:29 [INFO]: Number of steps: 48\n",
      "2025-03-16 15:07:29 [INFO]: Number of features: 37\n",
      "2025-03-16 15:07:29 [INFO]: Number of features: 37\n",
      "2025-03-16 15:07:29 [INFO]: Train set missing rate: 79.70%\n",
      "2025-03-16 15:07:29 [INFO]: Train set missing rate: 79.70%\n",
      "2025-03-16 15:07:29 [INFO]: Validating set missing rate: 81.75%\n",
      "2025-03-16 15:07:29 [INFO]: Validating set missing rate: 81.75%\n",
      "2025-03-16 15:07:29 [INFO]: Test set missing rate: 81.75%\n",
      "2025-03-16 15:07:29 [INFO]: Test set missing rate: 81.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['n_classes', 'n_steps', 'n_features', 'scaler', 'train_X', 'train_y', 'train_ICUType', 'val_X', 'val_y', 'val_ICUType', 'test_X', 'test_y', 'test_ICUType', 'female_gender_test_X', 'female_gender_test_y', 'test_ICUType_female_gender', 'male_gender_test_X', 'male_gender_test_y', 'test_ICUType_male_gender', 'undefined_gender_test_X', 'undefined_gender_test_y', 'test_ICUType_undefined_gender', 'more_than_or_equal_to_65_test_X', 'more_than_or_equal_to_65_test_y', 'test_ICUType_more_than_or_equal_to_65', 'less_than_65_test_X', 'less_than_65_test_y', 'test_ICUType_less_than_65', 'ICUType_1_test_X', 'ICUType_1_test_y', 'test_ICUType_1', 'ICUType_2_test_X', 'ICUType_2_test_y', 'test_ICUType_2', 'ICUType_3_test_X', 'ICUType_3_test_y', 'test_ICUType_3', 'ICUType_4_test_X', 'ICUType_4_test_y', 'test_ICUType_4', 'classificacao_undefined_test_X', 'classificacao_undefined_test_y', 'test_ICUType_classificacao_undefined', 'classificacao_baixo_peso_test_X', 'classificacao_baixo_peso_test_y', 'test_ICUType_classificao_baixo_peso', 'classificacao_normal_peso_test_X', 'classificacao_normal_peso_test_y', 'test_ICUType_classificacao_normal_peso', 'classificacao_sobrepeso_test_X', 'classificacao_sobrepeso_test_y', 'test_ICUType_classificacao_sobrepeso', 'classificacao_obesidade_1_test_X', 'classificacao_obesidade_1_test_y', 'test_ICUType_classificacao_obesidade_1', 'classificacao_obesidade_2_test_X', 'classificacao_obesidade_2_test_y', 'test_ICUType_classificacao_obesidade_2', 'classificacao_obesidade_3_test_X', 'classificacao_obesidade_3_test_y', 'test_ICUType_classificacao_obesidade_3', 'val_X_ori', 'test_X_ori', 'female_gender_test_X_ori', 'male_gender_test_X_ori', 'undefined_gender_test_X_ori', 'more_than_or_equal_to_65_test_X_ori', 'less_than_65_test_X_ori', 'ICUType_1_test_X_ori', 'ICUType_2_test_X_ori', 'ICUType_3_test_X_ori', 'ICUType_4_test_X_ori', 'classificacao_undefined_test_X_ori', 'classificacao_baixo_peso_test_X_ori', 'classificacao_normal_peso_test_X_ori', 'classificacao_sobrepeso_test_X_ori', 'classificacao_obesidade_1_test_X_ori', 'classificacao_obesidade_2_test_X_ori', 'classificacao_obesidade_3_test_X_ori'])\n"
     ]
    }
   ],
   "source": [
    "set_random_seed()\n",
    "physionet2012_dataset_standard = preprocess_physionet2012(subset=\"all\", rate=0.1, normalization=1)\n",
    "print(physionet2012_dataset_standard.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_standard = physionet2012_dataset_standard[\"scaler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_training_standard, dataset_for_validating_standard, dataset_for_testing_ori_standard, dataset_for_testing_standard = toolkits.separating_dataset(physionet2012_dataset_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 15:07:41 [INFO]: Have set the random seed as 2022 for numpy and pytorch.\n",
      "2025-03-16 15:07:41 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2025-03-16 15:07:41 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2025-03-16 15:07:41 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2025-03-16 15:07:41 [INFO]: Loaded successfully!\n",
      "2025-03-16 15:07:54 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-03-16 15:07:54 [WARNING]: Note that physionet_2012 has sparse observations in the time series, hence we don't add additional missing values to the training dataset. \n",
      "2025-03-16 15:07:55 [INFO]: 68807 values masked out in the val set as ground truth, take 9.97% of the original observed values\n",
      "2025-03-16 15:07:55 [INFO]: 68807 values masked out in the val set as ground truth, take 9.97% of the original observed values\n",
      "2025-03-16 15:07:55 [INFO]: 86319 values masked out in the test set as ground truth, take 9.99% of the original observed values\n",
      "2025-03-16 15:07:55 [INFO]: 86319 values masked out in the test set as ground truth, take 9.99% of the original observed values\n",
      "2025-03-16 15:07:55 [INFO]: Total sample number: 11988\n",
      "2025-03-16 15:07:55 [INFO]: Total sample number: 11988\n",
      "2025-03-16 15:07:55 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-03-16 15:07:55 [INFO]: Training set size: 7671 (63.99%)\n",
      "2025-03-16 15:07:55 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-03-16 15:07:55 [INFO]: Validation set size: 1918 (16.00%)\n",
      "2025-03-16 15:07:55 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-03-16 15:07:55 [INFO]: Test set size: 2399 (20.01%)\n",
      "2025-03-16 15:07:55 [INFO]: Number of steps: 48\n",
      "2025-03-16 15:07:55 [INFO]: Number of steps: 48\n",
      "2025-03-16 15:07:55 [INFO]: Number of features: 37\n",
      "2025-03-16 15:07:55 [INFO]: Number of features: 37\n",
      "2025-03-16 15:07:55 [INFO]: Train set missing rate: 79.70%\n",
      "2025-03-16 15:07:55 [INFO]: Train set missing rate: 79.70%\n",
      "2025-03-16 15:07:55 [INFO]: Validating set missing rate: 81.75%\n",
      "2025-03-16 15:07:55 [INFO]: Validating set missing rate: 81.75%\n",
      "2025-03-16 15:07:55 [INFO]: Test set missing rate: 81.75%\n",
      "2025-03-16 15:07:55 [INFO]: Test set missing rate: 81.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['n_classes', 'n_steps', 'n_features', 'scaler', 'train_X', 'train_y', 'train_ICUType', 'val_X', 'val_y', 'val_ICUType', 'test_X', 'test_y', 'test_ICUType', 'female_gender_test_X', 'female_gender_test_y', 'test_ICUType_female_gender', 'male_gender_test_X', 'male_gender_test_y', 'test_ICUType_male_gender', 'undefined_gender_test_X', 'undefined_gender_test_y', 'test_ICUType_undefined_gender', 'more_than_or_equal_to_65_test_X', 'more_than_or_equal_to_65_test_y', 'test_ICUType_more_than_or_equal_to_65', 'less_than_65_test_X', 'less_than_65_test_y', 'test_ICUType_less_than_65', 'ICUType_1_test_X', 'ICUType_1_test_y', 'test_ICUType_1', 'ICUType_2_test_X', 'ICUType_2_test_y', 'test_ICUType_2', 'ICUType_3_test_X', 'ICUType_3_test_y', 'test_ICUType_3', 'ICUType_4_test_X', 'ICUType_4_test_y', 'test_ICUType_4', 'classificacao_undefined_test_X', 'classificacao_undefined_test_y', 'test_ICUType_classificacao_undefined', 'classificacao_baixo_peso_test_X', 'classificacao_baixo_peso_test_y', 'test_ICUType_classificao_baixo_peso', 'classificacao_normal_peso_test_X', 'classificacao_normal_peso_test_y', 'test_ICUType_classificacao_normal_peso', 'classificacao_sobrepeso_test_X', 'classificacao_sobrepeso_test_y', 'test_ICUType_classificacao_sobrepeso', 'classificacao_obesidade_1_test_X', 'classificacao_obesidade_1_test_y', 'test_ICUType_classificacao_obesidade_1', 'classificacao_obesidade_2_test_X', 'classificacao_obesidade_2_test_y', 'test_ICUType_classificacao_obesidade_2', 'classificacao_obesidade_3_test_X', 'classificacao_obesidade_3_test_y', 'test_ICUType_classificacao_obesidade_3', 'val_X_ori', 'test_X_ori', 'female_gender_test_X_ori', 'male_gender_test_X_ori', 'undefined_gender_test_X_ori', 'more_than_or_equal_to_65_test_X_ori', 'less_than_65_test_X_ori', 'ICUType_1_test_X_ori', 'ICUType_2_test_X_ori', 'ICUType_3_test_X_ori', 'ICUType_4_test_X_ori', 'classificacao_undefined_test_X_ori', 'classificacao_baixo_peso_test_X_ori', 'classificacao_normal_peso_test_X_ori', 'classificacao_sobrepeso_test_X_ori', 'classificacao_obesidade_1_test_X_ori', 'classificacao_obesidade_2_test_X_ori', 'classificacao_obesidade_3_test_X_ori'])\n"
     ]
    }
   ],
   "source": [
    "set_random_seed()\n",
    "physionet2012_dataset_minmax = preprocess_physionet2012(subset=\"all\", rate=0.1, normalization=2)\n",
    "print(physionet2012_dataset_minmax.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_minmax = physionet2012_dataset_minmax[\"scaler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_training_minmax, dataset_for_validating_minmax, dataset_for_testing_minmax, dataset_for_testing_ori_minmax = toolkits.separating_dataset(physionet2012_dataset_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating indicating mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_testing_ori_standard_norm = toolkits.dict_to_list(dataset_for_testing_ori_standard)\n",
    "dataset_for_testing_standard_norm = toolkits.dict_to_list(dataset_for_testing_standard)\n",
    "indicating_mask_variable_standard_norm, test_X_ori_variable_standard_norm = toolkits.components_mae(dataset_for_testing_ori_standard_norm, dataset_for_testing_standard_norm)\n",
    "indicating_mask_variable_standard_norm = toolkits.pre_reshape(indicating_mask_variable_standard_norm)\n",
    "test_X_ori_variable_standard_norm = toolkits.pre_reshape(test_X_ori_variable_standard_norm)\n",
    "indicating_mask_variable_standard_norm = toolkits.reshape_variable(indicating_mask_variable_standard_norm)\n",
    "test_X_ori_variable_standard_norm = toolkits.reshape_variable(test_X_ori_variable_standard_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_testing_ori_standard_des = toolkits.dict_to_list(dataset_for_testing_ori_standard)\n",
    "dataset_for_testing_standard_des = toolkits.dict_to_list(dataset_for_testing_standard)\n",
    "dataset_for_testing_ori_standard_des = toolkits.pre_reshape(dataset_for_testing_ori_standard_des)\n",
    "dataset_for_testing_standard_des = toolkits.pre_reshape(dataset_for_testing_standard_des)\n",
    "dataset_for_testing_ori_standard_des = toolkits.desnormalization(dataset_for_testing_ori_standard_des, scaler_standard)\n",
    "dataset_for_testing_standard_des = toolkits.desnormalization(dataset_for_testing_standard_des, scaler_standard)\n",
    "indicating_mask_variable_standard_des, test_X_ori_variable_standard_des = toolkits.components_mae(dataset_for_testing_ori_standard_des, dataset_for_testing_standard_des)\n",
    "indicating_mask_variable_standard_des = toolkits.reshape_variable(indicating_mask_variable_standard_des)\n",
    "test_X_ori_variable_standard_des = toolkits.reshape_variable(test_X_ori_variable_standard_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_testing_ori_minmax_norm = toolkits.dict_to_list(dataset_for_testing_ori_minmax)\n",
    "dataset_for_testing_minmax_norm = toolkits.dict_to_list(dataset_for_testing_minmax)\n",
    "indicating_mask_variable_minmax_norm, test_X_ori_variable_minmax_norm = toolkits.components_mae(dataset_for_testing_ori_minmax_norm, dataset_for_testing_minmax_norm)\n",
    "indicating_mask_variable_minmax_norm = toolkits.pre_reshape(indicating_mask_variable_minmax_norm)\n",
    "test_X_ori_variable_minmax_norm = toolkits.pre_reshape(test_X_ori_variable_minmax_norm)\n",
    "indicating_mask_variable_minmax_norm = toolkits.reshape_variable(indicating_mask_variable_minmax_norm)\n",
    "test_X_ori_variable_minmax_norm = toolkits.reshape_variable(test_X_ori_variable_minmax_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_testing_ori_minmax_des = toolkits.dict_to_list(dataset_for_testing_ori_minmax)\n",
    "dataset_for_testing_minmax_des = toolkits.dict_to_list(dataset_for_testing_minmax)\n",
    "dataset_for_testing_ori_minmax_des = toolkits.pre_reshape(dataset_for_testing_ori_minmax_des)\n",
    "dataset_for_testing_minmax_des = toolkits.pre_reshape(dataset_for_testing_minmax_des)\n",
    "dataset_for_testing_ori_minmax_des = toolkits.desnormalization(dataset_for_testing_ori_minmax_des, scaler_minmax)\n",
    "dataset_for_testing_minmax_des = toolkits.desnormalization(dataset_for_testing_minmax_des, scaler_minmax)\n",
    "indicating_mask_variable_minmax_des, test_X_ori_variable_minmax_des = toolkits.components_mae(dataset_for_testing_ori_minmax_des, dataset_for_testing_minmax_des)\n",
    "indicating_mask_variable_minmax_des = toolkits.reshape_variable(indicating_mask_variable_minmax_des)\n",
    "test_X_ori_variable_minmax_des = toolkits.reshape_variable(test_X_ori_variable_minmax_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialize the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 15:11:15 [INFO]: No given device, using default device: cpu\n",
      "2025-03-16 15:11:15 [INFO]: Model files will be saved to tutorial_results/imputation/saits/20250316_T151115\n",
      "2025-03-16 15:11:15 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits/20250316_T151115/tensorboard\n",
      "2025-03-16 15:11:15 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 720,182\n"
     ]
    }
   ],
   "source": [
    "saits_standard = SAITS(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/saits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 14:52:27 [INFO]: No given device, using default device: cpu\n",
      "2025-03-16 14:52:27 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-03-16 14:52:27 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 720,182\n"
     ]
    }
   ],
   "source": [
    "saits_standard = SAITS(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 15:11:25 [INFO]: No given device, using default device: cpu\n",
      "2025-03-16 15:11:25 [INFO]: Model files will be saved to tutorial_results/imputation/saits/20250316_T151125\n",
      "2025-03-16 15:11:25 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits/20250316_T151125/tensorboard\n",
      "2025-03-16 15:11:25 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 720,182\n"
     ]
    }
   ],
   "source": [
    "saits_minmax = SAITS(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/saits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits_minmax = SAITS(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    n_layers=1,\n",
    "    d_model=256,\n",
    "    d_ffn=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 22:14:19 [INFO]: No given device, using default device: cpu\n",
      "2025-03-14 22:14:19 [INFO]: Model files will be saved to tutorial_results/imputation/brits/20250314_T221419\n",
      "2025-03-14 22:14:19 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/brits/20250314_T221419/tensorboard\n",
      "2025-03-14 22:14:19 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 239,344\n"
     ]
    }
   ],
   "source": [
    "brits_standard = BRITS(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/brits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:04:33 [INFO]: No given device, using default device: cpu\n",
      "2025-02-25 21:04:33 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-25 21:04:33 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 239,344\n"
     ]
    }
   ],
   "source": [
    "brits_standard = BRITS(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 22:14:24 [INFO]: No given device, using default device: cpu\n",
      "2025-03-14 22:14:24 [INFO]: Model files will be saved to tutorial_results/imputation/brits/20250314_T221424\n",
      "2025-03-14 22:14:24 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/brits/20250314_T221424/tensorboard\n",
      "2025-03-14 22:14:24 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 239,344\n"
     ]
    }
   ],
   "source": [
    "brits_minmax = BRITS(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/brits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits_minmax = BRITS(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 22:30:35 [INFO]: No given device, using default device: cpu\n",
      "2025-03-14 22:30:35 [INFO]: Model files will be saved to tutorial_results/imputation/us_gan/20250314_T223035\n",
      "2025-03-14 22:30:35 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/us_gan/20250314_T223035/tensorboard\n",
      "2025-03-14 22:30:36 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 1,258,517\n"
     ]
    }
   ],
   "source": [
    "us_gan_standard = USGAN(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=256,\n",
    "    lambda_mse=1,\n",
    "    dropout=0.1,\n",
    "    G_steps=1,\n",
    "    D_steps=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/us_gan\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:04:37 [INFO]: No given device, using default device: cpu\n",
      "2025-02-25 21:04:37 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-25 21:04:37 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 1,258,517\n"
     ]
    }
   ],
   "source": [
    "us_gan_standard = USGAN(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=256,\n",
    "    lambda_mse=1,\n",
    "    dropout=0.1,\n",
    "    G_steps=1,\n",
    "    D_steps=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 22:30:41 [INFO]: No given device, using default device: cpu\n",
      "2025-03-14 22:30:41 [INFO]: Model files will be saved to tutorial_results/imputation/us_gan/20250314_T223041\n",
      "2025-03-14 22:30:41 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/us_gan/20250314_T223041/tensorboard\n",
      "2025-03-14 22:30:41 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 1,258,517\n"
     ]
    }
   ],
   "source": [
    "us_gan_minmax = USGAN(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=256,\n",
    "    lambda_mse=1,\n",
    "    dropout=0.1,\n",
    "    G_steps=1,\n",
    "    D_steps=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/us_gan\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_gan_minmax = USGAN(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=256,\n",
    "    lambda_mse=1,\n",
    "    dropout=0.1,\n",
    "    G_steps=1,\n",
    "    D_steps=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:04:32 [INFO]: No given device, using default device: cpu\n",
      "2025-03-14 23:04:32 [INFO]: Model files will be saved to tutorial_results/imputation/gp_vae/20250314_T230432\n",
      "2025-03-14 23:04:32 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/gp_vae/20250314_T230432/tensorboard\n",
      "2025-03-14 23:04:32 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 229,652\n"
     ]
    }
   ],
   "source": [
    "gp_vae_standard = GPVAE(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    latent_size=37,\n",
    "    encoder_sizes=(128,128),\n",
    "    decoder_sizes=(256,256),\n",
    "    kernel=\"cauchy\",\n",
    "    beta=0.2,\n",
    "    M=1,\n",
    "    K=1,\n",
    "    sigma=1.005,\n",
    "    length_scale=7.0,\n",
    "    kernel_scales=1,\n",
    "    window_size=24,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/gp_vae\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:04:42 [INFO]: No given device, using default device: cpu\n",
      "2025-02-25 21:04:42 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-25 21:04:42 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 229,652\n"
     ]
    }
   ],
   "source": [
    "gp_vae_standard = GPVAE(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    latent_size=37,\n",
    "    encoder_sizes=(128,128),\n",
    "    decoder_sizes=(256,256),\n",
    "    kernel=\"cauchy\",\n",
    "    beta=0.2,\n",
    "    M=1,\n",
    "    K=1,\n",
    "    sigma=1.005,\n",
    "    length_scale=7.0,\n",
    "    kernel_scales=1,\n",
    "    window_size=24,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:04:44 [INFO]: No given device, using default device: cpu\n",
      "2025-03-14 23:04:44 [INFO]: Model files will be saved to tutorial_results/imputation/gp_vae/20250314_T230444\n",
      "2025-03-14 23:04:44 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/gp_vae/20250314_T230444/tensorboard\n",
      "2025-03-14 23:04:44 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 229,652\n"
     ]
    }
   ],
   "source": [
    "gp_vae_minmax = GPVAE(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    latent_size=37,\n",
    "    encoder_sizes=(128,128),\n",
    "    decoder_sizes=(256,256),\n",
    "    kernel=\"cauchy\",\n",
    "    beta=0.2,\n",
    "    M=1,\n",
    "    K=1,\n",
    "    sigma=1.005,\n",
    "    length_scale=7.0,\n",
    "    kernel_scales=1,\n",
    "    window_size=24,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/gp_vae\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_vae_minmax= GPVAE(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    latent_size=37,\n",
    "    encoder_sizes=(128,128),\n",
    "    decoder_sizes=(256,256),\n",
    "    kernel=\"cauchy\",\n",
    "    beta=0.2,\n",
    "    M=1,\n",
    "    K=1,\n",
    "    sigma=1.005,\n",
    "    length_scale=7.0,\n",
    "    kernel_scales=1,\n",
    "    window_size=24,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:18:57 [INFO]: No given device, using default device: cpu\n",
      "2025-03-14 23:18:57 [INFO]: Model files will be saved to tutorial_results/imputation/mrnn/20250314_T231857\n",
      "2025-03-14 23:18:57 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/mrnn/20250314_T231857/tensorboard\n",
      "2025-03-14 23:18:57 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 107,951\n"
     ]
    }
   ],
   "source": [
    "mrnn_standard = MRNN(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/mrnn\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (Standard)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:04:45 [INFO]: No given device, using default device: cpu\n",
      "2025-02-25 21:04:45 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-02-25 21:04:45 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 107,951\n"
     ]
    }
   ],
   "source": [
    "mrnn_standard = MRNN(\n",
    "    n_steps=physionet2012_dataset_standard['n_steps'],\n",
    "    n_features=physionet2012_dataset_standard['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize new model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:19:01 [INFO]: No given device, using default device: cpu\n",
      "2025-03-14 23:19:01 [INFO]: Model files will be saved to tutorial_results/imputation/mrnn/20250314_T231901\n",
      "2025-03-14 23:19:01 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/mrnn/20250314_T231901/tensorboard\n",
      "2025-03-14 23:19:01 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 107,951\n"
     ]
    }
   ],
   "source": [
    "mrnn_minmax = MRNN(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/mrnn\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Inicialize existing model (MinMax)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_minmax = MRNN(\n",
    "    n_steps=physionet2012_dataset_minmax['n_steps'],\n",
    "    n_features=physionet2012_dataset_minmax['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Load the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - Standard Scaler </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 15:11:49 [INFO]: Epoch 001 - training loss (MSE): 0.8186, validation MSE: 6.7439\n",
      "2025-03-16 15:11:59 [INFO]: Epoch 002 - training loss (MSE): 0.5314, validation MSE: 6.6840\n",
      "2025-03-16 15:12:08 [INFO]: Epoch 003 - training loss (MSE): 0.5029, validation MSE: 6.6729\n",
      "2025-03-16 15:12:18 [INFO]: Epoch 004 - training loss (MSE): 0.4546, validation MSE: 6.6573\n",
      "2025-03-16 15:12:28 [INFO]: Epoch 005 - training loss (MSE): 0.4356, validation MSE: 6.6584\n",
      "2025-03-16 15:12:37 [INFO]: Epoch 006 - training loss (MSE): 0.4727, validation MSE: 6.6585\n",
      "2025-03-16 15:12:47 [INFO]: Epoch 007 - training loss (MSE): 0.4030, validation MSE: 6.6770\n",
      "2025-03-16 15:12:47 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-03-16 15:12:47 [INFO]: Finished training. The best model is from epoch#4.\n",
      "2025-03-16 15:12:47 [INFO]: Saved the model to tutorial_results/imputation/saits/20250316_T151115/SAITS.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "saits_standard.fit(train_set=dataset_for_training_standard, val_set=dataset_for_validating_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 22:11:19 [INFO]: Epoch 001 - training loss (MSE): 0.0360, validation MSE: 0.0082\n",
      "2025-03-14 22:11:19 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS_epoch1_loss0.0082.pypots\n",
      "2025-03-14 22:11:29 [INFO]: Epoch 002 - training loss (MSE): 0.0163, validation MSE: 0.0064\n",
      "2025-03-14 22:11:29 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS_epoch2_loss0.0064.pypots\n",
      "2025-03-14 22:11:38 [INFO]: Epoch 003 - training loss (MSE): 0.0124, validation MSE: 0.0056\n",
      "2025-03-14 22:11:38 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS_epoch3_loss0.0056.pypots\n",
      "2025-03-14 22:11:47 [INFO]: Epoch 004 - training loss (MSE): 0.0100, validation MSE: 0.0045\n",
      "2025-03-14 22:11:47 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS_epoch4_loss0.0045.pypots\n",
      "2025-03-14 22:11:57 [INFO]: Epoch 005 - training loss (MSE): 0.0083, validation MSE: 0.0039\n",
      "2025-03-14 22:11:57 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS_epoch5_loss0.0039.pypots\n",
      "2025-03-14 22:12:06 [INFO]: Epoch 006 - training loss (MSE): 0.0071, validation MSE: 0.0035\n",
      "2025-03-14 22:12:06 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS_epoch6_loss0.0035.pypots\n",
      "2025-03-14 22:12:15 [INFO]: Epoch 007 - training loss (MSE): 0.0064, validation MSE: 0.0036\n",
      "2025-03-14 22:12:15 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS_epoch7_loss0.0036.pypots\n",
      "2025-03-14 22:12:25 [INFO]: Epoch 008 - training loss (MSE): 0.0060, validation MSE: 0.0033\n",
      "2025-03-14 22:12:25 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS_epoch8_loss0.0033.pypots\n",
      "2025-03-14 22:12:34 [INFO]: Epoch 009 - training loss (MSE): 0.0057, validation MSE: 0.0032\n",
      "2025-03-14 22:12:34 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS_epoch9_loss0.0032.pypots\n",
      "2025-03-14 22:12:43 [INFO]: Epoch 010 - training loss (MSE): 0.0054, validation MSE: 0.0031\n",
      "2025-03-14 22:12:43 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS_epoch10_loss0.0031.pypots\n",
      "2025-03-14 22:12:43 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-03-14 22:12:43 [INFO]: Saved the model to tutorial_results/imputation/saits/20250314_T220832/SAITS.pypots\n"
     ]
    }
   ],
   "source": [
    "saits_minmax.fit(train_set=dataset_for_training_minmax, val_set=dataset_for_validating_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Load - Standard Scaler </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-16 14:59:36 [INFO]: Model loaded successfully from tutorial_results/imputation/saits/20250316_T145645/SAITS.pypots\n"
     ]
    }
   ],
   "source": [
    "saits_standard.load(\"tutorial_results/imputation/saits/20250316_T145645/SAITS.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Load - Min/Max Scaler </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pypots/base.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model = torch.load(path, map_location=self.device)\n",
      "2025-02-26 19:23:02 [INFO]: Model loaded successfully from tutorial_results/imputation/saits/20250226_T191938/SAITS.pypots\n"
     ]
    }
   ],
   "source": [
    "saits_minmax.load(\"tutorial_results/imputation/saits/minmax/SAITS.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train - Standard Scaler </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 22:15:43 [INFO]: Epoch 001 - training loss (MSE): 0.9450, validation MSE: 6.7843\n",
      "2025-03-14 22:15:43 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221419/BRITS_epoch1_loss6.7843.pypots\n",
      "2025-03-14 22:16:20 [INFO]: Epoch 002 - training loss (MSE): 0.7358, validation MSE: 6.7442\n",
      "2025-03-14 22:16:20 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221419/BRITS_epoch2_loss6.7442.pypots\n",
      "2025-03-14 22:16:57 [INFO]: Epoch 003 - training loss (MSE): 0.6841, validation MSE: 6.7316\n",
      "2025-03-14 22:16:57 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221419/BRITS_epoch3_loss6.7316.pypots\n",
      "2025-03-14 22:17:35 [INFO]: Epoch 004 - training loss (MSE): 0.6600, validation MSE: 6.7264\n",
      "2025-03-14 22:17:35 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221419/BRITS_epoch4_loss6.7264.pypots\n",
      "2025-03-14 22:18:12 [INFO]: Epoch 005 - training loss (MSE): 0.6447, validation MSE: 6.7258\n",
      "2025-03-14 22:18:12 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221419/BRITS_epoch5_loss6.7258.pypots\n",
      "2025-03-14 22:18:49 [INFO]: Epoch 006 - training loss (MSE): 0.6332, validation MSE: 6.7248\n",
      "2025-03-14 22:18:49 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221419/BRITS_epoch6_loss6.7248.pypots\n",
      "2025-03-14 22:19:26 [INFO]: Epoch 007 - training loss (MSE): 0.6234, validation MSE: 6.7264\n",
      "2025-03-14 22:19:26 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221419/BRITS_epoch7_loss6.7264.pypots\n",
      "2025-03-14 22:20:03 [INFO]: Epoch 008 - training loss (MSE): 0.6151, validation MSE: 6.7302\n",
      "2025-03-14 22:20:03 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221419/BRITS_epoch8_loss6.7302.pypots\n",
      "2025-03-14 22:20:40 [INFO]: Epoch 009 - training loss (MSE): 0.6080, validation MSE: 6.7324\n",
      "2025-03-14 22:20:40 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221419/BRITS_epoch9_loss6.7324.pypots\n",
      "2025-03-14 22:20:40 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-03-14 22:20:40 [INFO]: Finished training. The best model is from epoch#6.\n",
      "2025-03-14 22:20:40 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221419/BRITS.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "brits_standard.fit(train_set=dataset_for_training_standard, val_set=dataset_for_validating_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 22:22:08 [INFO]: Epoch 001 - training loss (MSE): 0.1666, validation MSE: 0.0076\n",
      "2025-03-14 22:22:08 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS_epoch1_loss0.0076.pypots\n",
      "2025-03-14 22:22:45 [INFO]: Epoch 002 - training loss (MSE): 0.0979, validation MSE: 0.0053\n",
      "2025-03-14 22:22:45 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS_epoch2_loss0.0053.pypots\n",
      "2025-03-14 22:23:22 [INFO]: Epoch 003 - training loss (MSE): 0.0829, validation MSE: 0.0045\n",
      "2025-03-14 22:23:22 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS_epoch3_loss0.0045.pypots\n",
      "2025-03-14 22:23:59 [INFO]: Epoch 004 - training loss (MSE): 0.0759, validation MSE: 0.0039\n",
      "2025-03-14 22:23:59 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS_epoch4_loss0.0039.pypots\n",
      "2025-03-14 22:24:36 [INFO]: Epoch 005 - training loss (MSE): 0.0707, validation MSE: 0.0035\n",
      "2025-03-14 22:24:36 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS_epoch5_loss0.0035.pypots\n",
      "2025-03-14 22:25:13 [INFO]: Epoch 006 - training loss (MSE): 0.0675, validation MSE: 0.0032\n",
      "2025-03-14 22:25:13 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS_epoch6_loss0.0032.pypots\n",
      "2025-03-14 22:25:50 [INFO]: Epoch 007 - training loss (MSE): 0.0651, validation MSE: 0.0030\n",
      "2025-03-14 22:25:50 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS_epoch7_loss0.0030.pypots\n",
      "2025-03-14 22:26:27 [INFO]: Epoch 008 - training loss (MSE): 0.0632, validation MSE: 0.0028\n",
      "2025-03-14 22:26:27 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS_epoch8_loss0.0028.pypots\n",
      "2025-03-14 22:27:04 [INFO]: Epoch 009 - training loss (MSE): 0.0617, validation MSE: 0.0028\n",
      "2025-03-14 22:27:04 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS_epoch9_loss0.0028.pypots\n",
      "2025-03-14 22:27:41 [INFO]: Epoch 010 - training loss (MSE): 0.0606, validation MSE: 0.0027\n",
      "2025-03-14 22:27:41 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS_epoch10_loss0.0027.pypots\n",
      "2025-03-14 22:27:41 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-03-14 22:27:41 [INFO]: Saved the model to tutorial_results/imputation/brits/20250314_T221424/BRITS.pypots\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "brits_minmax.fit(train_set=dataset_for_training_minmax, val_set=dataset_for_validating_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:05:09 [INFO]: Model loaded successfully from tutorial_results/imputation/brits/standard_scaler/BRITS.pypots\n"
     ]
    }
   ],
   "source": [
    "brits_standard.load(\"tutorial_results/imputation/brits/standard/BRITS.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Min/Max Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'brits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbrits\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtutorial_results/imputation/brits/20250210_T213459/BRITS.pypots\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'brits' is not defined"
     ]
    }
   ],
   "source": [
    "brits_minmax.load(\"tutorial_results/imputation/brits/minmax/BRITS.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - Standard Scaler </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 22:32:37 [INFO]: Epoch 001 - generator training loss: 0.4208, discriminator training loss: 0.1856, validation loss: 6.7608\n",
      "2025-03-14 22:32:37 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN_epoch1_loss6.7608.pypots\n",
      "2025-03-14 22:34:03 [INFO]: Epoch 002 - generator training loss: 0.3496, discriminator training loss: 0.0542, validation loss: 6.7149\n",
      "2025-03-14 22:34:03 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN_epoch2_loss6.7149.pypots\n",
      "2025-03-14 22:35:29 [INFO]: Epoch 003 - generator training loss: 0.3237, discriminator training loss: 0.0362, validation loss: 6.7062\n",
      "2025-03-14 22:35:29 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN_epoch3_loss6.7062.pypots\n",
      "2025-03-14 22:36:56 [INFO]: Epoch 004 - generator training loss: 0.3136, discriminator training loss: 0.0306, validation loss: 6.6885\n",
      "2025-03-14 22:36:56 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN_epoch4_loss6.6885.pypots\n",
      "2025-03-14 22:38:23 [INFO]: Epoch 005 - generator training loss: 0.3009, discriminator training loss: 0.0283, validation loss: 6.6766\n",
      "2025-03-14 22:38:23 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN_epoch5_loss6.6766.pypots\n",
      "2025-03-14 22:39:49 [INFO]: Epoch 006 - generator training loss: 0.2917, discriminator training loss: 0.0267, validation loss: 6.6704\n",
      "2025-03-14 22:39:49 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN_epoch6_loss6.6704.pypots\n",
      "2025-03-14 22:41:15 [INFO]: Epoch 007 - generator training loss: 0.2851, discriminator training loss: 0.0254, validation loss: 6.6679\n",
      "2025-03-14 22:41:15 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN_epoch7_loss6.6679.pypots\n",
      "2025-03-14 22:42:41 [INFO]: Epoch 008 - generator training loss: 0.2772, discriminator training loss: 0.0245, validation loss: 6.6603\n",
      "2025-03-14 22:42:41 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN_epoch8_loss6.6603.pypots\n",
      "2025-03-14 22:44:07 [INFO]: Epoch 009 - generator training loss: 0.2730, discriminator training loss: 0.0235, validation loss: 6.6575\n",
      "2025-03-14 22:44:07 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN_epoch9_loss6.6575.pypots\n",
      "2025-03-14 22:45:33 [INFO]: Epoch 010 - generator training loss: 0.2679, discriminator training loss: 0.0226, validation loss: 6.6531\n",
      "2025-03-14 22:45:33 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN_epoch10_loss6.6531.pypots\n",
      "2025-03-14 22:45:33 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-03-14 22:45:33 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223035/USGAN.pypots\n"
     ]
    }
   ],
   "source": [
    "us_gan_standard.fit(train_set=dataset_for_training_standard, val_set=dataset_for_validating_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Train - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 22:49:15 [INFO]: Epoch 001 - generator training loss: -0.0580, discriminator training loss: 0.1818, validation loss: 0.0209\n",
      "2025-03-14 22:49:15 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223041/USGAN_epoch1_loss0.0209.pypots\n",
      "2025-03-14 22:50:42 [INFO]: Epoch 002 - generator training loss: -0.0013, discriminator training loss: 0.0539, validation loss: 0.0140\n",
      "2025-03-14 22:50:42 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223041/USGAN_epoch2_loss0.0140.pypots\n",
      "2025-03-14 22:52:08 [INFO]: Epoch 003 - generator training loss: 0.0013, discriminator training loss: 0.0377, validation loss: 0.0125\n",
      "2025-03-14 22:52:08 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223041/USGAN_epoch3_loss0.0125.pypots\n",
      "2025-03-14 22:53:36 [INFO]: Epoch 004 - generator training loss: -0.0007, discriminator training loss: 0.0319, validation loss: 0.0127\n",
      "2025-03-14 22:53:36 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223041/USGAN_epoch4_loss0.0127.pypots\n",
      "2025-03-14 22:55:03 [INFO]: Epoch 005 - generator training loss: 0.0011, discriminator training loss: 0.0292, validation loss: 0.0142\n",
      "2025-03-14 22:55:03 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223041/USGAN_epoch5_loss0.0142.pypots\n",
      "2025-03-14 22:56:31 [INFO]: Epoch 006 - generator training loss: 0.0005, discriminator training loss: 0.0270, validation loss: 0.0144\n",
      "2025-03-14 22:56:31 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223041/USGAN_epoch6_loss0.0144.pypots\n",
      "2025-03-14 22:56:31 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-03-14 22:56:31 [INFO]: Finished training. The best model is from epoch#3.\n",
      "2025-03-14 22:56:31 [INFO]: Saved the model to tutorial_results/imputation/us_gan/20250314_T223041/USGAN.pypots\n"
     ]
    }
   ],
   "source": [
    "us_gan_minmax.fit(train_set=dataset_for_training_minmax, val_set=dataset_for_validating_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:05:34 [INFO]: Model loaded successfully from tutorial_results/imputation/us_gan/standard_scaler/USGAN.pypots\n"
     ]
    }
   ],
   "source": [
    "us_gan_standard.load(\"tutorial_results/imputation/us_gan/standard/USGAN.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Min/Max Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_gan_minmax.load(\"tutorial_results/imputation/us_gan/minmax/USGAN.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:05:22 [INFO]: Epoch 001 - training loss (MSE): 26005.7651, validation MSE: 7.0331\n",
      "2025-03-14 23:05:22 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE_epoch1_loss7.0331.pypots\n",
      "2025-03-14 23:05:40 [INFO]: Epoch 002 - training loss (MSE): 22872.9500, validation MSE: 7.0307\n",
      "2025-03-14 23:05:40 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE_epoch2_loss7.0307.pypots\n",
      "2025-03-14 23:05:59 [INFO]: Epoch 003 - training loss (MSE): 22841.9535, validation MSE: 6.9962\n",
      "2025-03-14 23:05:59 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE_epoch3_loss6.9962.pypots\n",
      "2025-03-14 23:06:18 [INFO]: Epoch 004 - training loss (MSE): 22833.0582, validation MSE: 6.9798\n",
      "2025-03-14 23:06:18 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE_epoch4_loss6.9798.pypots\n",
      "2025-03-14 23:06:36 [INFO]: Epoch 005 - training loss (MSE): 22824.8062, validation MSE: 6.9814\n",
      "2025-03-14 23:06:36 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE_epoch5_loss6.9814.pypots\n",
      "2025-03-14 23:06:54 [INFO]: Epoch 006 - training loss (MSE): 22819.8682, validation MSE: 6.9444\n",
      "2025-03-14 23:06:54 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE_epoch6_loss6.9444.pypots\n",
      "2025-03-14 23:07:13 [INFO]: Epoch 007 - training loss (MSE): 22825.4004, validation MSE: 6.9441\n",
      "2025-03-14 23:07:13 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE_epoch7_loss6.9441.pypots\n",
      "2025-03-14 23:07:31 [INFO]: Epoch 008 - training loss (MSE): 22816.7705, validation MSE: 6.9394\n",
      "2025-03-14 23:07:31 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE_epoch8_loss6.9394.pypots\n",
      "2025-03-14 23:07:50 [INFO]: Epoch 009 - training loss (MSE): 22811.0886, validation MSE: 6.9218\n",
      "2025-03-14 23:07:50 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE_epoch9_loss6.9218.pypots\n",
      "2025-03-14 23:08:08 [INFO]: Epoch 010 - training loss (MSE): 22808.7664, validation MSE: 6.9250\n",
      "2025-03-14 23:08:08 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE_epoch10_loss6.9250.pypots\n",
      "2025-03-14 23:08:08 [INFO]: Finished training. The best model is from epoch#9.\n",
      "2025-03-14 23:08:08 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230432/GPVAE.pypots\n"
     ]
    }
   ],
   "source": [
    "gp_vae_standard.fit(train_set=dataset_for_training_standard, val_set=dataset_for_validating_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:12:00 [INFO]: Epoch 001 - training loss (MSE): 24751.6873, validation MSE: 0.0238\n",
      "2025-03-14 23:12:00 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE_epoch1_loss0.0238.pypots\n",
      "2025-03-14 23:12:22 [INFO]: Epoch 002 - training loss (MSE): 22721.8431, validation MSE: 0.0111\n",
      "2025-03-14 23:12:22 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE_epoch2_loss0.0111.pypots\n",
      "2025-03-14 23:12:46 [INFO]: Epoch 003 - training loss (MSE): 22712.9646, validation MSE: 0.0110\n",
      "2025-03-14 23:12:46 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE_epoch3_loss0.0110.pypots\n",
      "2025-03-14 23:13:10 [INFO]: Epoch 004 - training loss (MSE): 22710.9633, validation MSE: 0.0108\n",
      "2025-03-14 23:13:10 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE_epoch4_loss0.0108.pypots\n",
      "2025-03-14 23:13:34 [INFO]: Epoch 005 - training loss (MSE): 22710.2003, validation MSE: 0.0109\n",
      "2025-03-14 23:13:34 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE_epoch5_loss0.0109.pypots\n",
      "2025-03-14 23:13:58 [INFO]: Epoch 006 - training loss (MSE): 22709.8226, validation MSE: 0.0108\n",
      "2025-03-14 23:13:58 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE_epoch6_loss0.0108.pypots\n",
      "2025-03-14 23:14:23 [INFO]: Epoch 007 - training loss (MSE): 22709.6142, validation MSE: 0.0109\n",
      "2025-03-14 23:14:23 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE_epoch7_loss0.0109.pypots\n",
      "2025-03-14 23:14:47 [INFO]: Epoch 008 - training loss (MSE): 22709.4695, validation MSE: 0.0108\n",
      "2025-03-14 23:14:47 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE_epoch8_loss0.0108.pypots\n",
      "2025-03-14 23:15:12 [INFO]: Epoch 009 - training loss (MSE): 22709.3975, validation MSE: 0.0108\n",
      "2025-03-14 23:15:12 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE_epoch9_loss0.0108.pypots\n",
      "2025-03-14 23:15:37 [INFO]: Epoch 010 - training loss (MSE): 22709.3942, validation MSE: 0.0108\n",
      "2025-03-14 23:15:37 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE_epoch10_loss0.0108.pypots\n",
      "2025-03-14 23:15:37 [INFO]: Finished training. The best model is from epoch#10.\n",
      "2025-03-14 23:15:37 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20250314_T230444/GPVAE.pypots\n"
     ]
    }
   ],
   "source": [
    "gp_vae_minmax.fit(train_set=dataset_for_training_minmax, val_set=dataset_for_validating_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:05:46 [INFO]: Model loaded successfully from tutorial_results/imputation/gp_vae/standard_scaler/GPVAE.pypots\n"
     ]
    }
   ],
   "source": [
    "gp_vae_standard.load(\"tutorial_results/imputation/gp_vae/standard/GPVAE.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_vae_minmax.load(\"mae/tutorial_results/imputation/gp_vae/minmax/GPVAE.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:25:31 [INFO]: Epoch 001 - training loss (MSE): 0.7082, validation MSE: 7.3078\n",
      "2025-03-14 23:25:31 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231857/MRNN_epoch1_loss7.3078.pypots\n",
      "2025-03-14 23:31:32 [INFO]: Epoch 002 - training loss (MSE): 0.5308, validation MSE: 7.2712\n",
      "2025-03-14 23:31:32 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231857/MRNN_epoch2_loss7.2712.pypots\n",
      "2025-03-14 23:37:29 [INFO]: Epoch 003 - training loss (MSE): 0.4895, validation MSE: 7.2554\n",
      "2025-03-14 23:37:29 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231857/MRNN_epoch3_loss7.2554.pypots\n",
      "2025-03-14 23:43:30 [INFO]: Epoch 004 - training loss (MSE): 0.4704, validation MSE: 7.2477\n",
      "2025-03-14 23:43:30 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231857/MRNN_epoch4_loss7.2477.pypots\n",
      "2025-03-14 23:49:25 [INFO]: Epoch 005 - training loss (MSE): 0.4565, validation MSE: 7.2435\n",
      "2025-03-14 23:49:25 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231857/MRNN_epoch5_loss7.2435.pypots\n",
      "2025-03-14 23:55:21 [INFO]: Epoch 006 - training loss (MSE): 0.4497, validation MSE: 7.2439\n",
      "2025-03-14 23:55:21 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231857/MRNN_epoch6_loss7.2439.pypots\n",
      "2025-03-15 00:01:20 [INFO]: Epoch 007 - training loss (MSE): 0.4417, validation MSE: 7.2443\n",
      "2025-03-15 00:01:20 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231857/MRNN_epoch7_loss7.2443.pypots\n",
      "2025-03-15 00:07:19 [INFO]: Epoch 008 - training loss (MSE): 0.4315, validation MSE: 7.2471\n",
      "2025-03-15 00:07:19 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231857/MRNN_epoch8_loss7.2471.pypots\n",
      "2025-03-15 00:07:19 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-03-15 00:07:19 [INFO]: Finished training. The best model is from epoch#5.\n",
      "2025-03-15 00:07:19 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231857/MRNN.pypots\n"
     ]
    }
   ],
   "source": [
    "mrnn_standard.fit(train_set=dataset_for_training_standard, val_set=dataset_for_validating_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Train - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-15 00:15:40 [INFO]: Epoch 001 - training loss (MSE): 0.2016, validation MSE: 0.0279\n",
      "2025-03-15 00:15:40 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231901/MRNN_epoch1_loss0.0279.pypots\n",
      "2025-03-15 00:21:41 [INFO]: Epoch 002 - training loss (MSE): 0.0690, validation MSE: 0.0340\n",
      "2025-03-15 00:21:41 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231901/MRNN_epoch2_loss0.0340.pypots\n",
      "2025-03-15 00:27:44 [INFO]: Epoch 003 - training loss (MSE): 0.0536, validation MSE: 0.0429\n",
      "2025-03-15 00:27:44 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231901/MRNN_epoch3_loss0.0429.pypots\n",
      "2025-03-15 00:33:46 [INFO]: Epoch 004 - training loss (MSE): 0.0449, validation MSE: 0.0490\n",
      "2025-03-15 00:33:46 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231901/MRNN_epoch4_loss0.0490.pypots\n",
      "2025-03-15 00:33:46 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2025-03-15 00:33:46 [INFO]: Finished training. The best model is from epoch#1.\n",
      "2025-03-15 00:33:46 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20250314_T231901/MRNN.pypots\n"
     ]
    }
   ],
   "source": [
    "mrnn_minmax.fit(train_set=dataset_for_training_minmax, val_set=dataset_for_validating_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - Standard Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 21:05:52 [INFO]: Model loaded successfully from tutorial_results/imputation/mrnn/standard_scaler/MRNN.pypots\n"
     ]
    }
   ],
   "source": [
    "mrnn_standard.load(\"tutorial_results/imputation/mrnn/standard/MRNN.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Load - MinMax Scaler</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_minmax.load(\"tutorial_results/imputation/mrnn/minmax/MRNN.pypots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits_imputation_standard = toolkits.model_imputation(dataset_for_testing_standard, saits_standard)\n",
    "saits_imputation_standard = toolkits.pre_reshape(saits_imputation_standard)\n",
    "saits_imputation_standard_ori = toolkits.desnormalization(saits_imputation_standard, scaler_standard)\n",
    "saits_imputation_standard = toolkits.reshape_variable(saits_imputation_standard)\n",
    "saits_imputation_standard_ori = toolkits.reshape_variable(saits_imputation_standard_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "saits_imputation_minmax = toolkits.model_imputation(dataset_for_testing_minmax, saits_minmax)\n",
    "saits_imputation_minmax = toolkits.pre_reshape(saits_imputation_minmax)\n",
    "saits_imputation_minmax_ori = toolkits.desnormalization(saits_imputation_minmax, scaler_minmax)\n",
    "saits_imputation_minmax = toolkits.reshape_variable(saits_imputation_minmax)\n",
    "saits_imputation_minmax_ori = toolkits.reshape_variable(saits_imputation_minmax_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits_imputation_standard = toolkits.model_imputation(dataset_for_testing_standard, brits_standard)\n",
    "brits_imputation_standard = toolkits.pre_reshape(brits_imputation_standard)\n",
    "brits_imputation_standard_ori = toolkits.desnormalization(brits_imputation_standard, scaler_standard)\n",
    "brits_imputation_standard = toolkits.reshape_variable(brits_imputation_standard)\n",
    "brits_imputation_standard_ori = toolkits.reshape_variable(brits_imputation_standard_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "brits_imputation_minmax = toolkits.model_imputation(dataset_for_testing_minmax, brits_minmax)\n",
    "brits_imputation_minmax = toolkits.pre_reshape(brits_imputation_minmax)\n",
    "brits_imputation_minmax_ori = toolkits.desnormalization(brits_imputation_minmax, scaler_minmax) \n",
    "brits_imputation_minmax = toolkits.reshape_variable(brits_imputation_minmax)\n",
    "brits_imputation_minmax_ori = toolkits.reshape_variable(brits_imputation_minmax_ori)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgan_imputation_standard = toolkits.model_imputation(dataset_for_testing_standard, us_gan_standard)\n",
    "usgan_imputation_standard = toolkits.pre_reshape(usgan_imputation_standard)\n",
    "usgan_imputation_standard_ori = toolkits.desnormalization(usgan_imputation_standard, scaler_standard)\n",
    "usgan_imputation_standard = toolkits.reshape_variable(usgan_imputation_standard)\n",
    "usgan_imputation_standard_ori = toolkits.reshape_variable(usgan_imputation_standard_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgan_imputation_minmax = toolkits.model_imputation(dataset_for_testing_minmax, us_gan_minmax)\n",
    "usgan_imputation_minmax = toolkits.pre_reshape(usgan_imputation_minmax)\n",
    "usgan_imputation_minmax_ori = toolkits.desnormalization(usgan_imputation_minmax, scaler_minmax)\n",
    "usgan_imputation_minmax = toolkits.reshape_variable(usgan_imputation_minmax)\n",
    "usgan_imputation_minmax_ori = toolkits.reshape_variable(usgan_imputation_minmax_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpvae_imputation_standard = toolkits.model_imputation(dataset_for_testing_standard, gp_vae_standard)\n",
    "gpvae_imputation_standard = toolkits.pre_reshape(gpvae_imputation_standard)\n",
    "gpvae_imputation_standard_ori = toolkits.desnormalization(gpvae_imputation_standard, scaler_standard)\n",
    "gpvae_imputation_standard = toolkits.reshape_variable(gpvae_imputation_standard)\n",
    "gpvae_imputation_standard_ori = toolkits.reshape_variable(gpvae_imputation_standard_ori) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpvae_imputation_minmax = toolkits.model_imputation(dataset_for_testing_minmax, gp_vae_minmax)\n",
    "gpvae_imputation_minmax = toolkits.pre_reshape(gpvae_imputation_minmax)\n",
    "gpvae_imputation_minmax_ori = toolkits.desnormalization(gpvae_imputation_minmax, scaler_minmax)\n",
    "gpvae_imputation_minmax = toolkits.reshape_variable(gpvae_imputation_minmax)\n",
    "gpvae_imputation_minmax_ori = toolkits.reshape_variable(gpvae_imputation_minmax_ori) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_imputation_standard = toolkits.model_imputation(dataset_for_testing_standard, mrnn_standard)\n",
    "mrnn_imputation_standard = toolkits.pre_reshape(mrnn_imputation_standard)\n",
    "mrnn_imputation_standard_ori = toolkits.desnormalization(mrnn_imputation_standard, scaler_standard)\n",
    "mrnn_imputation_standard = toolkits.reshape_variable(mrnn_imputation_standard)\n",
    "mrnn_imputation_standard_ori = toolkits.reshape_variable(mrnn_imputation_standard_ori) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrnn_imputation_minmax = toolkits.model_imputation(dataset_for_testing_minmax, mrnn_minmax)\n",
    "mrnn_imputation_minmax = toolkits.pre_reshape(mrnn_imputation_minmax)\n",
    "mrnn_imputation_minmax_ori = toolkits.desnormalization(mrnn_imputation_minmax, scaler_minmax)\n",
    "mrnn_imputation_minmax = toolkits.reshape_variable(mrnn_imputation_minmax)\n",
    "mrnn_imputation_minmax_ori = toolkits.reshape_variable(mrnn_imputation_minmax_ori) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_saits_variables_standard = toolkits.calculate_mae(saits_imputation_standard, test_X_ori_variable_standard_norm, indicating_mask_variable_standard_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_saits_variables_standard_ori = toolkits.calculate_mae(saits_imputation_standard_ori, test_X_ori_variable_standard_des, indicating_mask_variable_standard_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_saits_variables_minmax = toolkits.calculate_mae(saits_imputation_minmax, test_X_ori_variable_minmax_norm, indicating_mask_variable_minmax_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_saits_variables_minmax_ori = toolkits.calculate_mae(saits_imputation_minmax_ori, test_X_ori_variable_minmax_des, indicating_mask_variable_minmax_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_brits_variables_standard = toolkits.calculate_mae(brits_imputation_standard, test_X_ori_variable_standard, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_brits_variables_standard_ori = toolkits.calculate_mae(brits_imputation_standard_ori, test_X_ori_variable_standard_ori, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_brits_variables_minmax = toolkits.calculate_mae(brits_imputation_minmax, test_X_ori_variable_minmax, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_brits_variables_minmax_ori = toolkits.calculate_mae(brits_imputation_minmax_ori, test_X_ori_variable_minmax_ori, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_usgan_variables_standard = toolkits.calculate_mae(usgan_imputation_standard, test_X_ori_variable_standard, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_usgan_variables_standard_ori = toolkits.calculate_mae(usgan_imputation_standard_ori, test_X_ori_variable_standard_ori, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_usgan_variables_minmax = toolkits.calculate_mae(usgan_imputation_minmax, test_X_ori_variable_minmax, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_usgan_variables_minmax_ori = toolkits.calculate_mae(usgan_imputation_minmax_ori, test_X_ori_variable_minmax_ori, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_gpvae_variables_standard = toolkits.calculate_mae(gpvae_imputation_standard, test_X_ori_variable_standard, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_gpvae_variables_standard_ori = toolkits.calculate_mae(gpvae_imputation_standard_ori, test_X_ori_variable_standard_ori, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_gpvae_variables_minmax = toolkits.calculate_mae(gpvae_imputation_minmax, test_X_ori_variable_minmax, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_gpvae_variables_minmax_ori = toolkits.calculate_mae(gpvae_imputation_minmax_ori, test_X_ori_variable_minmax_ori, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_mrnn_variables_standard = toolkits.calculate_mae(mrnn_imputation_standard, test_X_ori_variable_standard, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_mrnn_variables_standard_ori = toolkits.calculate_mae(mrnn_imputation_standard_ori, test_X_ori_variable_standard_ori, indicating_mask_variable_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (C/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_mrnn_variables_minmax = toolkits.calculate_mae(mrnn_imputation_minmax, test_X_ori_variable_minmax, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler (S/Normalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mae_mrnn_variables_minmax_ori = toolkits.calculate_mae(mrnn_imputation_minmax_ori, test_X_ori_variable_minmax_ori, indicating_mask_variable_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups = [\"General\", \"Female\", \"Male\", \"Undefined Gender\", \"+65\", \"-65\", \"ICUType 1\", \"ICUType 2\", \"ICUType 3\", \"ICUType 4\", \"Undefined classification\", \"Low Weight\", \"Normal Weight\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"ALP\", \"ALT\", \"AST\", \"Albumin\", \"BUN\", \"Bilirubin\", \"Cholesterol\", \"Creatinine\", \"DiasABP\", \"FiO2\", \"GCS\", \"Glucose\", \"HCO3\", \"HCT\", \"HR\", \"K\", \"Lactate\", \"MAP\", \"MechVent\", \"Mg\", \"NIDiasABP\", \"NIMAP\", \"NISysABP\", \"Na\", \"PaCO2\", \"PaO2\", \"Platelets\", \"RespRate\", \"SaO2\", \"SysABP\", \"Temp\", \"TroponinI\", \"TroponinT\", \"Urine\", \"WBC\", \"Weight\", \"Ph\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Standard Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAITS - MAE\n",
      "************\n",
      "General\n",
      "-------------\n",
      "ALP : 0.35842167090154575\n",
      "ALT : 0.22873105156973475\n",
      "AST : 0.2360348751195685\n",
      "Albumin : 0.5669722943273814\n",
      "BUN : 0.21043372275375152\n",
      "Bilirubin : 0.2799269029920604\n",
      "Cholesterol : 0.8929161650373507\n",
      "Creatinine : 0.1904952408486541\n",
      "DiasABP : 0.20309505135870332\n",
      "FiO2 : 0.46299985373247476\n",
      "GCS : 0.25740588128546604\n",
      "Glucose : 0.58622658054476\n",
      "HCO3 : 0.4138240084389746\n",
      "HCT : 0.5282681467592029\n",
      "HR : 0.26094783579023706\n",
      "K : 0.6036355663482271\n",
      "Lactate : 0.3863171737590092\n",
      "MAP : 0.17923546222764594\n",
      "MechVent : 0.010012425656927714\n",
      "Mg : 0.5337098468198467\n",
      "NIDiasABP : 0.2158271249873218\n",
      "NIMAP : 0.142324202564339\n",
      "NISysABP : 0.24474839151507188\n",
      "Na : 0.41931335367054434\n",
      "PaCO2 : 0.5161239232628241\n",
      "PaO2 : 0.5040665952956557\n",
      "Platelets : 0.36926333356189006\n",
      "RespRate : 0.461044411387632\n",
      "SaO2 : 0.353425374402371\n",
      "SysABP : 0.24250818731396934\n",
      "Temp : 0.32115819242427524\n",
      "TroponinI : 0.6336863382590735\n",
      "TroponinT : 0.35103726598576673\n",
      "Urine : 0.3666884967716744\n",
      "WBC : 0.12523799449898662\n",
      "Weight : 0.14129578117596783\n",
      "Ph : 0.12801651262829142\n",
      "Female\n",
      "-------------\n",
      "ALP : 0.3696252511951618\n",
      "ALT : 0.18187023788127346\n",
      "AST : 0.2025166371537436\n",
      "Albumin : 0.5964840248561778\n",
      "BUN : 0.21972235349903785\n",
      "Bilirubin : 0.3058830791919902\n",
      "Cholesterol : 1.1637824087759958\n",
      "Creatinine : 0.16928510033174468\n",
      "DiasABP : 0.20235116951885324\n",
      "FiO2 : 0.43698951376275014\n",
      "GCS : 0.24538378958742615\n",
      "Glucose : 0.6037374867026734\n",
      "HCO3 : 0.3965694016221081\n",
      "HCT : 0.5374278322320917\n",
      "HR : 0.2700957158493666\n",
      "K : 0.6348063232728578\n",
      "Lactate : 0.3693711215366292\n",
      "MAP : 0.2006347530461642\n",
      "MechVent : 0.009203133336811305\n",
      "Mg : 0.5427839117474526\n",
      "NIDiasABP : 0.22399791749193879\n",
      "NIMAP : 0.14912135146723582\n",
      "NISysABP : 0.24802468417518056\n",
      "Na : 0.45064364001804164\n",
      "PaCO2 : 0.5074045216383538\n",
      "PaO2 : 0.49275971955105596\n",
      "Platelets : 0.39167387505453904\n",
      "RespRate : 0.46298780996913813\n",
      "SaO2 : 0.5311661933400138\n",
      "SysABP : 0.2342849983079083\n",
      "Temp : 0.3648352382065341\n",
      "TroponinI : 0.4202350173950446\n",
      "TroponinT : 0.33413575127394746\n",
      "Urine : 0.33273066379115346\n",
      "WBC : 0.126315873416097\n",
      "Weight : 0.13630347556045594\n",
      "Ph : 0.06806211388434125\n",
      "Male\n",
      "-------------\n",
      "ALP : 0.43776274802937126\n",
      "ALT : 0.1663119524345468\n",
      "AST : 0.1709154411758198\n",
      "Albumin : 0.6254686534064436\n",
      "BUN : 0.2201558056939073\n",
      "Bilirubin : 0.2585628836027691\n",
      "Cholesterol : 0.7779478712621526\n",
      "Creatinine : 0.1694940929383445\n",
      "DiasABP : 0.19926177086247793\n",
      "FiO2 : 0.48935105622082165\n",
      "GCS : 0.26221328442069103\n",
      "Glucose : 0.5910747684929615\n",
      "HCO3 : 0.4197505099889761\n",
      "HCT : 0.5305496313641958\n",
      "HR : 0.26261713862488223\n",
      "K : 0.549820240797391\n",
      "Lactate : 0.4312488292451638\n",
      "MAP : 0.1940234507366166\n",
      "MechVent : 0.010256575414940508\n",
      "Mg : 0.5413384645242936\n",
      "NIDiasABP : 0.22674655269185048\n",
      "NIMAP : 0.14035650667163393\n",
      "NISysABP : 0.2506059918531008\n",
      "Na : 0.43175167714536267\n",
      "PaCO2 : 0.4868908443305093\n",
      "PaO2 : 0.4733373196606049\n",
      "Platelets : 0.34296485932861853\n",
      "RespRate : 0.474536234161213\n",
      "SaO2 : 0.40427216369723595\n",
      "SysABP : 0.24477582452411958\n",
      "Temp : 0.34895007143541457\n",
      "TroponinI : 0.5919252029144014\n",
      "TroponinT : 0.42288817263105855\n",
      "Urine : 0.38614120735729784\n",
      "WBC : 0.13573924599347653\n",
      "Weight : 0.1377665472923627\n",
      "Ph : 0.0658331777236716\n",
      "Undefined Gender\n",
      "-------------\n",
      "ALP : 0.0\n",
      "ALT : 0.0\n",
      "AST : 0.0\n",
      "Albumin : 0.0\n",
      "BUN : 0.0\n",
      "Bilirubin : 0.0\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.0\n",
      "DiasABP : 0.16871399106619206\n",
      "FiO2 : 0.0\n",
      "GCS : 0.168698758561118\n",
      "Glucose : 0.3810877177538562\n",
      "HCO3 : 0.20996288696429188\n",
      "HCT : 0.0\n",
      "HR : 0.23672383187427173\n",
      "K : 0.7651846683002798\n",
      "Lactate : 0.0\n",
      "MAP : 0.24063321643368718\n",
      "MechVent : 0.0\n",
      "Mg : 0.22214295485315827\n",
      "NIDiasABP : 0.27109895982709425\n",
      "NIMAP : 0.13648910389107533\n",
      "NISysABP : 0.05879538186266211\n",
      "Na : 0.0\n",
      "PaCO2 : 0.0\n",
      "PaO2 : 0.0\n",
      "Platelets : 0.0\n",
      "RespRate : 0.13176806271705094\n",
      "SaO2 : 0.0\n",
      "SysABP : 0.19477538793061502\n",
      "Temp : 0.4330268313700056\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.0\n",
      "Urine : 0.31781953039862965\n",
      "WBC : 0.07660158857870995\n",
      "Weight : 0.08288284724850521\n",
      "Ph : 0.05413980745075028\n",
      "+65\n",
      "-------------\n",
      "ALP : 0.3938935790384729\n",
      "ALT : 0.1890065819712924\n",
      "AST : 0.155606065620387\n",
      "Albumin : 0.5344365668424997\n",
      "BUN : 0.21808880422213653\n",
      "Bilirubin : 0.2894217115355984\n",
      "Cholesterol : 0.7834316718974167\n",
      "Creatinine : 0.15698471370600772\n",
      "DiasABP : 0.2049608524778838\n",
      "FiO2 : 0.45403044347801547\n",
      "GCS : 0.2592332285545616\n",
      "Glucose : 0.5661609390021349\n",
      "HCO3 : 0.45203857888350907\n",
      "HCT : 0.49339035404962733\n",
      "HR : 0.2614555503170421\n",
      "K : 0.5342058076340177\n",
      "Lactate : 0.3392039656006369\n",
      "MAP : 0.1970566429584255\n",
      "MechVent : 0.009134375467371534\n",
      "Mg : 0.5311263501537942\n",
      "NIDiasABP : 0.21863609573717116\n",
      "NIMAP : 0.14056495288820006\n",
      "NISysABP : 0.2544344131603688\n",
      "Na : 0.4256070666480688\n",
      "PaCO2 : 0.47598179176283695\n",
      "PaO2 : 0.4529503819445867\n",
      "Platelets : 0.36519662281828147\n",
      "RespRate : 0.4534354251140228\n",
      "SaO2 : 0.42660889537613783\n",
      "SysABP : 0.24151436156780717\n",
      "Temp : 0.34295638102733184\n",
      "TroponinI : 0.9786871112851302\n",
      "TroponinT : 0.385321723292161\n",
      "Urine : 0.3277466881466815\n",
      "WBC : 0.14107234123217394\n",
      "Weight : 0.13800513730200945\n",
      "Ph : 0.06811428458998267\n",
      "-65\n",
      "-------------\n",
      "ALP : 0.7603008546154557\n",
      "ALT : 0.2071163501699724\n",
      "AST : 0.22405744436584674\n",
      "Albumin : 0.6299463363428933\n",
      "BUN : 0.20982592435174172\n",
      "Bilirubin : 0.321925273194522\n",
      "Cholesterol : 1.172714590557716\n",
      "Creatinine : 0.1683772708669397\n",
      "DiasABP : 0.19318495026998928\n",
      "FiO2 : 0.47126248207190474\n",
      "GCS : 0.2582503508986298\n",
      "Glucose : 0.6169470706514152\n",
      "HCO3 : 0.4419477858647135\n",
      "HCT : 0.5183789816064887\n",
      "HR : 0.2612945869480069\n",
      "K : 0.5888505811258\n",
      "Lactate : 0.40928652462258286\n",
      "MAP : 0.18817110891097535\n",
      "MechVent : 0.010386474315822219\n",
      "Mg : 0.5244018821200265\n",
      "NIDiasABP : 0.23215565953948383\n",
      "NIMAP : 0.15036758279188012\n",
      "NISysABP : 0.2256910010248831\n",
      "Na : 0.46479034438252276\n",
      "PaCO2 : 0.5005968304926699\n",
      "PaO2 : 0.49584145365243676\n",
      "Platelets : 0.3662874683107376\n",
      "RespRate : 0.4598816518360986\n",
      "SaO2 : 0.5326730265016449\n",
      "SysABP : 0.22538267056907046\n",
      "Temp : 0.37258995710802895\n",
      "TroponinI : 0.5228267990987603\n",
      "TroponinT : 0.4368305080748239\n",
      "Urine : 0.42768342929645664\n",
      "WBC : 0.12327411544103989\n",
      "Weight : 0.15057989038673317\n",
      "Ph : 0.06667891216052625\n",
      "ICUType 1\n",
      "-------------\n",
      "ALP : 0.227423956136859\n",
      "ALT : 0.21774119436287298\n",
      "AST : 0.3265781519401656\n",
      "Albumin : 0.466343522302439\n",
      "BUN : 0.19412107881285182\n",
      "Bilirubin : 0.2242319659087393\n",
      "Cholesterol : 0.9469834483749289\n",
      "Creatinine : 0.18622212724792025\n",
      "DiasABP : 0.2454191678354999\n",
      "FiO2 : 0.4380990525859521\n",
      "GCS : 0.23821335119483955\n",
      "Glucose : 0.7047567984262361\n",
      "HCO3 : 0.3628812038823827\n",
      "HCT : 0.5187447610397161\n",
      "HR : 0.24223214551164832\n",
      "K : 0.5755380497152655\n",
      "Lactate : 0.542667006415503\n",
      "MAP : 0.2455138976242284\n",
      "MechVent : 0.008780327328252912\n",
      "Mg : 0.48128880581812733\n",
      "NIDiasABP : 0.20098728055788317\n",
      "NIMAP : 0.13900404959776416\n",
      "NISysABP : 0.225675007246798\n",
      "Na : 0.41406051382429987\n",
      "PaCO2 : 0.4753512886723769\n",
      "PaO2 : 0.5445452320872491\n",
      "Platelets : 0.3225660394593911\n",
      "RespRate : 0.464321878145851\n",
      "SaO2 : 0.5114423901450659\n",
      "SysABP : 0.2531821404939284\n",
      "Temp : 0.3843831511710361\n",
      "TroponinI : 1.4362288200340874\n",
      "TroponinT : 0.4977678434318944\n",
      "Urine : 0.4457077679337897\n",
      "WBC : 0.09535644950180323\n",
      "Weight : 0.15220261787751296\n",
      "Ph : 0.08460008987087597\n",
      "ICUType 2\n",
      "-------------\n",
      "ALP : 0.28782554735356913\n",
      "ALT : 0.15931797174047105\n",
      "AST : 0.15003627107434916\n",
      "Albumin : 0.7478309702667395\n",
      "BUN : 0.17679403142673644\n",
      "Bilirubin : 0.2442138747705205\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.16292000401680254\n",
      "DiasABP : 0.16989820836325822\n",
      "FiO2 : 0.4447498110382541\n",
      "GCS : 0.36392865413666103\n",
      "Glucose : 0.5228745420170151\n",
      "HCO3 : 0.36305705566569374\n",
      "HCT : 0.48801621959530805\n",
      "HR : 0.2336678173654891\n",
      "K : 0.5976197053966714\n",
      "Lactate : 0.39225851265240985\n",
      "MAP : 0.14627195640909232\n",
      "MechVent : 0.009654392121356535\n",
      "Mg : 0.6213508066365104\n",
      "NIDiasABP : 0.21460705552786719\n",
      "NIMAP : 0.11813145408139733\n",
      "NISysABP : 0.2251844545007375\n",
      "Na : 0.4293535303041505\n",
      "PaCO2 : 0.4488854015293362\n",
      "PaO2 : 0.4741136025897876\n",
      "Platelets : 0.33843902777889595\n",
      "RespRate : 0.376051625932006\n",
      "SaO2 : 0.36243495324123653\n",
      "SysABP : 0.22158766698073323\n",
      "Temp : 0.22126979541987268\n",
      "TroponinI : 0.5649153537224161\n",
      "TroponinT : 0.416885962123077\n",
      "Urine : 0.35010390082667403\n",
      "WBC : 0.12950465404291453\n",
      "Weight : 0.12495307799120958\n",
      "Ph : 0.05409925929509088\n",
      "ICUType 3\n",
      "-------------\n",
      "ALP : 0.7718363152035325\n",
      "ALT : 0.1906545611510019\n",
      "AST : 0.27111748298793076\n",
      "Albumin : 0.6933622273494303\n",
      "BUN : 0.26353159393734354\n",
      "Bilirubin : 0.347262073799954\n",
      "Cholesterol : 0.8853058109331705\n",
      "Creatinine : 0.2296513006669503\n",
      "DiasABP : 0.20131533639810703\n",
      "FiO2 : 0.475020185379145\n",
      "GCS : 0.2501921671521601\n",
      "Glucose : 0.7498213686520653\n",
      "HCO3 : 0.4669988634945551\n",
      "HCT : 0.56174390142056\n",
      "HR : 0.2727893871224845\n",
      "K : 0.672058107562652\n",
      "Lactate : 0.4874627070397644\n",
      "MAP : 0.1856373600870395\n",
      "MechVent : 0.01065370998897021\n",
      "Mg : 0.5992882834902556\n",
      "NIDiasABP : 0.2191916596891537\n",
      "NIMAP : 0.14529836941707705\n",
      "NISysABP : 0.24426529510108838\n",
      "Na : 0.4491524048282529\n",
      "PaCO2 : 0.5439616146475706\n",
      "PaO2 : 0.48258762507732933\n",
      "Platelets : 0.4351366776264058\n",
      "RespRate : 0.49368576847863094\n",
      "SaO2 : 1.5508442497736532\n",
      "SysABP : 0.23169485006212306\n",
      "Temp : 0.48868314480937775\n",
      "TroponinI : 0.346459784289641\n",
      "TroponinT : 0.26534983105904764\n",
      "Urine : 0.37210179914488434\n",
      "WBC : 0.13738738931058067\n",
      "Weight : 0.1438479700378875\n",
      "Ph : 0.07357600186406815\n",
      "ICUType 4\n",
      "-------------\n",
      "ALP : 0.373259952080211\n",
      "ALT : 0.19317699547024914\n",
      "AST : 0.268487341214255\n",
      "Albumin : 0.7520840694482427\n",
      "BUN : 0.17308041634350158\n",
      "Bilirubin : 0.25232696545320255\n",
      "Cholesterol : 0.9062283323101271\n",
      "Creatinine : 0.15307107206800555\n",
      "DiasABP : 0.19497903357376706\n",
      "FiO2 : 0.4529214322248565\n",
      "GCS : 0.23790800870043807\n",
      "Glucose : 0.5066334361755652\n",
      "HCO3 : 0.3768241022060421\n",
      "HCT : 0.5037645993285983\n",
      "HR : 0.276340684943879\n",
      "K : 0.4997261757813077\n",
      "Lactate : 0.4045310340466006\n",
      "MAP : 0.19561507441819873\n",
      "MechVent : 0.009552232525867463\n",
      "Mg : 0.5292716550542088\n",
      "NIDiasABP : 0.23391731408758498\n",
      "NIMAP : 0.1468929295206585\n",
      "NISysABP : 0.2538645828324913\n",
      "Na : 0.4690974775627716\n",
      "PaCO2 : 0.49111811233169217\n",
      "PaO2 : 0.4972260365463602\n",
      "Platelets : 0.3493460942416767\n",
      "RespRate : 0.4776655819338583\n",
      "SaO2 : 0.42670381173945376\n",
      "SysABP : 0.2543835048003213\n",
      "Temp : 0.41558030057621675\n",
      "TroponinI : 1.0173372665712812\n",
      "TroponinT : 0.2496406557070237\n",
      "Urine : 0.38011718876741996\n",
      "WBC : 0.10135228083461492\n",
      "Weight : 0.13902407197381303\n",
      "Ph : 0.06598973977122545\n",
      "Undefined classification\n",
      "-------------\n",
      "ALP : 0.2982818798803751\n",
      "ALT : 0.19573587688412245\n",
      "AST : 0.2213914986573132\n",
      "Albumin : 0.7441098718367213\n",
      "BUN : 0.23730576161478883\n",
      "Bilirubin : 0.3447846375514707\n",
      "Cholesterol : 0.5269065789160877\n",
      "Creatinine : 0.17749048590599406\n",
      "DiasABP : 0.2025425480703193\n",
      "FiO2 : 0.42850502592982104\n",
      "GCS : 0.21977268293899851\n",
      "Glucose : 0.6365012797238909\n",
      "HCO3 : 0.4623877226016189\n",
      "HCT : 0.5372652876306108\n",
      "HR : 0.26867991036825306\n",
      "K : 0.5907711549066136\n",
      "Lactate : 0.38321693476542174\n",
      "MAP : 0.2052747097907682\n",
      "MechVent : 0.009811745900829276\n",
      "Mg : 0.4725456933678214\n",
      "NIDiasABP : 0.22704791281431025\n",
      "NIMAP : 0.1508907169299716\n",
      "NISysABP : 0.2517119833480795\n",
      "Na : 0.48745787337997\n",
      "PaCO2 : 0.5787623946229065\n",
      "PaO2 : 0.528559551354643\n",
      "Platelets : 0.4135848023862684\n",
      "RespRate : 0.48122951876941633\n",
      "SaO2 : 0.6187237796582191\n",
      "SysABP : 0.2499991187934056\n",
      "Temp : 0.3996521647706491\n",
      "TroponinI : 0.6632425458298634\n",
      "TroponinT : 0.3421452654627624\n",
      "Urine : 0.38182261953345875\n",
      "WBC : 0.14709311687992505\n",
      "Weight : 0.150157300686787\n",
      "Ph : 0.0800944602847264\n",
      "Low Weight\n",
      "-------------\n",
      "ALP : 0.2162388015503292\n",
      "ALT : 0.13753436166747796\n",
      "AST : 0.5028621214675539\n",
      "Albumin : 0.992486168830187\n",
      "BUN : 0.22664708578286105\n",
      "Bilirubin : 0.8738544843787969\n",
      "Cholesterol : 0.2981384780228495\n",
      "Creatinine : 0.11778445176761146\n",
      "DiasABP : 0.20394686836129197\n",
      "FiO2 : 0.358766703394349\n",
      "GCS : 0.2739524986648289\n",
      "Glucose : 0.9010269123998977\n",
      "HCO3 : 0.49853002048972683\n",
      "HCT : 0.5850348302948677\n",
      "HR : 0.29580524649610546\n",
      "K : 0.5339739686485152\n",
      "Lactate : 0.4621353793984186\n",
      "MAP : 0.21426841338784167\n",
      "MechVent : 0.009547680541271817\n",
      "Mg : 0.5017855691331706\n",
      "NIDiasABP : 0.2885183504452916\n",
      "NIMAP : 0.19274308763110395\n",
      "NISysABP : 0.25096960646717886\n",
      "Na : 0.5429508910292875\n",
      "PaCO2 : 0.4624538784173391\n",
      "PaO2 : 0.5384221415891975\n",
      "Platelets : 0.39252910687609344\n",
      "RespRate : 0.7099768435701227\n",
      "SaO2 : 0.3059230255392362\n",
      "SysABP : 0.22888568304658857\n",
      "Temp : 0.2507179624687218\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.003235762201616349\n",
      "Urine : 0.3987437472434232\n",
      "WBC : 0.14967863592699768\n",
      "Weight : 0.11789380355931524\n",
      "Ph : 0.07760851219185988\n",
      "Normal Weight\n",
      "-------------\n",
      "ALP : 0.3447937644235448\n",
      "ALT : 0.19675068562772396\n",
      "AST : 0.14357344418966528\n",
      "Albumin : 0.7787357886125517\n",
      "BUN : 0.2148464542628673\n",
      "Bilirubin : 0.3618450479116823\n",
      "Cholesterol : 0.9274812209050906\n",
      "Creatinine : 0.19547774663951747\n",
      "DiasABP : 0.1839550543594675\n",
      "FiO2 : 0.4202355433646209\n",
      "GCS : 0.2985144569357866\n",
      "Glucose : 0.5707214365304913\n",
      "HCO3 : 0.4061510945260135\n",
      "HCT : 0.4578193174832344\n",
      "HR : 0.25842538256809805\n",
      "K : 0.5779007385091064\n",
      "Lactate : 0.30862499395291376\n",
      "MAP : 0.16994662760526316\n",
      "MechVent : 0.009619053904941288\n",
      "Mg : 0.5843370570748591\n",
      "NIDiasABP : 0.20639198184346305\n",
      "NIMAP : 0.13874497373104558\n",
      "NISysABP : 0.21435494820010684\n",
      "Na : 0.5009211032547202\n",
      "PaCO2 : 0.49545717237104303\n",
      "PaO2 : 0.4847650666130561\n",
      "Platelets : 0.32628286927868894\n",
      "RespRate : 0.4439731710411805\n",
      "SaO2 : 0.4388679233713408\n",
      "SysABP : 0.22566328032777183\n",
      "Temp : 0.2968777591304075\n",
      "TroponinI : 0.6095391843916117\n",
      "TroponinT : 0.27541861972684584\n",
      "Urine : 0.3239567335168024\n",
      "WBC : 0.12790979414312842\n",
      "Weight : 0.11452778631073578\n",
      "Ph : 0.060245151322367574\n",
      "Overweight\n",
      "-------------\n",
      "ALP : 0.2130859747726841\n",
      "ALT : 0.2348171242300844\n",
      "AST : 0.2694910977272869\n",
      "Albumin : 0.6900517528407268\n",
      "BUN : 0.18227505132470806\n",
      "Bilirubin : 0.23062530532808345\n",
      "Cholesterol : 1.0379753158763556\n",
      "Creatinine : 0.18845434003246883\n",
      "DiasABP : 0.2029711367736911\n",
      "FiO2 : 0.3954176078614923\n",
      "GCS : 0.29162552205211756\n",
      "Glucose : 0.6459503624255607\n",
      "HCO3 : 0.37743317064973425\n",
      "HCT : 0.5309486260391978\n",
      "HR : 0.24609786193888677\n",
      "K : 0.6148418634698282\n",
      "Lactate : 0.467530553271605\n",
      "MAP : 0.18471073098129642\n",
      "MechVent : 0.00895063687158114\n",
      "Mg : 0.5225134107841675\n",
      "NIDiasABP : 0.2214175985162663\n",
      "NIMAP : 0.1472066268003436\n",
      "NISysABP : 0.23155639230611008\n",
      "Na : 0.4248715120369724\n",
      "PaCO2 : 0.4973410812965392\n",
      "PaO2 : 0.46031564273089065\n",
      "Platelets : 0.3618795206720569\n",
      "RespRate : 0.4347007220228369\n",
      "SaO2 : 0.374092400151626\n",
      "SysABP : 0.24040491738047615\n",
      "Temp : 0.2585656698732238\n",
      "TroponinI : 0.8607272513940917\n",
      "TroponinT : 0.4650496585132947\n",
      "Urine : 0.33717218918329267\n",
      "WBC : 0.11210217442876141\n",
      "Weight : 0.11050562609815173\n",
      "Ph : 0.05990464503873025\n",
      "Obesity 1\n",
      "-------------\n",
      "ALP : 0.18148233861528848\n",
      "ALT : 0.14655079337582164\n",
      "AST : 0.1387765018334754\n",
      "Albumin : 0.48583935021778313\n",
      "BUN : 0.20248313155914693\n",
      "Bilirubin : 0.21395115755318872\n",
      "Cholesterol : 0.5856328796928869\n",
      "Creatinine : 0.14965085668566042\n",
      "DiasABP : 0.20074412448127743\n",
      "FiO2 : 0.5327741759540591\n",
      "GCS : 0.25906635665752636\n",
      "Glucose : 0.5419293867604116\n",
      "HCO3 : 0.40516176464135534\n",
      "HCT : 0.5239430350583951\n",
      "HR : 0.24272102699498424\n",
      "K : 0.5488347572331603\n",
      "Lactate : 0.4352612762516357\n",
      "MAP : 0.18191528636370977\n",
      "MechVent : 0.009208090458957952\n",
      "Mg : 0.46707780638443375\n",
      "NIDiasABP : 0.2124912778767664\n",
      "NIMAP : 0.1349888905911548\n",
      "NISysABP : 0.22403527868176834\n",
      "Na : 0.45865443605747713\n",
      "PaCO2 : 0.44902012869589564\n",
      "PaO2 : 0.4919138873408172\n",
      "Platelets : 0.3297272579749662\n",
      "RespRate : 0.45567497375200805\n",
      "SaO2 : 0.3728483373240035\n",
      "SysABP : 0.21613782067761017\n",
      "Temp : 0.24587419148057096\n",
      "TroponinI : 0.6150509588086341\n",
      "TroponinT : 0.2826231655910522\n",
      "Urine : 0.39157045958189435\n",
      "WBC : 0.18403735496671716\n",
      "Weight : 0.12625924981898257\n",
      "Ph : 0.9469372258107145\n",
      "Obesity 2\n",
      "-------------\n",
      "ALP : 0.3475744245471861\n",
      "ALT : 0.05479427318898274\n",
      "AST : 0.1675688419112458\n",
      "Albumin : 0.532378795538056\n",
      "BUN : 0.2639632536211254\n",
      "Bilirubin : 0.21864807305334888\n",
      "Cholesterol : 0.2419623540959394\n",
      "Creatinine : 0.1474073912260663\n",
      "DiasABP : 0.20743425370174892\n",
      "FiO2 : 0.47477315214644633\n",
      "GCS : 0.2925111836838947\n",
      "Glucose : 0.6922619339571453\n",
      "HCO3 : 0.3215023718754384\n",
      "HCT : 0.44716602413189593\n",
      "HR : 0.24097303304084172\n",
      "K : 0.5748965005310039\n",
      "Lactate : 0.44257166944648785\n",
      "MAP : 0.1768207978508765\n",
      "MechVent : 0.010618269125755213\n",
      "Mg : 0.6104385392214591\n",
      "NIDiasABP : 0.24344522741138014\n",
      "NIMAP : 0.10910234368001569\n",
      "NISysABP : 0.21449006533324638\n",
      "Na : 0.3227297842808681\n",
      "PaCO2 : 0.4332853217694135\n",
      "PaO2 : 0.3413588412381844\n",
      "Platelets : 0.35486556724264273\n",
      "RespRate : 0.4776022147400815\n",
      "SaO2 : 0.417498492531639\n",
      "SysABP : 0.20947018408239798\n",
      "Temp : 0.3203176691156558\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.9221216319912744\n",
      "Urine : 0.35477384274711427\n",
      "WBC : 0.09318921703522798\n",
      "Weight : 0.1091423637594448\n",
      "Ph : 0.07019459789158267\n",
      "Obesity 3\n",
      "-------------\n",
      "ALP : 0.23041289606255475\n",
      "ALT : 0.6436962125058109\n",
      "AST : 0.09653561796002381\n",
      "Albumin : 0.49852452626521615\n",
      "BUN : 0.24348618763161758\n",
      "Bilirubin : 0.21715339450041612\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.13078669043992983\n",
      "DiasABP : 0.19195288845093986\n",
      "FiO2 : 0.6280103381400287\n",
      "GCS : 0.28955436300042836\n",
      "Glucose : 0.5574112445863414\n",
      "HCO3 : 0.4706381807639943\n",
      "HCT : 0.54924893435794\n",
      "HR : 0.24550168840761852\n",
      "K : 0.6245977253853949\n",
      "Lactate : 0.37632509243388546\n",
      "MAP : 0.16122391708958592\n",
      "MechVent : 0.012647782784117885\n",
      "Mg : 0.5068650283700716\n",
      "NIDiasABP : 0.2827437399608298\n",
      "NIMAP : 0.176071712953009\n",
      "NISysABP : 0.26147178635989626\n",
      "Na : 0.41146332183421924\n",
      "PaCO2 : 0.5388499735941227\n",
      "PaO2 : 0.3887194825500364\n",
      "Platelets : 0.2630486641333494\n",
      "RespRate : 0.4476765037042773\n",
      "SaO2 : 0.4628381982082615\n",
      "SysABP : 0.22888551336478877\n",
      "Temp : 0.2811236891311357\n",
      "TroponinI : 0.28791450320762885\n",
      "TroponinT : 0.8195205366174975\n",
      "Urine : 0.36892198844520996\n",
      "WBC : 0.0950156098067894\n",
      "Weight : 0.16591385730254696\n",
      "Ph : 0.056852252884312025\n"
     ]
    }
   ],
   "source": [
    "print(\"SAITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_saits_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saits_mae_standard = toolkits.create_table(testing_mae_saits_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>General</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Undefined Gender</th>\n",
       "      <th>+65</th>\n",
       "      <th>-65</th>\n",
       "      <th>ICUType 1</th>\n",
       "      <th>ICUType 2</th>\n",
       "      <th>ICUType 3</th>\n",
       "      <th>ICUType 4</th>\n",
       "      <th>Undefined classification</th>\n",
       "      <th>Low Weight</th>\n",
       "      <th>Normal Weight</th>\n",
       "      <th>Overweight</th>\n",
       "      <th>Obesity 1</th>\n",
       "      <th>Obesity 2</th>\n",
       "      <th>Obesity 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALP</td>\n",
       "      <td>0.355492</td>\n",
       "      <td>0.398791</td>\n",
       "      <td>0.388989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331751</td>\n",
       "      <td>0.740879</td>\n",
       "      <td>0.222358</td>\n",
       "      <td>0.195952</td>\n",
       "      <td>0.723086</td>\n",
       "      <td>0.505640</td>\n",
       "      <td>0.296971</td>\n",
       "      <td>0.216395</td>\n",
       "      <td>0.284060</td>\n",
       "      <td>0.236059</td>\n",
       "      <td>0.184009</td>\n",
       "      <td>0.347203</td>\n",
       "      <td>0.266653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALT</td>\n",
       "      <td>0.346164</td>\n",
       "      <td>0.267721</td>\n",
       "      <td>0.253539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280160</td>\n",
       "      <td>0.329866</td>\n",
       "      <td>0.296968</td>\n",
       "      <td>0.351267</td>\n",
       "      <td>0.311489</td>\n",
       "      <td>0.272558</td>\n",
       "      <td>0.325780</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.251692</td>\n",
       "      <td>0.305938</td>\n",
       "      <td>0.274798</td>\n",
       "      <td>0.318639</td>\n",
       "      <td>0.823624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AST</td>\n",
       "      <td>0.283204</td>\n",
       "      <td>0.257204</td>\n",
       "      <td>0.204749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206225</td>\n",
       "      <td>0.243144</td>\n",
       "      <td>0.340113</td>\n",
       "      <td>0.273045</td>\n",
       "      <td>0.305834</td>\n",
       "      <td>0.306141</td>\n",
       "      <td>0.244520</td>\n",
       "      <td>0.699315</td>\n",
       "      <td>0.218977</td>\n",
       "      <td>0.318423</td>\n",
       "      <td>0.207619</td>\n",
       "      <td>0.237748</td>\n",
       "      <td>0.209382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albumin</td>\n",
       "      <td>0.586538</td>\n",
       "      <td>0.609286</td>\n",
       "      <td>0.621456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.536556</td>\n",
       "      <td>0.658661</td>\n",
       "      <td>0.490084</td>\n",
       "      <td>0.717311</td>\n",
       "      <td>0.708582</td>\n",
       "      <td>0.754085</td>\n",
       "      <td>0.776171</td>\n",
       "      <td>1.172093</td>\n",
       "      <td>0.840292</td>\n",
       "      <td>0.814925</td>\n",
       "      <td>0.467807</td>\n",
       "      <td>0.670209</td>\n",
       "      <td>0.729714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUN</td>\n",
       "      <td>0.235495</td>\n",
       "      <td>0.253363</td>\n",
       "      <td>0.228878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242391</td>\n",
       "      <td>0.244288</td>\n",
       "      <td>0.213308</td>\n",
       "      <td>0.222898</td>\n",
       "      <td>0.294829</td>\n",
       "      <td>0.209661</td>\n",
       "      <td>0.257893</td>\n",
       "      <td>0.200723</td>\n",
       "      <td>0.219590</td>\n",
       "      <td>0.235670</td>\n",
       "      <td>0.210442</td>\n",
       "      <td>0.309354</td>\n",
       "      <td>0.251667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bilirubin</td>\n",
       "      <td>0.292134</td>\n",
       "      <td>0.301597</td>\n",
       "      <td>0.249420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231827</td>\n",
       "      <td>0.336409</td>\n",
       "      <td>0.200403</td>\n",
       "      <td>0.318250</td>\n",
       "      <td>0.280385</td>\n",
       "      <td>0.267887</td>\n",
       "      <td>0.312005</td>\n",
       "      <td>0.954099</td>\n",
       "      <td>0.392959</td>\n",
       "      <td>0.242916</td>\n",
       "      <td>0.335206</td>\n",
       "      <td>0.310083</td>\n",
       "      <td>0.216768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cholesterol</td>\n",
       "      <td>0.861914</td>\n",
       "      <td>1.451439</td>\n",
       "      <td>0.918617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.736495</td>\n",
       "      <td>1.183795</td>\n",
       "      <td>1.002457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.906296</td>\n",
       "      <td>0.725019</td>\n",
       "      <td>0.618523</td>\n",
       "      <td>0.085870</td>\n",
       "      <td>0.704775</td>\n",
       "      <td>1.232571</td>\n",
       "      <td>0.533449</td>\n",
       "      <td>0.347422</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Creatinine</td>\n",
       "      <td>0.227236</td>\n",
       "      <td>0.218419</td>\n",
       "      <td>0.210438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200026</td>\n",
       "      <td>0.211177</td>\n",
       "      <td>0.193449</td>\n",
       "      <td>0.183120</td>\n",
       "      <td>0.265242</td>\n",
       "      <td>0.217446</td>\n",
       "      <td>0.233475</td>\n",
       "      <td>0.177189</td>\n",
       "      <td>0.227362</td>\n",
       "      <td>0.206401</td>\n",
       "      <td>0.179911</td>\n",
       "      <td>0.192612</td>\n",
       "      <td>0.192658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DiasABP</td>\n",
       "      <td>0.256964</td>\n",
       "      <td>0.260941</td>\n",
       "      <td>0.240924</td>\n",
       "      <td>0.182851</td>\n",
       "      <td>0.271154</td>\n",
       "      <td>0.231552</td>\n",
       "      <td>0.299906</td>\n",
       "      <td>0.221175</td>\n",
       "      <td>0.250333</td>\n",
       "      <td>0.241564</td>\n",
       "      <td>0.250752</td>\n",
       "      <td>0.246645</td>\n",
       "      <td>0.246105</td>\n",
       "      <td>0.255920</td>\n",
       "      <td>0.251888</td>\n",
       "      <td>0.235695</td>\n",
       "      <td>0.250253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FiO2</td>\n",
       "      <td>0.484346</td>\n",
       "      <td>0.452276</td>\n",
       "      <td>0.517671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491061</td>\n",
       "      <td>0.501327</td>\n",
       "      <td>0.482516</td>\n",
       "      <td>0.467761</td>\n",
       "      <td>0.507122</td>\n",
       "      <td>0.438264</td>\n",
       "      <td>0.460432</td>\n",
       "      <td>0.476691</td>\n",
       "      <td>0.439314</td>\n",
       "      <td>0.414027</td>\n",
       "      <td>0.507958</td>\n",
       "      <td>0.501128</td>\n",
       "      <td>0.573521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GCS</td>\n",
       "      <td>0.277826</td>\n",
       "      <td>0.266519</td>\n",
       "      <td>0.292769</td>\n",
       "      <td>0.136040</td>\n",
       "      <td>0.276112</td>\n",
       "      <td>0.273780</td>\n",
       "      <td>0.264396</td>\n",
       "      <td>0.406222</td>\n",
       "      <td>0.266837</td>\n",
       "      <td>0.245789</td>\n",
       "      <td>0.241954</td>\n",
       "      <td>0.314878</td>\n",
       "      <td>0.326068</td>\n",
       "      <td>0.331490</td>\n",
       "      <td>0.288411</td>\n",
       "      <td>0.299649</td>\n",
       "      <td>0.295149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.557457</td>\n",
       "      <td>0.561478</td>\n",
       "      <td>0.590789</td>\n",
       "      <td>0.240908</td>\n",
       "      <td>0.590401</td>\n",
       "      <td>0.619428</td>\n",
       "      <td>0.607252</td>\n",
       "      <td>0.465918</td>\n",
       "      <td>0.685012</td>\n",
       "      <td>0.443831</td>\n",
       "      <td>0.597806</td>\n",
       "      <td>0.981742</td>\n",
       "      <td>0.534537</td>\n",
       "      <td>0.562222</td>\n",
       "      <td>0.453594</td>\n",
       "      <td>0.578586</td>\n",
       "      <td>0.632032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HCO3</td>\n",
       "      <td>0.422355</td>\n",
       "      <td>0.431529</td>\n",
       "      <td>0.433305</td>\n",
       "      <td>0.214275</td>\n",
       "      <td>0.461885</td>\n",
       "      <td>0.446012</td>\n",
       "      <td>0.410033</td>\n",
       "      <td>0.348082</td>\n",
       "      <td>0.486334</td>\n",
       "      <td>0.399666</td>\n",
       "      <td>0.476982</td>\n",
       "      <td>0.577168</td>\n",
       "      <td>0.430699</td>\n",
       "      <td>0.355796</td>\n",
       "      <td>0.380486</td>\n",
       "      <td>0.310929</td>\n",
       "      <td>0.473612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HCT</td>\n",
       "      <td>0.547848</td>\n",
       "      <td>0.565190</td>\n",
       "      <td>0.536183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509117</td>\n",
       "      <td>0.527232</td>\n",
       "      <td>0.563329</td>\n",
       "      <td>0.492718</td>\n",
       "      <td>0.589372</td>\n",
       "      <td>0.515674</td>\n",
       "      <td>0.563045</td>\n",
       "      <td>0.545528</td>\n",
       "      <td>0.493551</td>\n",
       "      <td>0.545997</td>\n",
       "      <td>0.541935</td>\n",
       "      <td>0.537001</td>\n",
       "      <td>0.585495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HR</td>\n",
       "      <td>0.278652</td>\n",
       "      <td>0.284302</td>\n",
       "      <td>0.275293</td>\n",
       "      <td>0.277222</td>\n",
       "      <td>0.280410</td>\n",
       "      <td>0.274431</td>\n",
       "      <td>0.271171</td>\n",
       "      <td>0.238003</td>\n",
       "      <td>0.286875</td>\n",
       "      <td>0.297353</td>\n",
       "      <td>0.285716</td>\n",
       "      <td>0.329126</td>\n",
       "      <td>0.272804</td>\n",
       "      <td>0.258814</td>\n",
       "      <td>0.250932</td>\n",
       "      <td>0.256350</td>\n",
       "      <td>0.266091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>K</td>\n",
       "      <td>0.617374</td>\n",
       "      <td>0.636121</td>\n",
       "      <td>0.591587</td>\n",
       "      <td>0.648198</td>\n",
       "      <td>0.565924</td>\n",
       "      <td>0.627978</td>\n",
       "      <td>0.577943</td>\n",
       "      <td>0.669014</td>\n",
       "      <td>0.707610</td>\n",
       "      <td>0.524622</td>\n",
       "      <td>0.612569</td>\n",
       "      <td>0.564057</td>\n",
       "      <td>0.599145</td>\n",
       "      <td>0.655028</td>\n",
       "      <td>0.532059</td>\n",
       "      <td>0.533458</td>\n",
       "      <td>0.600833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lactate</td>\n",
       "      <td>0.360208</td>\n",
       "      <td>0.334491</td>\n",
       "      <td>0.406861</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329401</td>\n",
       "      <td>0.372352</td>\n",
       "      <td>0.469862</td>\n",
       "      <td>0.339788</td>\n",
       "      <td>0.468984</td>\n",
       "      <td>0.406692</td>\n",
       "      <td>0.381816</td>\n",
       "      <td>0.708488</td>\n",
       "      <td>0.339499</td>\n",
       "      <td>0.431342</td>\n",
       "      <td>0.421622</td>\n",
       "      <td>0.363462</td>\n",
       "      <td>0.390225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MAP</td>\n",
       "      <td>0.276148</td>\n",
       "      <td>0.280798</td>\n",
       "      <td>0.295768</td>\n",
       "      <td>0.271983</td>\n",
       "      <td>0.299645</td>\n",
       "      <td>0.285722</td>\n",
       "      <td>0.291251</td>\n",
       "      <td>0.281118</td>\n",
       "      <td>0.274120</td>\n",
       "      <td>0.276270</td>\n",
       "      <td>0.293808</td>\n",
       "      <td>0.252903</td>\n",
       "      <td>0.279803</td>\n",
       "      <td>0.297996</td>\n",
       "      <td>0.287979</td>\n",
       "      <td>0.264226</td>\n",
       "      <td>0.279977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MechVent</td>\n",
       "      <td>0.016132</td>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.014512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012942</td>\n",
       "      <td>0.018278</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>0.012379</td>\n",
       "      <td>0.015399</td>\n",
       "      <td>0.016767</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>0.011590</td>\n",
       "      <td>0.013785</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.014115</td>\n",
       "      <td>0.014136</td>\n",
       "      <td>0.021396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mg</td>\n",
       "      <td>0.542887</td>\n",
       "      <td>0.555896</td>\n",
       "      <td>0.558361</td>\n",
       "      <td>0.385353</td>\n",
       "      <td>0.539592</td>\n",
       "      <td>0.533720</td>\n",
       "      <td>0.489979</td>\n",
       "      <td>0.605776</td>\n",
       "      <td>0.597667</td>\n",
       "      <td>0.542778</td>\n",
       "      <td>0.492026</td>\n",
       "      <td>0.481586</td>\n",
       "      <td>0.584020</td>\n",
       "      <td>0.517669</td>\n",
       "      <td>0.483443</td>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.548680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NIDiasABP</td>\n",
       "      <td>0.238053</td>\n",
       "      <td>0.264227</td>\n",
       "      <td>0.242982</td>\n",
       "      <td>0.220633</td>\n",
       "      <td>0.258731</td>\n",
       "      <td>0.236991</td>\n",
       "      <td>0.227916</td>\n",
       "      <td>0.243122</td>\n",
       "      <td>0.254380</td>\n",
       "      <td>0.258237</td>\n",
       "      <td>0.252622</td>\n",
       "      <td>0.285099</td>\n",
       "      <td>0.236137</td>\n",
       "      <td>0.242912</td>\n",
       "      <td>0.215284</td>\n",
       "      <td>0.242149</td>\n",
       "      <td>0.318941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NIMAP</td>\n",
       "      <td>0.177311</td>\n",
       "      <td>0.189465</td>\n",
       "      <td>0.169662</td>\n",
       "      <td>0.111674</td>\n",
       "      <td>0.177068</td>\n",
       "      <td>0.181710</td>\n",
       "      <td>0.163234</td>\n",
       "      <td>0.162545</td>\n",
       "      <td>0.180201</td>\n",
       "      <td>0.187123</td>\n",
       "      <td>0.184013</td>\n",
       "      <td>0.250974</td>\n",
       "      <td>0.177435</td>\n",
       "      <td>0.169020</td>\n",
       "      <td>0.170466</td>\n",
       "      <td>0.161756</td>\n",
       "      <td>0.222985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NISysABP</td>\n",
       "      <td>0.305999</td>\n",
       "      <td>0.314224</td>\n",
       "      <td>0.313024</td>\n",
       "      <td>0.146763</td>\n",
       "      <td>0.323843</td>\n",
       "      <td>0.282415</td>\n",
       "      <td>0.295748</td>\n",
       "      <td>0.315086</td>\n",
       "      <td>0.305427</td>\n",
       "      <td>0.317214</td>\n",
       "      <td>0.309477</td>\n",
       "      <td>0.288290</td>\n",
       "      <td>0.298370</td>\n",
       "      <td>0.298371</td>\n",
       "      <td>0.291263</td>\n",
       "      <td>0.298250</td>\n",
       "      <td>0.340271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Na</td>\n",
       "      <td>0.509447</td>\n",
       "      <td>0.527968</td>\n",
       "      <td>0.496132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475615</td>\n",
       "      <td>0.537811</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.460561</td>\n",
       "      <td>0.484647</td>\n",
       "      <td>0.548096</td>\n",
       "      <td>0.571253</td>\n",
       "      <td>0.509801</td>\n",
       "      <td>0.547527</td>\n",
       "      <td>0.502086</td>\n",
       "      <td>0.421026</td>\n",
       "      <td>0.404838</td>\n",
       "      <td>0.453562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PaCO2</td>\n",
       "      <td>0.544875</td>\n",
       "      <td>0.556747</td>\n",
       "      <td>0.517135</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.484015</td>\n",
       "      <td>0.546376</td>\n",
       "      <td>0.474012</td>\n",
       "      <td>0.477323</td>\n",
       "      <td>0.665766</td>\n",
       "      <td>0.476861</td>\n",
       "      <td>0.639565</td>\n",
       "      <td>0.600192</td>\n",
       "      <td>0.494365</td>\n",
       "      <td>0.494167</td>\n",
       "      <td>0.451934</td>\n",
       "      <td>0.427268</td>\n",
       "      <td>0.554823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PaO2</td>\n",
       "      <td>0.504418</td>\n",
       "      <td>0.499373</td>\n",
       "      <td>0.481931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452471</td>\n",
       "      <td>0.506370</td>\n",
       "      <td>0.516944</td>\n",
       "      <td>0.473982</td>\n",
       "      <td>0.492036</td>\n",
       "      <td>0.495441</td>\n",
       "      <td>0.541774</td>\n",
       "      <td>0.592228</td>\n",
       "      <td>0.488552</td>\n",
       "      <td>0.472735</td>\n",
       "      <td>0.473042</td>\n",
       "      <td>0.346057</td>\n",
       "      <td>0.397762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Platelets</td>\n",
       "      <td>0.379108</td>\n",
       "      <td>0.395210</td>\n",
       "      <td>0.355038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375186</td>\n",
       "      <td>0.372662</td>\n",
       "      <td>0.341986</td>\n",
       "      <td>0.348146</td>\n",
       "      <td>0.453156</td>\n",
       "      <td>0.339871</td>\n",
       "      <td>0.412391</td>\n",
       "      <td>0.437693</td>\n",
       "      <td>0.344135</td>\n",
       "      <td>0.404718</td>\n",
       "      <td>0.375442</td>\n",
       "      <td>0.399016</td>\n",
       "      <td>0.259510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RespRate</td>\n",
       "      <td>0.497555</td>\n",
       "      <td>0.509812</td>\n",
       "      <td>0.492689</td>\n",
       "      <td>0.032527</td>\n",
       "      <td>0.478573</td>\n",
       "      <td>0.490380</td>\n",
       "      <td>0.484568</td>\n",
       "      <td>0.426515</td>\n",
       "      <td>0.529868</td>\n",
       "      <td>0.519861</td>\n",
       "      <td>0.508258</td>\n",
       "      <td>0.688640</td>\n",
       "      <td>0.466189</td>\n",
       "      <td>0.449605</td>\n",
       "      <td>0.475635</td>\n",
       "      <td>0.486548</td>\n",
       "      <td>0.469704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SaO2</td>\n",
       "      <td>0.395204</td>\n",
       "      <td>0.592285</td>\n",
       "      <td>0.409306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.426569</td>\n",
       "      <td>0.622148</td>\n",
       "      <td>0.547977</td>\n",
       "      <td>0.351711</td>\n",
       "      <td>1.584391</td>\n",
       "      <td>0.453893</td>\n",
       "      <td>0.649609</td>\n",
       "      <td>0.399676</td>\n",
       "      <td>0.401025</td>\n",
       "      <td>0.414118</td>\n",
       "      <td>0.394813</td>\n",
       "      <td>0.438395</td>\n",
       "      <td>0.490332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SysABP</td>\n",
       "      <td>0.287632</td>\n",
       "      <td>0.283227</td>\n",
       "      <td>0.285330</td>\n",
       "      <td>0.189362</td>\n",
       "      <td>0.287563</td>\n",
       "      <td>0.262217</td>\n",
       "      <td>0.304643</td>\n",
       "      <td>0.268987</td>\n",
       "      <td>0.274074</td>\n",
       "      <td>0.281045</td>\n",
       "      <td>0.285016</td>\n",
       "      <td>0.277839</td>\n",
       "      <td>0.268097</td>\n",
       "      <td>0.281955</td>\n",
       "      <td>0.248811</td>\n",
       "      <td>0.264923</td>\n",
       "      <td>0.266914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Temp</td>\n",
       "      <td>0.365474</td>\n",
       "      <td>0.392193</td>\n",
       "      <td>0.389191</td>\n",
       "      <td>0.153703</td>\n",
       "      <td>0.389892</td>\n",
       "      <td>0.395586</td>\n",
       "      <td>0.439425</td>\n",
       "      <td>0.284733</td>\n",
       "      <td>0.558586</td>\n",
       "      <td>0.432690</td>\n",
       "      <td>0.444269</td>\n",
       "      <td>0.309291</td>\n",
       "      <td>0.349890</td>\n",
       "      <td>0.319342</td>\n",
       "      <td>0.290006</td>\n",
       "      <td>0.364659</td>\n",
       "      <td>0.331229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TroponinI</td>\n",
       "      <td>0.574282</td>\n",
       "      <td>0.399557</td>\n",
       "      <td>0.526789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912480</td>\n",
       "      <td>0.465250</td>\n",
       "      <td>1.414748</td>\n",
       "      <td>0.537893</td>\n",
       "      <td>0.343210</td>\n",
       "      <td>0.860652</td>\n",
       "      <td>0.556770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546397</td>\n",
       "      <td>0.781567</td>\n",
       "      <td>0.522427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TroponinT</td>\n",
       "      <td>0.562503</td>\n",
       "      <td>0.612332</td>\n",
       "      <td>0.716181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560979</td>\n",
       "      <td>0.707258</td>\n",
       "      <td>0.619598</td>\n",
       "      <td>0.703199</td>\n",
       "      <td>0.538425</td>\n",
       "      <td>0.567663</td>\n",
       "      <td>0.526654</td>\n",
       "      <td>0.194489</td>\n",
       "      <td>0.410644</td>\n",
       "      <td>0.652150</td>\n",
       "      <td>0.412647</td>\n",
       "      <td>1.451607</td>\n",
       "      <td>1.036925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Urine</td>\n",
       "      <td>0.455580</td>\n",
       "      <td>0.422339</td>\n",
       "      <td>0.467929</td>\n",
       "      <td>0.420597</td>\n",
       "      <td>0.403842</td>\n",
       "      <td>0.551916</td>\n",
       "      <td>0.520197</td>\n",
       "      <td>0.434151</td>\n",
       "      <td>0.444566</td>\n",
       "      <td>0.493390</td>\n",
       "      <td>0.474954</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.388207</td>\n",
       "      <td>0.433395</td>\n",
       "      <td>0.493662</td>\n",
       "      <td>0.442158</td>\n",
       "      <td>0.554933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WBC</td>\n",
       "      <td>0.151106</td>\n",
       "      <td>0.147661</td>\n",
       "      <td>0.168883</td>\n",
       "      <td>0.018873</td>\n",
       "      <td>0.168543</td>\n",
       "      <td>0.151410</td>\n",
       "      <td>0.118287</td>\n",
       "      <td>0.178617</td>\n",
       "      <td>0.152978</td>\n",
       "      <td>0.150609</td>\n",
       "      <td>0.166715</td>\n",
       "      <td>0.190047</td>\n",
       "      <td>0.165702</td>\n",
       "      <td>0.141938</td>\n",
       "      <td>0.224131</td>\n",
       "      <td>0.157970</td>\n",
       "      <td>0.109519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Weight</td>\n",
       "      <td>0.140954</td>\n",
       "      <td>0.146240</td>\n",
       "      <td>0.129140</td>\n",
       "      <td>0.124492</td>\n",
       "      <td>0.124462</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.129569</td>\n",
       "      <td>0.151631</td>\n",
       "      <td>0.133710</td>\n",
       "      <td>0.153065</td>\n",
       "      <td>0.135421</td>\n",
       "      <td>0.237374</td>\n",
       "      <td>0.127653</td>\n",
       "      <td>0.094309</td>\n",
       "      <td>0.130453</td>\n",
       "      <td>0.137707</td>\n",
       "      <td>0.288451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ph</td>\n",
       "      <td>0.164482</td>\n",
       "      <td>0.102898</td>\n",
       "      <td>0.111622</td>\n",
       "      <td>0.035273</td>\n",
       "      <td>0.100316</td>\n",
       "      <td>0.101102</td>\n",
       "      <td>0.114209</td>\n",
       "      <td>0.117398</td>\n",
       "      <td>0.101323</td>\n",
       "      <td>0.096553</td>\n",
       "      <td>0.105180</td>\n",
       "      <td>0.109046</td>\n",
       "      <td>0.102359</td>\n",
       "      <td>0.101065</td>\n",
       "      <td>1.006256</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>0.101651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0   General    Female      Male  Undefined Gender       +65  \\\n",
       "0           ALP  0.355492  0.398791  0.388989          0.000000  0.331751   \n",
       "1           ALT  0.346164  0.267721  0.253539          0.000000  0.280160   \n",
       "2           AST  0.283204  0.257204  0.204749          0.000000  0.206225   \n",
       "3       Albumin  0.586538  0.609286  0.621456          0.000000  0.536556   \n",
       "4           BUN  0.235495  0.253363  0.228878          0.000000  0.242391   \n",
       "5     Bilirubin  0.292134  0.301597  0.249420          0.000000  0.231827   \n",
       "6   Cholesterol  0.861914  1.451439  0.918617          0.000000  0.736495   \n",
       "7    Creatinine  0.227236  0.218419  0.210438          0.000000  0.200026   \n",
       "8       DiasABP  0.256964  0.260941  0.240924          0.182851  0.271154   \n",
       "9          FiO2  0.484346  0.452276  0.517671          0.000000  0.491061   \n",
       "10          GCS  0.277826  0.266519  0.292769          0.136040  0.276112   \n",
       "11      Glucose  0.557457  0.561478  0.590789          0.240908  0.590401   \n",
       "12         HCO3  0.422355  0.431529  0.433305          0.214275  0.461885   \n",
       "13          HCT  0.547848  0.565190  0.536183          0.000000  0.509117   \n",
       "14           HR  0.278652  0.284302  0.275293          0.277222  0.280410   \n",
       "15            K  0.617374  0.636121  0.591587          0.648198  0.565924   \n",
       "16      Lactate  0.360208  0.334491  0.406861          0.000000  0.329401   \n",
       "17          MAP  0.276148  0.280798  0.295768          0.271983  0.299645   \n",
       "18     MechVent  0.016132  0.014552  0.014512          0.000000  0.012942   \n",
       "19           Mg  0.542887  0.555896  0.558361          0.385353  0.539592   \n",
       "20    NIDiasABP  0.238053  0.264227  0.242982          0.220633  0.258731   \n",
       "21        NIMAP  0.177311  0.189465  0.169662          0.111674  0.177068   \n",
       "22     NISysABP  0.305999  0.314224  0.313024          0.146763  0.323843   \n",
       "23           Na  0.509447  0.527968  0.496132          0.000000  0.475615   \n",
       "24        PaCO2  0.544875  0.556747  0.517135          0.000000  0.484015   \n",
       "25         PaO2  0.504418  0.499373  0.481931          0.000000  0.452471   \n",
       "26    Platelets  0.379108  0.395210  0.355038          0.000000  0.375186   \n",
       "27     RespRate  0.497555  0.509812  0.492689          0.032527  0.478573   \n",
       "28         SaO2  0.395204  0.592285  0.409306          0.000000  0.426569   \n",
       "29       SysABP  0.287632  0.283227  0.285330          0.189362  0.287563   \n",
       "30         Temp  0.365474  0.392193  0.389191          0.153703  0.389892   \n",
       "31    TroponinI  0.574282  0.399557  0.526789          0.000000  0.912480   \n",
       "32    TroponinT  0.562503  0.612332  0.716181          0.000000  0.560979   \n",
       "33        Urine  0.455580  0.422339  0.467929          0.420597  0.403842   \n",
       "34          WBC  0.151106  0.147661  0.168883          0.018873  0.168543   \n",
       "35       Weight  0.140954  0.146240  0.129140          0.124492  0.124462   \n",
       "36           Ph  0.164482  0.102898  0.111622          0.035273  0.100316   \n",
       "\n",
       "         -65  ICUType 1  ICUType 2  ICUType 3  ICUType 4  \\\n",
       "0   0.740879   0.222358   0.195952   0.723086   0.505640   \n",
       "1   0.329866   0.296968   0.351267   0.311489   0.272558   \n",
       "2   0.243144   0.340113   0.273045   0.305834   0.306141   \n",
       "3   0.658661   0.490084   0.717311   0.708582   0.754085   \n",
       "4   0.244288   0.213308   0.222898   0.294829   0.209661   \n",
       "5   0.336409   0.200403   0.318250   0.280385   0.267887   \n",
       "6   1.183795   1.002457   0.000000   0.906296   0.725019   \n",
       "7   0.211177   0.193449   0.183120   0.265242   0.217446   \n",
       "8   0.231552   0.299906   0.221175   0.250333   0.241564   \n",
       "9   0.501327   0.482516   0.467761   0.507122   0.438264   \n",
       "10  0.273780   0.264396   0.406222   0.266837   0.245789   \n",
       "11  0.619428   0.607252   0.465918   0.685012   0.443831   \n",
       "12  0.446012   0.410033   0.348082   0.486334   0.399666   \n",
       "13  0.527232   0.563329   0.492718   0.589372   0.515674   \n",
       "14  0.274431   0.271171   0.238003   0.286875   0.297353   \n",
       "15  0.627978   0.577943   0.669014   0.707610   0.524622   \n",
       "16  0.372352   0.469862   0.339788   0.468984   0.406692   \n",
       "17  0.285722   0.291251   0.281118   0.274120   0.276270   \n",
       "18  0.018278   0.012980   0.012379   0.015399   0.016767   \n",
       "19  0.533720   0.489979   0.605776   0.597667   0.542778   \n",
       "20  0.236991   0.227916   0.243122   0.254380   0.258237   \n",
       "21  0.181710   0.163234   0.162545   0.180201   0.187123   \n",
       "22  0.282415   0.295748   0.315086   0.305427   0.317214   \n",
       "23  0.537811   0.465400   0.460561   0.484647   0.548096   \n",
       "24  0.546376   0.474012   0.477323   0.665766   0.476861   \n",
       "25  0.506370   0.516944   0.473982   0.492036   0.495441   \n",
       "26  0.372662   0.341986   0.348146   0.453156   0.339871   \n",
       "27  0.490380   0.484568   0.426515   0.529868   0.519861   \n",
       "28  0.622148   0.547977   0.351711   1.584391   0.453893   \n",
       "29  0.262217   0.304643   0.268987   0.274074   0.281045   \n",
       "30  0.395586   0.439425   0.284733   0.558586   0.432690   \n",
       "31  0.465250   1.414748   0.537893   0.343210   0.860652   \n",
       "32  0.707258   0.619598   0.703199   0.538425   0.567663   \n",
       "33  0.551916   0.520197   0.434151   0.444566   0.493390   \n",
       "34  0.151410   0.118287   0.178617   0.152978   0.150609   \n",
       "35  0.156613   0.129569   0.151631   0.133710   0.153065   \n",
       "36  0.101102   0.114209   0.117398   0.101323   0.096553   \n",
       "\n",
       "    Undefined classification  Low Weight  Normal Weight  Overweight  \\\n",
       "0                   0.296971    0.216395       0.284060    0.236059   \n",
       "1                   0.325780    0.238700       0.251692    0.305938   \n",
       "2                   0.244520    0.699315       0.218977    0.318423   \n",
       "3                   0.776171    1.172093       0.840292    0.814925   \n",
       "4                   0.257893    0.200723       0.219590    0.235670   \n",
       "5                   0.312005    0.954099       0.392959    0.242916   \n",
       "6                   0.618523    0.085870       0.704775    1.232571   \n",
       "7                   0.233475    0.177189       0.227362    0.206401   \n",
       "8                   0.250752    0.246645       0.246105    0.255920   \n",
       "9                   0.460432    0.476691       0.439314    0.414027   \n",
       "10                  0.241954    0.314878       0.326068    0.331490   \n",
       "11                  0.597806    0.981742       0.534537    0.562222   \n",
       "12                  0.476982    0.577168       0.430699    0.355796   \n",
       "13                  0.563045    0.545528       0.493551    0.545997   \n",
       "14                  0.285716    0.329126       0.272804    0.258814   \n",
       "15                  0.612569    0.564057       0.599145    0.655028   \n",
       "16                  0.381816    0.708488       0.339499    0.431342   \n",
       "17                  0.293808    0.252903       0.279803    0.297996   \n",
       "18                  0.015603    0.011590       0.013785    0.014702   \n",
       "19                  0.492026    0.481586       0.584020    0.517669   \n",
       "20                  0.252622    0.285099       0.236137    0.242912   \n",
       "21                  0.184013    0.250974       0.177435    0.169020   \n",
       "22                  0.309477    0.288290       0.298370    0.298371   \n",
       "23                  0.571253    0.509801       0.547527    0.502086   \n",
       "24                  0.639565    0.600192       0.494365    0.494167   \n",
       "25                  0.541774    0.592228       0.488552    0.472735   \n",
       "26                  0.412391    0.437693       0.344135    0.404718   \n",
       "27                  0.508258    0.688640       0.466189    0.449605   \n",
       "28                  0.649609    0.399676       0.401025    0.414118   \n",
       "29                  0.285016    0.277839       0.268097    0.281955   \n",
       "30                  0.444269    0.309291       0.349890    0.319342   \n",
       "31                  0.556770    0.000000       0.546397    0.781567   \n",
       "32                  0.526654    0.194489       0.410644    0.652150   \n",
       "33                  0.474954    0.493808       0.388207    0.433395   \n",
       "34                  0.166715    0.190047       0.165702    0.141938   \n",
       "35                  0.135421    0.237374       0.127653    0.094309   \n",
       "36                  0.105180    0.109046       0.102359    0.101065   \n",
       "\n",
       "    Obesity 1  Obesity 2  Obesity 3  \n",
       "0    0.184009   0.347203   0.266653  \n",
       "1    0.274798   0.318639   0.823624  \n",
       "2    0.207619   0.237748   0.209382  \n",
       "3    0.467807   0.670209   0.729714  \n",
       "4    0.210442   0.309354   0.251667  \n",
       "5    0.335206   0.310083   0.216768  \n",
       "6    0.533449   0.347422   0.000000  \n",
       "7    0.179911   0.192612   0.192658  \n",
       "8    0.251888   0.235695   0.250253  \n",
       "9    0.507958   0.501128   0.573521  \n",
       "10   0.288411   0.299649   0.295149  \n",
       "11   0.453594   0.578586   0.632032  \n",
       "12   0.380486   0.310929   0.473612  \n",
       "13   0.541935   0.537001   0.585495  \n",
       "14   0.250932   0.256350   0.266091  \n",
       "15   0.532059   0.533458   0.600833  \n",
       "16   0.421622   0.363462   0.390225  \n",
       "17   0.287979   0.264226   0.279977  \n",
       "18   0.014115   0.014136   0.021396  \n",
       "19   0.483443   0.647887   0.548680  \n",
       "20   0.215284   0.242149   0.318941  \n",
       "21   0.170466   0.161756   0.222985  \n",
       "22   0.291263   0.298250   0.340271  \n",
       "23   0.421026   0.404838   0.453562  \n",
       "24   0.451934   0.427268   0.554823  \n",
       "25   0.473042   0.346057   0.397762  \n",
       "26   0.375442   0.399016   0.259510  \n",
       "27   0.475635   0.486548   0.469704  \n",
       "28   0.394813   0.438395   0.490332  \n",
       "29   0.248811   0.264923   0.266914  \n",
       "30   0.290006   0.364659   0.331229  \n",
       "31   0.522427   0.000000   0.263956  \n",
       "32   0.412647   1.451607   1.036925  \n",
       "33   0.493662   0.442158   0.554933  \n",
       "34   0.224131   0.157970   0.109519  \n",
       "35   0.130453   0.137707   0.288451  \n",
       "36   1.006256   0.094737   0.101651  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saits_mae_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_code = df_saits_mae_standard.to_latex(index=False)\n",
    "\n",
    "with open(\"tabela.tex\", \"w\") as f:\n",
    "    f.write(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrrrrrrrrrrr}\n",
      "\\toprule\n",
      "0 & General & Female & Male & Undefined Gender & +65 & -65 & ICUType 1 & ICUType 2 & ICUType 3 & ICUType 4 & Undefined classification & Low Weight & Normal Weight & Overweight & Obesity 1 & Obesity 2 & Obesity 3 \\\\\n",
      "\\midrule\n",
      "ALP & 0.355492 & 0.398791 & 0.388989 & 0.000000 & 0.331751 & 0.740879 & 0.222358 & 0.195952 & 0.723086 & 0.505640 & 0.296971 & 0.216395 & 0.284060 & 0.236059 & 0.184009 & 0.347203 & 0.266653 \\\\\n",
      "ALT & 0.346164 & 0.267721 & 0.253539 & 0.000000 & 0.280160 & 0.329866 & 0.296968 & 0.351267 & 0.311489 & 0.272558 & 0.325780 & 0.238700 & 0.251692 & 0.305938 & 0.274798 & 0.318639 & 0.823624 \\\\\n",
      "AST & 0.283204 & 0.257204 & 0.204749 & 0.000000 & 0.206225 & 0.243144 & 0.340113 & 0.273045 & 0.305834 & 0.306141 & 0.244520 & 0.699315 & 0.218977 & 0.318423 & 0.207619 & 0.237748 & 0.209382 \\\\\n",
      "Albumin & 0.586538 & 0.609286 & 0.621456 & 0.000000 & 0.536556 & 0.658661 & 0.490084 & 0.717311 & 0.708582 & 0.754085 & 0.776171 & 1.172093 & 0.840292 & 0.814925 & 0.467807 & 0.670209 & 0.729714 \\\\\n",
      "BUN & 0.235495 & 0.253363 & 0.228878 & 0.000000 & 0.242391 & 0.244288 & 0.213308 & 0.222898 & 0.294829 & 0.209661 & 0.257893 & 0.200723 & 0.219590 & 0.235670 & 0.210442 & 0.309354 & 0.251667 \\\\\n",
      "Bilirubin & 0.292134 & 0.301597 & 0.249420 & 0.000000 & 0.231827 & 0.336409 & 0.200403 & 0.318250 & 0.280385 & 0.267887 & 0.312005 & 0.954099 & 0.392959 & 0.242916 & 0.335206 & 0.310083 & 0.216768 \\\\\n",
      "Cholesterol & 0.861914 & 1.451439 & 0.918617 & 0.000000 & 0.736495 & 1.183795 & 1.002457 & 0.000000 & 0.906296 & 0.725019 & 0.618523 & 0.085870 & 0.704775 & 1.232571 & 0.533449 & 0.347422 & 0.000000 \\\\\n",
      "Creatinine & 0.227236 & 0.218419 & 0.210438 & 0.000000 & 0.200026 & 0.211177 & 0.193449 & 0.183120 & 0.265242 & 0.217446 & 0.233475 & 0.177189 & 0.227362 & 0.206401 & 0.179911 & 0.192612 & 0.192658 \\\\\n",
      "DiasABP & 0.256964 & 0.260941 & 0.240924 & 0.182851 & 0.271154 & 0.231552 & 0.299906 & 0.221175 & 0.250333 & 0.241564 & 0.250752 & 0.246645 & 0.246105 & 0.255920 & 0.251888 & 0.235695 & 0.250253 \\\\\n",
      "FiO2 & 0.484346 & 0.452276 & 0.517671 & 0.000000 & 0.491061 & 0.501327 & 0.482516 & 0.467761 & 0.507122 & 0.438264 & 0.460432 & 0.476691 & 0.439314 & 0.414027 & 0.507958 & 0.501128 & 0.573521 \\\\\n",
      "GCS & 0.277826 & 0.266519 & 0.292769 & 0.136040 & 0.276112 & 0.273780 & 0.264396 & 0.406222 & 0.266837 & 0.245789 & 0.241954 & 0.314878 & 0.326068 & 0.331490 & 0.288411 & 0.299649 & 0.295149 \\\\\n",
      "Glucose & 0.557457 & 0.561478 & 0.590789 & 0.240908 & 0.590401 & 0.619428 & 0.607252 & 0.465918 & 0.685012 & 0.443831 & 0.597806 & 0.981742 & 0.534537 & 0.562222 & 0.453594 & 0.578586 & 0.632032 \\\\\n",
      "HCO3 & 0.422355 & 0.431529 & 0.433305 & 0.214275 & 0.461885 & 0.446012 & 0.410033 & 0.348082 & 0.486334 & 0.399666 & 0.476982 & 0.577168 & 0.430699 & 0.355796 & 0.380486 & 0.310929 & 0.473612 \\\\\n",
      "HCT & 0.547848 & 0.565190 & 0.536183 & 0.000000 & 0.509117 & 0.527232 & 0.563329 & 0.492718 & 0.589372 & 0.515674 & 0.563045 & 0.545528 & 0.493551 & 0.545997 & 0.541935 & 0.537001 & 0.585495 \\\\\n",
      "HR & 0.278652 & 0.284302 & 0.275293 & 0.277222 & 0.280410 & 0.274431 & 0.271171 & 0.238003 & 0.286875 & 0.297353 & 0.285716 & 0.329126 & 0.272804 & 0.258814 & 0.250932 & 0.256350 & 0.266091 \\\\\n",
      "K & 0.617374 & 0.636121 & 0.591587 & 0.648198 & 0.565924 & 0.627978 & 0.577943 & 0.669014 & 0.707610 & 0.524622 & 0.612569 & 0.564057 & 0.599145 & 0.655028 & 0.532059 & 0.533458 & 0.600833 \\\\\n",
      "Lactate & 0.360208 & 0.334491 & 0.406861 & 0.000000 & 0.329401 & 0.372352 & 0.469862 & 0.339788 & 0.468984 & 0.406692 & 0.381816 & 0.708488 & 0.339499 & 0.431342 & 0.421622 & 0.363462 & 0.390225 \\\\\n",
      "MAP & 0.276148 & 0.280798 & 0.295768 & 0.271983 & 0.299645 & 0.285722 & 0.291251 & 0.281118 & 0.274120 & 0.276270 & 0.293808 & 0.252903 & 0.279803 & 0.297996 & 0.287979 & 0.264226 & 0.279977 \\\\\n",
      "MechVent & 0.016132 & 0.014552 & 0.014512 & 0.000000 & 0.012942 & 0.018278 & 0.012980 & 0.012379 & 0.015399 & 0.016767 & 0.015603 & 0.011590 & 0.013785 & 0.014702 & 0.014115 & 0.014136 & 0.021396 \\\\\n",
      "Mg & 0.542887 & 0.555896 & 0.558361 & 0.385353 & 0.539592 & 0.533720 & 0.489979 & 0.605776 & 0.597667 & 0.542778 & 0.492026 & 0.481586 & 0.584020 & 0.517669 & 0.483443 & 0.647887 & 0.548680 \\\\\n",
      "NIDiasABP & 0.238053 & 0.264227 & 0.242982 & 0.220633 & 0.258731 & 0.236991 & 0.227916 & 0.243122 & 0.254380 & 0.258237 & 0.252622 & 0.285099 & 0.236137 & 0.242912 & 0.215284 & 0.242149 & 0.318941 \\\\\n",
      "NIMAP & 0.177311 & 0.189465 & 0.169662 & 0.111674 & 0.177068 & 0.181710 & 0.163234 & 0.162545 & 0.180201 & 0.187123 & 0.184013 & 0.250974 & 0.177435 & 0.169020 & 0.170466 & 0.161756 & 0.222985 \\\\\n",
      "NISysABP & 0.305999 & 0.314224 & 0.313024 & 0.146763 & 0.323843 & 0.282415 & 0.295748 & 0.315086 & 0.305427 & 0.317214 & 0.309477 & 0.288290 & 0.298370 & 0.298371 & 0.291263 & 0.298250 & 0.340271 \\\\\n",
      "Na & 0.509447 & 0.527968 & 0.496132 & 0.000000 & 0.475615 & 0.537811 & 0.465400 & 0.460561 & 0.484647 & 0.548096 & 0.571253 & 0.509801 & 0.547527 & 0.502086 & 0.421026 & 0.404838 & 0.453562 \\\\\n",
      "PaCO2 & 0.544875 & 0.556747 & 0.517135 & 0.000000 & 0.484015 & 0.546376 & 0.474012 & 0.477323 & 0.665766 & 0.476861 & 0.639565 & 0.600192 & 0.494365 & 0.494167 & 0.451934 & 0.427268 & 0.554823 \\\\\n",
      "PaO2 & 0.504418 & 0.499373 & 0.481931 & 0.000000 & 0.452471 & 0.506370 & 0.516944 & 0.473982 & 0.492036 & 0.495441 & 0.541774 & 0.592228 & 0.488552 & 0.472735 & 0.473042 & 0.346057 & 0.397762 \\\\\n",
      "Platelets & 0.379108 & 0.395210 & 0.355038 & 0.000000 & 0.375186 & 0.372662 & 0.341986 & 0.348146 & 0.453156 & 0.339871 & 0.412391 & 0.437693 & 0.344135 & 0.404718 & 0.375442 & 0.399016 & 0.259510 \\\\\n",
      "RespRate & 0.497555 & 0.509812 & 0.492689 & 0.032527 & 0.478573 & 0.490380 & 0.484568 & 0.426515 & 0.529868 & 0.519861 & 0.508258 & 0.688640 & 0.466189 & 0.449605 & 0.475635 & 0.486548 & 0.469704 \\\\\n",
      "SaO2 & 0.395204 & 0.592285 & 0.409306 & 0.000000 & 0.426569 & 0.622148 & 0.547977 & 0.351711 & 1.584391 & 0.453893 & 0.649609 & 0.399676 & 0.401025 & 0.414118 & 0.394813 & 0.438395 & 0.490332 \\\\\n",
      "SysABP & 0.287632 & 0.283227 & 0.285330 & 0.189362 & 0.287563 & 0.262217 & 0.304643 & 0.268987 & 0.274074 & 0.281045 & 0.285016 & 0.277839 & 0.268097 & 0.281955 & 0.248811 & 0.264923 & 0.266914 \\\\\n",
      "Temp & 0.365474 & 0.392193 & 0.389191 & 0.153703 & 0.389892 & 0.395586 & 0.439425 & 0.284733 & 0.558586 & 0.432690 & 0.444269 & 0.309291 & 0.349890 & 0.319342 & 0.290006 & 0.364659 & 0.331229 \\\\\n",
      "TroponinI & 0.574282 & 0.399557 & 0.526789 & 0.000000 & 0.912480 & 0.465250 & 1.414748 & 0.537893 & 0.343210 & 0.860652 & 0.556770 & 0.000000 & 0.546397 & 0.781567 & 0.522427 & 0.000000 & 0.263956 \\\\\n",
      "TroponinT & 0.562503 & 0.612332 & 0.716181 & 0.000000 & 0.560979 & 0.707258 & 0.619598 & 0.703199 & 0.538425 & 0.567663 & 0.526654 & 0.194489 & 0.410644 & 0.652150 & 0.412647 & 1.451607 & 1.036925 \\\\\n",
      "Urine & 0.455580 & 0.422339 & 0.467929 & 0.420597 & 0.403842 & 0.551916 & 0.520197 & 0.434151 & 0.444566 & 0.493390 & 0.474954 & 0.493808 & 0.388207 & 0.433395 & 0.493662 & 0.442158 & 0.554933 \\\\\n",
      "WBC & 0.151106 & 0.147661 & 0.168883 & 0.018873 & 0.168543 & 0.151410 & 0.118287 & 0.178617 & 0.152978 & 0.150609 & 0.166715 & 0.190047 & 0.165702 & 0.141938 & 0.224131 & 0.157970 & 0.109519 \\\\\n",
      "Weight & 0.140954 & 0.146240 & 0.129140 & 0.124492 & 0.124462 & 0.156613 & 0.129569 & 0.151631 & 0.133710 & 0.153065 & 0.135421 & 0.237374 & 0.127653 & 0.094309 & 0.130453 & 0.137707 & 0.288451 \\\\\n",
      "Ph & 0.164482 & 0.102898 & 0.111622 & 0.035273 & 0.100316 & 0.101102 & 0.114209 & 0.117398 & 0.101323 & 0.096553 & 0.105180 & 0.109046 & 0.102359 & 0.101065 & 1.006256 & 0.094737 & 0.101651 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "MechVent\n",
      "0.016131509283529688\n",
      "--------------------\n",
      "Female\n",
      "MechVent\n",
      "0.014552340041130598\n",
      "--------------------\n",
      "Male\n",
      "MechVent\n",
      "0.014512000309648741\n",
      "--------------------\n",
      "Undefined Gender\n",
      "TroponinT\n",
      "0.0\n",
      "--------------------\n",
      "+65\n",
      "MechVent\n",
      "0.012942020909290175\n",
      "--------------------\n",
      "-65\n",
      "MechVent\n",
      "0.01827778097688463\n",
      "--------------------\n",
      "ICUType 1\n",
      "MechVent\n",
      "0.012979781499681835\n",
      "--------------------\n",
      "ICUType 2\n",
      "Cholesterol\n",
      "0.0\n",
      "--------------------\n",
      "ICUType 3\n",
      "MechVent\n",
      "0.015398824789147038\n",
      "--------------------\n",
      "ICUType 4\n",
      "MechVent\n",
      "0.01676655673093299\n",
      "--------------------\n",
      "Undefined classification\n",
      "MechVent\n",
      "0.015602762801210616\n",
      "--------------------\n",
      "Low Weight\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Normal Weight\n",
      "MechVent\n",
      "0.013784697335756175\n",
      "--------------------\n",
      "Overweight\n",
      "MechVent\n",
      "0.014702130836156856\n",
      "--------------------\n",
      "Obesity 1\n",
      "MechVent\n",
      "0.014115385650734221\n",
      "--------------------\n",
      "Obesity 2\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Obesity 3\n",
      "Cholesterol\n",
      "0.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.min_value_in_subgroup(df_saits_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "Cholesterol\n",
      "0.8619142944054452\n",
      "--------------------\n",
      "Female\n",
      "Cholesterol\n",
      "1.451438736858023\n",
      "--------------------\n",
      "Male\n",
      "Cholesterol\n",
      "0.9186172160150028\n",
      "--------------------\n",
      "Undefined Gender\n",
      "K\n",
      "0.6481980955579346\n",
      "--------------------\n",
      "+65\n",
      "TroponinI\n",
      "0.912479961059112\n",
      "--------------------\n",
      "-65\n",
      "Cholesterol\n",
      "1.1837952493687076\n",
      "--------------------\n",
      "ICUType 1\n",
      "TroponinI\n",
      "1.4147481358654532\n",
      "--------------------\n",
      "ICUType 2\n",
      "Albumin\n",
      "0.7173112394561357\n",
      "--------------------\n",
      "ICUType 3\n",
      "SaO2\n",
      "1.5843908865861114\n",
      "--------------------\n",
      "ICUType 4\n",
      "TroponinI\n",
      "0.8606517177368178\n",
      "--------------------\n",
      "Undefined classification\n",
      "Albumin\n",
      "0.7761709655447225\n",
      "--------------------\n",
      "Low Weight\n",
      "Albumin\n",
      "1.1720931746200332\n",
      "--------------------\n",
      "Normal Weight\n",
      "Albumin\n",
      "0.8402915751610249\n",
      "--------------------\n",
      "Overweight\n",
      "Cholesterol\n",
      "1.2325708781179912\n",
      "--------------------\n",
      "Obesity 1\n",
      "Ph\n",
      "1.006255796754588\n",
      "--------------------\n",
      "Obesity 2\n",
      "TroponinT\n",
      "1.4516069413777122\n",
      "--------------------\n",
      "Obesity 3\n",
      "TroponinT\n",
      "1.03692539683769\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.max_value_in_subgroup(df_saits_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Standard Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAITS - MAE\n",
      "************\n",
      "General\n",
      "-------------\n",
      "ALP : 46.05178056160585\n",
      "ALT : 406.2112769981204\n",
      "AST : 496.992055665294\n",
      "Albumin : 0.3851788459064264\n",
      "BUN : 5.380332453297799\n",
      "Bilirubin : 1.6736765518848495\n",
      "Cholesterol : 38.400731277463905\n",
      "Creatinine : 0.3503998682295476\n",
      "DiasABP : 3.286241163141612\n",
      "FiO2 : 0.09104833207391178\n",
      "GCS : 1.1115600990723302\n",
      "Glucose : 35.67556060432917\n",
      "HCO3 : 1.9973297300677588\n",
      "HCT : 2.7491760875685074\n",
      "HR : 4.954845594432205\n",
      "K : 0.4188106400472619\n",
      "Lactate : 0.9578556416874965\n",
      "MAP : 4.378979286697319\n",
      "MechVent : 0.01613150967851928\n",
      "Mg : 0.2635250708242739\n",
      "NIDiasABP : 3.5702379063267773\n",
      "NIMAP : 2.6413458948607875\n",
      "NISysABP : 6.786345809909647\n",
      "Na : 2.6723842429156246\n",
      "PaCO2 : 4.967970567426776\n",
      "PaO2 : 43.05042321078405\n",
      "Platelets : 40.793265397332874\n",
      "RespRate : 2.670931693010336\n",
      "SaO2 : 1.3594024396249635\n",
      "SysABP : 6.740216718958554\n",
      "Temp : 0.466368066859829\n",
      "TroponinI : 6.362012470364254\n",
      "TroponinT : 1.6937565297072215\n",
      "Urine : 71.26128697044311\n",
      "WBC : 6.241771727095261\n",
      "Weight : 3.5664098735233485\n",
      "Ph : 0.18469525605924028\n",
      "Female\n",
      "-------------\n",
      "ALP : 51.66096512258799\n",
      "ALT : 314.16135726176077\n",
      "AST : 451.3646475829517\n",
      "Albumin : 0.4001175714575669\n",
      "BUN : 5.788564088250268\n",
      "Bilirubin : 1.7278912619557247\n",
      "Cholesterol : 64.66571960447925\n",
      "Creatinine : 0.33680434505059753\n",
      "DiasABP : 3.3371077214478366\n",
      "FiO2 : 0.08501981755176852\n",
      "GCS : 1.066320071636215\n",
      "Glucose : 35.93291002638785\n",
      "HCO3 : 2.0407128984881586\n",
      "HCT : 2.8362027441808366\n",
      "HR : 5.055317625173118\n",
      "K : 0.43152827071147426\n",
      "Lactate : 0.889467970314867\n",
      "MAP : 4.452708960049217\n",
      "MechVent : 0.01455234083128562\n",
      "Mg : 0.2698400849542428\n",
      "NIDiasABP : 3.9627790889930186\n",
      "NIMAP : 2.8223901657898494\n",
      "NISysABP : 6.968756677792292\n",
      "Na : 2.76954235839843\n",
      "PaCO2 : 5.076210057566668\n",
      "PaO2 : 42.6199125332455\n",
      "Platelets : 42.52599482447767\n",
      "RespRate : 2.736726413277237\n",
      "SaO2 : 2.037314854311128\n",
      "SysABP : 6.637011280306397\n",
      "Temp : 0.5004634565556003\n",
      "TroponinI : 4.426368298530137\n",
      "TroponinT : 1.8437977938642127\n",
      "Urine : 66.06174144135662\n",
      "WBC : 6.099460337407328\n",
      "Weight : 3.7001598949352306\n",
      "Ph : 0.11554354483026787\n",
      "Male\n",
      "-------------\n",
      "ALP : 50.39110101593819\n",
      "ALT : 297.5188237667059\n",
      "AST : 359.3123356241838\n",
      "Albumin : 0.40810927588764717\n",
      "BUN : 5.229164365219731\n",
      "Bilirubin : 1.4289606822573466\n",
      "Cholesterol : 40.92700723501037\n",
      "Creatinine : 0.3244964992088713\n",
      "DiasABP : 3.081118774127645\n",
      "FiO2 : 0.09731282861502656\n",
      "GCS : 1.1713475901473689\n",
      "Glucose : 37.80874896165792\n",
      "HCO3 : 2.0491132717132525\n",
      "HCT : 2.6906403657207294\n",
      "HR : 4.895125595320714\n",
      "K : 0.40131754988715684\n",
      "Lactate : 1.0819140754717325\n",
      "MAP : 4.69010174759743\n",
      "MechVent : 0.01451199920706472\n",
      "Mg : 0.27103637156980426\n",
      "NIDiasABP : 3.6441475552862346\n",
      "NIMAP : 2.5274039615328903\n",
      "NISysABP : 6.94214598245344\n",
      "Na : 2.602539251943199\n",
      "PaCO2 : 4.71504163012212\n",
      "PaO2 : 41.131271870458235\n",
      "Platelets : 38.203295539523474\n",
      "RespRate : 2.6448094045422725\n",
      "SaO2 : 1.4079095573835427\n",
      "SysABP : 6.686288937537448\n",
      "Temp : 0.49663251113300555\n",
      "TroponinI : 5.8358767032613565\n",
      "TroponinT : 2.1564997472888265\n",
      "Urine : 73.19283149591294\n",
      "WBC : 6.976083777780721\n",
      "Weight : 3.2674972600489025\n",
      "Ph : 0.12533992836007382\n",
      "Undefined Gender\n",
      "-------------\n",
      "ALP : 0.0\n",
      "ALT : 0.0\n",
      "AST : 0.0\n",
      "Albumin : 0.0\n",
      "BUN : 0.0\n",
      "Bilirubin : 0.0\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.0\n",
      "DiasABP : 2.3384372393287283\n",
      "FiO2 : 0.0\n",
      "GCS : 0.5442860126494\n",
      "Glucose : 15.417404174789269\n",
      "HCO3 : 1.0133123397822081\n",
      "HCT : 0.0\n",
      "HR : 4.929430007933954\n",
      "K : 0.4397212028499017\n",
      "Lactate : 0.0\n",
      "MAP : 4.312929153441305\n",
      "MechVent : 0.0\n",
      "Mg : 0.18705601692180995\n",
      "NIDiasABP : 3.308967590331204\n",
      "NIMAP : 1.6635646565749669\n",
      "NISysABP : 3.2548637390120443\n",
      "Na : 0.0\n",
      "PaCO2 : 0.0\n",
      "PaO2 : 0.0\n",
      "Platelets : 0.0\n",
      "RespRate : 0.1746082305906457\n",
      "SaO2 : 0.0\n",
      "SysABP : 4.437422116596939\n",
      "Temp : 0.1961349487303728\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.0\n",
      "Urine : 65.78923797600842\n",
      "WBC : 0.7795959472652357\n",
      "Weight : 3.149891662597133\n",
      "Ph : 0.03960796356197207\n",
      "+65\n",
      "-------------\n",
      "ALP : 42.976265907287\n",
      "ALT : 328.7574866577783\n",
      "AST : 361.9022886474364\n",
      "Albumin : 0.3523559426608103\n",
      "BUN : 5.537895235522026\n",
      "Bilirubin : 1.3281681107605348\n",
      "Cholesterol : 32.81292915343965\n",
      "Creatinine : 0.3084420324607758\n",
      "DiasABP : 3.467723413940888\n",
      "FiO2 : 0.09231071173688223\n",
      "GCS : 1.1047040640852828\n",
      "Glucose : 37.783915898571166\n",
      "HCO3 : 2.184270097704351\n",
      "HCT : 2.554819262186682\n",
      "HR : 4.986114600287188\n",
      "K : 0.38390818814762206\n",
      "Lactate : 0.875933847533801\n",
      "MAP : 4.751581840602852\n",
      "MechVent : 0.012942019615087409\n",
      "Mg : 0.26192556133359124\n",
      "NIDiasABP : 3.8803488370452275\n",
      "NIMAP : 2.637724553222784\n",
      "NISysABP : 7.182069988507984\n",
      "Na : 2.4949168578243865\n",
      "PaCO2 : 4.41307026980121\n",
      "PaO2 : 38.616914317993285\n",
      "Platelets : 40.371299396143606\n",
      "RespRate : 2.569035206248215\n",
      "SaO2 : 1.4672900053954157\n",
      "SysABP : 6.738620775816521\n",
      "Temp : 0.4975272756609809\n",
      "TroponinI : 10.108636496816695\n",
      "TroponinT : 1.6891689444529159\n",
      "Urine : 63.16842527587327\n",
      "WBC : 6.962047923988748\n",
      "Weight : 3.149132052334872\n",
      "Ph : 0.11264412938421788\n",
      "-65\n",
      "-------------\n",
      "ALP : 95.97627978551864\n",
      "ALT : 387.0863286061045\n",
      "AST : 426.6912528872452\n",
      "Albumin : 0.4325417479346729\n",
      "BUN : 5.581243542388619\n",
      "Bilirubin : 1.9273336317429177\n",
      "Cholesterol : 52.74143981930956\n",
      "Creatinine : 0.3256360618014758\n",
      "DiasABP : 2.9612583808197432\n",
      "FiO2 : 0.0942405249968858\n",
      "GCS : 1.0953734252295624\n",
      "Glucose : 39.64157242414871\n",
      "HCO3 : 2.1092031887599347\n",
      "HCT : 2.645721973478789\n",
      "HR : 4.879795803116856\n",
      "K : 0.4260041787889257\n",
      "Lactate : 0.9901478323936417\n",
      "MAP : 4.530798021741367\n",
      "MechVent : 0.01827777949621049\n",
      "Mg : 0.2590755844348242\n",
      "NIDiasABP : 3.5542983582307426\n",
      "NIMAP : 2.706873300405319\n",
      "NISysABP : 6.26331046331776\n",
      "Na : 2.8211743925239903\n",
      "PaCO2 : 4.981651285126445\n",
      "PaO2 : 43.2170577051652\n",
      "Platelets : 40.09969155115015\n",
      "RespRate : 2.632417129740781\n",
      "SaO2 : 2.1400352556680784\n",
      "SysABP : 6.144653781612791\n",
      "Temp : 0.5047930774388528\n",
      "TroponinI : 5.154131215810132\n",
      "TroponinT : 2.129631582317532\n",
      "Urine : 86.33001497469883\n",
      "WBC : 6.254304017110732\n",
      "Weight : 3.9626179418353606\n",
      "Ph : 0.11352647697639547\n",
      "ICUType 1\n",
      "-------------\n",
      "ALP : 28.80510221827983\n",
      "ALT : 348.4814086913889\n",
      "AST : 596.8619574737311\n",
      "Albumin : 0.3218379282951195\n",
      "BUN : 4.873432011567307\n",
      "Bilirubin : 1.1481393058299558\n",
      "Cholesterol : 44.66229071983581\n",
      "Creatinine : 0.29830057631938556\n",
      "DiasABP : 3.835419332075771\n",
      "FiO2 : 0.09070434031213107\n",
      "GCS : 1.0578275761514315\n",
      "Glucose : 38.86233806610077\n",
      "HCO3 : 1.9390585570208623\n",
      "HCT : 2.826863423100203\n",
      "HR : 4.821826086187305\n",
      "K : 0.3920617413520785\n",
      "Lactate : 1.2494451138280651\n",
      "MAP : 4.618476856540434\n",
      "MechVent : 0.01297977845445802\n",
      "Mg : 0.23784265266201168\n",
      "NIDiasABP : 3.4181964004477745\n",
      "NIMAP : 2.431636214552369\n",
      "NISysABP : 6.5590040707504516\n",
      "Na : 2.4413327636718556\n",
      "PaCO2 : 4.321863279567895\n",
      "PaO2 : 44.11948643308664\n",
      "Platelets : 36.79881926714333\n",
      "RespRate : 2.6012137658322296\n",
      "SaO2 : 1.8849040164222963\n",
      "SysABP : 7.138861364199781\n",
      "Temp : 0.5607339926074482\n",
      "TroponinI : 15.672864359615275\n",
      "TroponinT : 1.8656773567199156\n",
      "Urine : 81.36862636589443\n",
      "WBC : 4.886106611544392\n",
      "Weight : 3.278339916103877\n",
      "Ph : 0.12824460029601947\n",
      "ICUType 2\n",
      "-------------\n",
      "ALP : 25.38439052581686\n",
      "ALT : 412.19935768526824\n",
      "AST : 479.1650450333097\n",
      "Albumin : 0.4710576261792191\n",
      "BUN : 5.092528218236428\n",
      "Bilirubin : 1.8232998125253972\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.2823729615012786\n",
      "DiasABP : 2.8285571352938566\n",
      "FiO2 : 0.08793066475873944\n",
      "GCS : 1.6252640888812577\n",
      "Glucose : 29.81737903040673\n",
      "HCO3 : 1.6460921542244566\n",
      "HCT : 2.472529186672625\n",
      "HR : 4.232061443518225\n",
      "K : 0.45384214276172713\n",
      "Lactate : 0.9035550075310829\n",
      "MAP : 4.45779238544505\n",
      "MechVent : 0.012379252939903735\n",
      "Mg : 0.29405228463378913\n",
      "NIDiasABP : 3.646250816576401\n",
      "NIMAP : 2.4213830825829414\n",
      "NISysABP : 6.987859919465402\n",
      "Na : 2.415946739307331\n",
      "PaCO2 : 4.352052779984253\n",
      "PaO2 : 40.452805034602704\n",
      "Platelets : 37.46167339345115\n",
      "RespRate : 2.2895783079621617\n",
      "SaO2 : 1.2097994766983284\n",
      "SysABP : 6.303301727312764\n",
      "Temp : 0.36333748102651153\n",
      "TroponinI : 5.958890848158598\n",
      "TroponinT : 2.1174076032636195\n",
      "Urine : 67.90931297991905\n",
      "WBC : 7.378177407904238\n",
      "Weight : 3.836552319254494\n",
      "Ph : 0.13182476195434206\n",
      "ICUType 3\n",
      "-------------\n",
      "ALP : 93.67135052541994\n",
      "ALT : 365.52148441884293\n",
      "AST : 536.7061729221494\n",
      "Albumin : 0.4653248725398819\n",
      "BUN : 6.735940424601213\n",
      "Bilirubin : 1.6063643666255376\n",
      "Cholesterol : 40.37807846068326\n",
      "Creatinine : 0.40900538263391883\n",
      "DiasABP : 3.2014454458928094\n",
      "FiO2 : 0.09532982967828904\n",
      "GCS : 1.0675935987848348\n",
      "Glucose : 43.838713221725264\n",
      "HCO3 : 2.299889882405592\n",
      "HCT : 2.957551867341334\n",
      "HR : 5.101063610451106\n",
      "K : 0.48002469691824257\n",
      "Lactate : 1.2471091088338822\n",
      "MAP : 4.3468250172171805\n",
      "MechVent : 0.015398822799574146\n",
      "Mg : 0.2901163232823203\n",
      "NIDiasABP : 3.8151040660594657\n",
      "NIMAP : 2.6843915757430397\n",
      "NISysABP : 6.773642897159039\n",
      "Na : 2.5422922935858527\n",
      "PaCO2 : 6.07020888457425\n",
      "PaO2 : 41.99370483581165\n",
      "Platelets : 48.761127578628475\n",
      "RespRate : 2.8443934504300863\n",
      "SaO2 : 5.449910583495985\n",
      "SysABP : 6.422521423682301\n",
      "Temp : 0.7127910964319795\n",
      "TroponinI : 3.8021482706065726\n",
      "TroponinT : 1.6212570404197937\n",
      "Urine : 69.53847547142071\n",
      "WBC : 6.31907870779\n",
      "Weight : 3.3831192694390246\n",
      "Ph : 0.11377474598630612\n",
      "ICUType 4\n",
      "-------------\n",
      "ALP : 65.50252282980618\n",
      "ALT : 319.8372587610411\n",
      "AST : 537.2446980476282\n",
      "Albumin : 0.4952068552057688\n",
      "BUN : 4.7901061594995475\n",
      "Bilirubin : 1.534761647569608\n",
      "Cholesterol : 32.301643371576645\n",
      "Creatinine : 0.33530397217162194\n",
      "DiasABP : 3.0893029952969076\n",
      "FiO2 : 0.08238571232515011\n",
      "GCS : 0.9833831299919424\n",
      "Glucose : 28.40384942799916\n",
      "HCO3 : 1.890031798203096\n",
      "HCT : 2.5877233300505313\n",
      "HR : 5.287386469106963\n",
      "K : 0.3558900977884007\n",
      "Lactate : 1.0814650762845974\n",
      "MAP : 4.380907739323294\n",
      "MechVent : 0.016766555871361\n",
      "Mg : 0.26347241144180195\n",
      "NIDiasABP : 3.8729472926486275\n",
      "NIMAP : 2.787510014620271\n",
      "NISysABP : 7.035062129527314\n",
      "Na : 2.8751240410048036\n",
      "PaCO2 : 4.347839797473964\n",
      "PaO2 : 42.284284185347296\n",
      "Platelets : 36.571257913919176\n",
      "RespRate : 2.790671415550697\n",
      "SaO2 : 1.5612806289914807\n",
      "SysABP : 6.5858676500100675\n",
      "Temp : 0.5521394531323801\n",
      "TroponinI : 9.53447285890341\n",
      "TroponinT : 1.709294360975353\n",
      "Urine : 77.17551767279978\n",
      "WBC : 6.221232049514825\n",
      "Weight : 3.8728339041935946\n",
      "Ph : 0.10841840303169083\n",
      "Undefined classification\n",
      "-------------\n",
      "ALP : 38.47078970576897\n",
      "ALT : 382.29107576683623\n",
      "AST : 429.10566036324303\n",
      "Albumin : 0.5097107227031935\n",
      "BUN : 5.892069691703418\n",
      "Bilirubin : 1.7875226760429412\n",
      "Cholesterol : 27.556950887041207\n",
      "Creatinine : 0.36002049715525875\n",
      "DiasABP : 3.2068054010242033\n",
      "FiO2 : 0.08655294954483134\n",
      "GCS : 0.9680396234963077\n",
      "Glucose : 38.25779208176418\n",
      "HCO3 : 2.2556630449341966\n",
      "HCT : 2.825439974962368\n",
      "HR : 5.080464644960053\n",
      "K : 0.4155512719353031\n",
      "Lactate : 1.0153133819472573\n",
      "MAP : 4.659014347281974\n",
      "MechVent : 0.015602763073054438\n",
      "Mg : 0.2388366613632587\n",
      "NIDiasABP : 3.788726407956317\n",
      "NIMAP : 2.7411737097302624\n",
      "NISysABP : 6.86347512032869\n",
      "Na : 2.996600752895311\n",
      "PaCO2 : 5.8313165346011555\n",
      "PaO2 : 46.23868213056694\n",
      "Platelets : 44.37470881485096\n",
      "RespRate : 2.7283888829006897\n",
      "SaO2 : 2.2344934868089745\n",
      "SysABP : 6.678919613333208\n",
      "Temp : 0.5669153381653383\n",
      "TroponinI : 6.168007691700225\n",
      "TroponinT : 1.585811419568628\n",
      "Urine : 74.29172782042843\n",
      "WBC : 6.886530875494021\n",
      "Weight : 3.426405583915603\n",
      "Ph : 0.11810573725980841\n",
      "Low Weight\n",
      "-------------\n",
      "ALP : 28.032644271836567\n",
      "ALT : 280.1059371947682\n",
      "AST : 1227.220855712584\n",
      "Albumin : 0.7697125116982767\n",
      "BUN : 4.585910320281753\n",
      "Bilirubin : 5.4661681628216305\n",
      "Cholesterol : 3.8257446289024237\n",
      "Creatinine : 0.2732267882142553\n",
      "DiasABP : 3.1542830407469853\n",
      "FiO2 : 0.08960932929341167\n",
      "GCS : 1.2598022073507114\n",
      "Glucose : 62.828564519464834\n",
      "HCO3 : 2.7294473648069015\n",
      "HCT : 2.7375357151030357\n",
      "HR : 5.852352612370918\n",
      "K : 0.38264192498246885\n",
      "Lactate : 1.883991222381403\n",
      "MAP : 4.010373960506318\n",
      "MechVent : 0.011589542511970393\n",
      "Mg : 0.23376860141751815\n",
      "NIDiasABP : 4.275811119432743\n",
      "NIMAP : 3.738669792274328\n",
      "NISysABP : 6.393593126139561\n",
      "Na : 2.6742431640622324\n",
      "PaCO2 : 5.472329588497225\n",
      "PaO2 : 50.54477410567301\n",
      "Platelets : 47.09731778231106\n",
      "RespRate : 3.6966971442811207\n",
      "SaO2 : 1.374786921909779\n",
      "SysABP : 6.5107422091436264\n",
      "Temp : 0.39467467972726084\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.5856282329553469\n",
      "Urine : 77.24078870299627\n",
      "WBC : 7.850301190522398\n",
      "Weight : 6.006028230311456\n",
      "Ph : 0.12244664448063251\n",
      "Normal Weight\n",
      "-------------\n",
      "ALP : 36.79828274634458\n",
      "ALT : 295.35217630294125\n",
      "AST : 384.2799291610597\n",
      "Albumin : 0.5518186631955309\n",
      "BUN : 5.016969056438163\n",
      "Bilirubin : 2.2513187786486184\n",
      "Cholesterol : 31.399719238265547\n",
      "Creatinine : 0.35059412121510175\n",
      "DiasABP : 3.1473687536098898\n",
      "FiO2 : 0.08258322905769279\n",
      "GCS : 1.3045712906017615\n",
      "Glucose : 34.20876362698109\n",
      "HCO3 : 2.0367908694527213\n",
      "HCT : 2.47670673005123\n",
      "HR : 4.850858771037831\n",
      "K : 0.40644470953171924\n",
      "Lactate : 0.9027872596222534\n",
      "MAP : 4.436942632291324\n",
      "MechVent : 0.01378469708507042\n",
      "Mg : 0.2834916990946926\n",
      "NIDiasABP : 3.5414967292517217\n",
      "NIMAP : 2.6431825018107147\n",
      "NISysABP : 6.617142326096914\n",
      "Na : 2.872143554687479\n",
      "PaCO2 : 4.507437592073496\n",
      "PaO2 : 41.69636711633083\n",
      "Platelets : 37.030077049200216\n",
      "RespRate : 2.5025555358550458\n",
      "SaO2 : 1.3794267360980563\n",
      "SysABP : 6.282461802488352\n",
      "Temp : 0.44648260829268355\n",
      "TroponinI : 6.05310086905889\n",
      "TroponinT : 1.2364933426588895\n",
      "Urine : 60.722855756580294\n",
      "WBC : 6.844688283657789\n",
      "Weight : 3.2298661726273004\n",
      "Ph : 0.11493820333156417\n",
      "Overweight\n",
      "-------------\n",
      "ALP : 30.579933732747076\n",
      "ALT : 359.0071884261137\n",
      "AST : 558.7985157739283\n",
      "Albumin : 0.5351605465537342\n",
      "BUN : 5.384335879575082\n",
      "Bilirubin : 1.3916990672548222\n",
      "Cholesterol : 54.91453552245309\n",
      "Creatinine : 0.31827274845423364\n",
      "DiasABP : 3.2729002315636357\n",
      "FiO2 : 0.07782961667720864\n",
      "GCS : 1.3262643233168525\n",
      "Glucose : 35.980537977374674\n",
      "HCO3 : 1.6825710235103377\n",
      "HCT : 2.7398912608066324\n",
      "HR : 4.6021114382905175\n",
      "K : 0.44435432651849843\n",
      "Lactate : 1.1470115094683757\n",
      "MAP : 4.72543516924305\n",
      "MechVent : 0.01470213082798736\n",
      "Mg : 0.2512837775103678\n",
      "NIDiasABP : 3.6431027845708535\n",
      "NIMAP : 2.51783553714659\n",
      "NISysABP : 6.617165854008634\n",
      "Na : 2.6337741216023582\n",
      "PaCO2 : 4.50563347721\n",
      "PaO2 : 40.34638167135945\n",
      "Platelets : 43.549061575053386\n",
      "RespRate : 2.4135312872784715\n",
      "SaO2 : 1.4244625061515637\n",
      "SysABP : 6.607206299584039\n",
      "Temp : 0.4075013666501968\n",
      "TroponinI : 8.658358027933295\n",
      "TroponinT : 1.9636939786029184\n",
      "Urine : 67.79108807770332\n",
      "WBC : 5.8630435616506995\n",
      "Weight : 2.3861971152103747\n",
      "Ph : 0.11348477596328337\n",
      "Obesity 1\n",
      "-------------\n",
      "ALP : 23.837228775022712\n",
      "ALT : 322.46549224851907\n",
      "AST : 364.3485252380189\n",
      "Albumin : 0.30720858176546717\n",
      "BUN : 4.807964770793851\n",
      "Bilirubin : 1.920442553361129\n",
      "Cholesterol : 23.766652425122285\n",
      "Creatinine : 0.2774236132701198\n",
      "DiasABP : 3.221336068591681\n",
      "FiO2 : 0.09548698082716729\n",
      "GCS : 1.1539098351688668\n",
      "Glucose : 29.028685452644776\n",
      "HCO3 : 1.7993300617604757\n",
      "HCT : 2.719506032126266\n",
      "HR : 4.4619465227106465\n",
      "K : 0.3609349336968803\n",
      "Lactate : 1.121164606942046\n",
      "MAP : 4.5665926463909905\n",
      "MechVent : 0.014115385917010151\n",
      "Mg : 0.2346702548435723\n",
      "NIDiasABP : 3.2287443599553938\n",
      "NIMAP : 2.5393682273390517\n",
      "NISysABP : 6.459523659564058\n",
      "Na : 2.2085611820220605\n",
      "PaCO2 : 4.12056379607227\n",
      "PaO2 : 40.37265600160087\n",
      "Platelets : 40.39886775769632\n",
      "RespRate : 2.553265241236452\n",
      "SaO2 : 1.358057741258943\n",
      "SysABP : 5.830505141120157\n",
      "Temp : 0.37006682033592997\n",
      "TroponinI : 5.787554240223852\n",
      "TroponinT : 1.2425258558445582\n",
      "Urine : 77.21800006146067\n",
      "WBC : 9.258224974694647\n",
      "Weight : 3.300714970859906\n",
      "Ph : 1.1299163582515832\n",
      "Obesity 2\n",
      "-------------\n",
      "ALP : 44.97806888155967\n",
      "ALT : 373.9118107386464\n",
      "AST : 417.2213083901604\n",
      "Albumin : 0.4401253382364174\n",
      "BUN : 7.067805782440942\n",
      "Bilirubin : 1.7765093048411622\n",
      "Cholesterol : 15.478622436507958\n",
      "Creatinine : 0.297009190718323\n",
      "DiasABP : 3.014249161849573\n",
      "FiO2 : 0.09420302315004778\n",
      "GCS : 1.198872768878927\n",
      "Glucose : 37.02777173442106\n",
      "HCO3 : 1.4703910827635984\n",
      "HCT : 2.6947443209195683\n",
      "HR : 4.558285544432656\n",
      "K : 0.3618845829596747\n",
      "Lactate : 0.966506917135987\n",
      "MAP : 4.189925570044996\n",
      "MechVent : 0.014135570799717905\n",
      "Mg : 0.3144939016412689\n",
      "NIDiasABP : 3.631662831161934\n",
      "NIMAP : 2.4096254259259764\n",
      "NISysABP : 6.614484902584142\n",
      "Na : 2.123645666873752\n",
      "PaCO2 : 3.8956738398624706\n",
      "PaO2 : 29.534833115797323\n",
      "Platelets : 42.935500805192916\n",
      "RespRate : 2.6118436076423746\n",
      "SaO2 : 1.5079718695745692\n",
      "SysABP : 6.208068965148901\n",
      "Temp : 0.4653277073705541\n",
      "TroponinI : 0.0\n",
      "TroponinT : 4.370945342380337\n",
      "Urine : 69.16182996500383\n",
      "WBC : 6.525281016095782\n",
      "Weight : 3.4842566780421853\n",
      "Ph : 0.1063791053635715\n",
      "Obesity 3\n",
      "-------------\n",
      "ALP : 34.54320214011138\n",
      "ALT : 966.4939270016309\n",
      "AST : 367.4427413939511\n",
      "Albumin : 0.4792022895812029\n",
      "BUN : 5.749823434012003\n",
      "Bilirubin : 1.2418936332064785\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.297079869527961\n",
      "DiasABP : 3.2004186539422794\n",
      "FiO2 : 0.10781168612368258\n",
      "GCS : 1.1808664994161542\n",
      "Glucose : 40.44816746431119\n",
      "HCO3 : 2.2397280861349005\n",
      "HCT : 2.9380933435951073\n",
      "HR : 4.731496658242615\n",
      "K : 0.40759009520211453\n",
      "Lactate : 1.037674771745971\n",
      "MAP : 4.439700884997249\n",
      "MechVent : 0.02139570124178022\n",
      "Mg : 0.2663369444438314\n",
      "NIDiasABP : 4.783360535059183\n",
      "NIMAP : 3.321727289491924\n",
      "NISysABP : 7.546407262885554\n",
      "Na : 2.3792339822519346\n",
      "PaCO2 : 5.05867512281545\n",
      "PaO2 : 33.94774004761162\n",
      "Platelets : 27.924108982085308\n",
      "RespRate : 2.5214254591199157\n",
      "SaO2 : 1.6866217719183405\n",
      "SysABP : 6.254736462069271\n",
      "Temp : 0.42266965802206197\n",
      "TroponinI : 2.9241609573335015\n",
      "TroponinT : 3.1222943325036487\n",
      "Urine : 86.80190595817005\n",
      "WBC : 4.523911418336673\n",
      "Weight : 7.298355828012707\n",
      "Ph : 0.11414275540245747\n"
     ]
    }
   ],
   "source": [
    "print(\"SAITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_saits_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saits_mae_standard_ori = toolkits.create_table(testing_mae_saits_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>General</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Undefined Gender</th>\n",
       "      <th>+65</th>\n",
       "      <th>-65</th>\n",
       "      <th>ICUType 1</th>\n",
       "      <th>ICUType 2</th>\n",
       "      <th>ICUType 3</th>\n",
       "      <th>ICUType 4</th>\n",
       "      <th>Undefined classification</th>\n",
       "      <th>Low Weight</th>\n",
       "      <th>Normal Weight</th>\n",
       "      <th>Overweight</th>\n",
       "      <th>Obesity 1</th>\n",
       "      <th>Obesity 2</th>\n",
       "      <th>Obesity 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALP</td>\n",
       "      <td>46.051781</td>\n",
       "      <td>51.660965</td>\n",
       "      <td>50.391101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.976266</td>\n",
       "      <td>95.976280</td>\n",
       "      <td>28.805102</td>\n",
       "      <td>25.384391</td>\n",
       "      <td>93.671351</td>\n",
       "      <td>65.502523</td>\n",
       "      <td>38.470790</td>\n",
       "      <td>28.032644</td>\n",
       "      <td>36.798283</td>\n",
       "      <td>30.579934</td>\n",
       "      <td>23.837229</td>\n",
       "      <td>44.978069</td>\n",
       "      <td>34.543202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALT</td>\n",
       "      <td>406.211277</td>\n",
       "      <td>314.161357</td>\n",
       "      <td>297.518824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>328.757487</td>\n",
       "      <td>387.086329</td>\n",
       "      <td>348.481409</td>\n",
       "      <td>412.199358</td>\n",
       "      <td>365.521484</td>\n",
       "      <td>319.837259</td>\n",
       "      <td>382.291076</td>\n",
       "      <td>280.105937</td>\n",
       "      <td>295.352176</td>\n",
       "      <td>359.007188</td>\n",
       "      <td>322.465492</td>\n",
       "      <td>373.911811</td>\n",
       "      <td>966.493927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AST</td>\n",
       "      <td>496.992056</td>\n",
       "      <td>451.364648</td>\n",
       "      <td>359.312336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>361.902289</td>\n",
       "      <td>426.691253</td>\n",
       "      <td>596.861957</td>\n",
       "      <td>479.165045</td>\n",
       "      <td>536.706173</td>\n",
       "      <td>537.244698</td>\n",
       "      <td>429.105660</td>\n",
       "      <td>1227.220856</td>\n",
       "      <td>384.279929</td>\n",
       "      <td>558.798516</td>\n",
       "      <td>364.348525</td>\n",
       "      <td>417.221308</td>\n",
       "      <td>367.442741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albumin</td>\n",
       "      <td>0.385179</td>\n",
       "      <td>0.400118</td>\n",
       "      <td>0.408109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352356</td>\n",
       "      <td>0.432542</td>\n",
       "      <td>0.321838</td>\n",
       "      <td>0.471058</td>\n",
       "      <td>0.465325</td>\n",
       "      <td>0.495207</td>\n",
       "      <td>0.509711</td>\n",
       "      <td>0.769713</td>\n",
       "      <td>0.551819</td>\n",
       "      <td>0.535161</td>\n",
       "      <td>0.307209</td>\n",
       "      <td>0.440125</td>\n",
       "      <td>0.479202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUN</td>\n",
       "      <td>5.380332</td>\n",
       "      <td>5.788564</td>\n",
       "      <td>5.229164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.537895</td>\n",
       "      <td>5.581244</td>\n",
       "      <td>4.873432</td>\n",
       "      <td>5.092528</td>\n",
       "      <td>6.735940</td>\n",
       "      <td>4.790106</td>\n",
       "      <td>5.892070</td>\n",
       "      <td>4.585910</td>\n",
       "      <td>5.016969</td>\n",
       "      <td>5.384336</td>\n",
       "      <td>4.807965</td>\n",
       "      <td>7.067806</td>\n",
       "      <td>5.749823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bilirubin</td>\n",
       "      <td>1.673677</td>\n",
       "      <td>1.727891</td>\n",
       "      <td>1.428961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.328168</td>\n",
       "      <td>1.927334</td>\n",
       "      <td>1.148139</td>\n",
       "      <td>1.823300</td>\n",
       "      <td>1.606364</td>\n",
       "      <td>1.534762</td>\n",
       "      <td>1.787523</td>\n",
       "      <td>5.466168</td>\n",
       "      <td>2.251319</td>\n",
       "      <td>1.391699</td>\n",
       "      <td>1.920443</td>\n",
       "      <td>1.776509</td>\n",
       "      <td>1.241894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cholesterol</td>\n",
       "      <td>38.400731</td>\n",
       "      <td>64.665720</td>\n",
       "      <td>40.927007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.812929</td>\n",
       "      <td>52.741440</td>\n",
       "      <td>44.662291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.378078</td>\n",
       "      <td>32.301643</td>\n",
       "      <td>27.556951</td>\n",
       "      <td>3.825745</td>\n",
       "      <td>31.399719</td>\n",
       "      <td>54.914536</td>\n",
       "      <td>23.766652</td>\n",
       "      <td>15.478622</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Creatinine</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.336804</td>\n",
       "      <td>0.324496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308442</td>\n",
       "      <td>0.325636</td>\n",
       "      <td>0.298301</td>\n",
       "      <td>0.282373</td>\n",
       "      <td>0.409005</td>\n",
       "      <td>0.335304</td>\n",
       "      <td>0.360020</td>\n",
       "      <td>0.273227</td>\n",
       "      <td>0.350594</td>\n",
       "      <td>0.318273</td>\n",
       "      <td>0.277424</td>\n",
       "      <td>0.297009</td>\n",
       "      <td>0.297080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DiasABP</td>\n",
       "      <td>3.286241</td>\n",
       "      <td>3.337108</td>\n",
       "      <td>3.081119</td>\n",
       "      <td>2.338437</td>\n",
       "      <td>3.467723</td>\n",
       "      <td>2.961258</td>\n",
       "      <td>3.835419</td>\n",
       "      <td>2.828557</td>\n",
       "      <td>3.201445</td>\n",
       "      <td>3.089303</td>\n",
       "      <td>3.206805</td>\n",
       "      <td>3.154283</td>\n",
       "      <td>3.147369</td>\n",
       "      <td>3.272900</td>\n",
       "      <td>3.221336</td>\n",
       "      <td>3.014249</td>\n",
       "      <td>3.200419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FiO2</td>\n",
       "      <td>0.091048</td>\n",
       "      <td>0.085020</td>\n",
       "      <td>0.097313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092311</td>\n",
       "      <td>0.094241</td>\n",
       "      <td>0.090704</td>\n",
       "      <td>0.087931</td>\n",
       "      <td>0.095330</td>\n",
       "      <td>0.082386</td>\n",
       "      <td>0.086553</td>\n",
       "      <td>0.089609</td>\n",
       "      <td>0.082583</td>\n",
       "      <td>0.077830</td>\n",
       "      <td>0.095487</td>\n",
       "      <td>0.094203</td>\n",
       "      <td>0.107812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GCS</td>\n",
       "      <td>1.111560</td>\n",
       "      <td>1.066320</td>\n",
       "      <td>1.171348</td>\n",
       "      <td>0.544286</td>\n",
       "      <td>1.104704</td>\n",
       "      <td>1.095373</td>\n",
       "      <td>1.057828</td>\n",
       "      <td>1.625264</td>\n",
       "      <td>1.067594</td>\n",
       "      <td>0.983383</td>\n",
       "      <td>0.968040</td>\n",
       "      <td>1.259802</td>\n",
       "      <td>1.304571</td>\n",
       "      <td>1.326264</td>\n",
       "      <td>1.153910</td>\n",
       "      <td>1.198873</td>\n",
       "      <td>1.180866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Glucose</td>\n",
       "      <td>35.675561</td>\n",
       "      <td>35.932910</td>\n",
       "      <td>37.808749</td>\n",
       "      <td>15.417404</td>\n",
       "      <td>37.783916</td>\n",
       "      <td>39.641572</td>\n",
       "      <td>38.862338</td>\n",
       "      <td>29.817379</td>\n",
       "      <td>43.838713</td>\n",
       "      <td>28.403849</td>\n",
       "      <td>38.257792</td>\n",
       "      <td>62.828565</td>\n",
       "      <td>34.208764</td>\n",
       "      <td>35.980538</td>\n",
       "      <td>29.028685</td>\n",
       "      <td>37.027772</td>\n",
       "      <td>40.448167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HCO3</td>\n",
       "      <td>1.997330</td>\n",
       "      <td>2.040713</td>\n",
       "      <td>2.049113</td>\n",
       "      <td>1.013312</td>\n",
       "      <td>2.184270</td>\n",
       "      <td>2.109203</td>\n",
       "      <td>1.939059</td>\n",
       "      <td>1.646092</td>\n",
       "      <td>2.299890</td>\n",
       "      <td>1.890032</td>\n",
       "      <td>2.255663</td>\n",
       "      <td>2.729447</td>\n",
       "      <td>2.036791</td>\n",
       "      <td>1.682571</td>\n",
       "      <td>1.799330</td>\n",
       "      <td>1.470391</td>\n",
       "      <td>2.239728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HCT</td>\n",
       "      <td>2.749176</td>\n",
       "      <td>2.836203</td>\n",
       "      <td>2.690640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.554819</td>\n",
       "      <td>2.645722</td>\n",
       "      <td>2.826863</td>\n",
       "      <td>2.472529</td>\n",
       "      <td>2.957552</td>\n",
       "      <td>2.587723</td>\n",
       "      <td>2.825440</td>\n",
       "      <td>2.737536</td>\n",
       "      <td>2.476707</td>\n",
       "      <td>2.739891</td>\n",
       "      <td>2.719506</td>\n",
       "      <td>2.694744</td>\n",
       "      <td>2.938093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HR</td>\n",
       "      <td>4.954846</td>\n",
       "      <td>5.055318</td>\n",
       "      <td>4.895126</td>\n",
       "      <td>4.929430</td>\n",
       "      <td>4.986115</td>\n",
       "      <td>4.879796</td>\n",
       "      <td>4.821826</td>\n",
       "      <td>4.232061</td>\n",
       "      <td>5.101064</td>\n",
       "      <td>5.287386</td>\n",
       "      <td>5.080465</td>\n",
       "      <td>5.852353</td>\n",
       "      <td>4.850859</td>\n",
       "      <td>4.602111</td>\n",
       "      <td>4.461947</td>\n",
       "      <td>4.558286</td>\n",
       "      <td>4.731497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>K</td>\n",
       "      <td>0.418811</td>\n",
       "      <td>0.431528</td>\n",
       "      <td>0.401318</td>\n",
       "      <td>0.439721</td>\n",
       "      <td>0.383908</td>\n",
       "      <td>0.426004</td>\n",
       "      <td>0.392062</td>\n",
       "      <td>0.453842</td>\n",
       "      <td>0.480025</td>\n",
       "      <td>0.355890</td>\n",
       "      <td>0.415551</td>\n",
       "      <td>0.382642</td>\n",
       "      <td>0.406445</td>\n",
       "      <td>0.444354</td>\n",
       "      <td>0.360935</td>\n",
       "      <td>0.361885</td>\n",
       "      <td>0.407590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lactate</td>\n",
       "      <td>0.957856</td>\n",
       "      <td>0.889468</td>\n",
       "      <td>1.081914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875934</td>\n",
       "      <td>0.990148</td>\n",
       "      <td>1.249445</td>\n",
       "      <td>0.903555</td>\n",
       "      <td>1.247109</td>\n",
       "      <td>1.081465</td>\n",
       "      <td>1.015313</td>\n",
       "      <td>1.883991</td>\n",
       "      <td>0.902787</td>\n",
       "      <td>1.147012</td>\n",
       "      <td>1.121165</td>\n",
       "      <td>0.966507</td>\n",
       "      <td>1.037675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MAP</td>\n",
       "      <td>4.378979</td>\n",
       "      <td>4.452709</td>\n",
       "      <td>4.690102</td>\n",
       "      <td>4.312929</td>\n",
       "      <td>4.751582</td>\n",
       "      <td>4.530798</td>\n",
       "      <td>4.618477</td>\n",
       "      <td>4.457792</td>\n",
       "      <td>4.346825</td>\n",
       "      <td>4.380908</td>\n",
       "      <td>4.659014</td>\n",
       "      <td>4.010374</td>\n",
       "      <td>4.436943</td>\n",
       "      <td>4.725435</td>\n",
       "      <td>4.566593</td>\n",
       "      <td>4.189926</td>\n",
       "      <td>4.439701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MechVent</td>\n",
       "      <td>0.016132</td>\n",
       "      <td>0.014552</td>\n",
       "      <td>0.014512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012942</td>\n",
       "      <td>0.018278</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>0.012379</td>\n",
       "      <td>0.015399</td>\n",
       "      <td>0.016767</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>0.011590</td>\n",
       "      <td>0.013785</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.014115</td>\n",
       "      <td>0.014136</td>\n",
       "      <td>0.021396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mg</td>\n",
       "      <td>0.263525</td>\n",
       "      <td>0.269840</td>\n",
       "      <td>0.271036</td>\n",
       "      <td>0.187056</td>\n",
       "      <td>0.261926</td>\n",
       "      <td>0.259076</td>\n",
       "      <td>0.237843</td>\n",
       "      <td>0.294052</td>\n",
       "      <td>0.290116</td>\n",
       "      <td>0.263472</td>\n",
       "      <td>0.238837</td>\n",
       "      <td>0.233769</td>\n",
       "      <td>0.283492</td>\n",
       "      <td>0.251284</td>\n",
       "      <td>0.234670</td>\n",
       "      <td>0.314494</td>\n",
       "      <td>0.266337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NIDiasABP</td>\n",
       "      <td>3.570238</td>\n",
       "      <td>3.962779</td>\n",
       "      <td>3.644148</td>\n",
       "      <td>3.308968</td>\n",
       "      <td>3.880349</td>\n",
       "      <td>3.554298</td>\n",
       "      <td>3.418196</td>\n",
       "      <td>3.646251</td>\n",
       "      <td>3.815104</td>\n",
       "      <td>3.872947</td>\n",
       "      <td>3.788726</td>\n",
       "      <td>4.275811</td>\n",
       "      <td>3.541497</td>\n",
       "      <td>3.643103</td>\n",
       "      <td>3.228744</td>\n",
       "      <td>3.631663</td>\n",
       "      <td>4.783361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NIMAP</td>\n",
       "      <td>2.641346</td>\n",
       "      <td>2.822390</td>\n",
       "      <td>2.527404</td>\n",
       "      <td>1.663565</td>\n",
       "      <td>2.637725</td>\n",
       "      <td>2.706873</td>\n",
       "      <td>2.431636</td>\n",
       "      <td>2.421383</td>\n",
       "      <td>2.684392</td>\n",
       "      <td>2.787510</td>\n",
       "      <td>2.741174</td>\n",
       "      <td>3.738670</td>\n",
       "      <td>2.643183</td>\n",
       "      <td>2.517836</td>\n",
       "      <td>2.539368</td>\n",
       "      <td>2.409625</td>\n",
       "      <td>3.321727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NISysABP</td>\n",
       "      <td>6.786346</td>\n",
       "      <td>6.968757</td>\n",
       "      <td>6.942146</td>\n",
       "      <td>3.254864</td>\n",
       "      <td>7.182070</td>\n",
       "      <td>6.263310</td>\n",
       "      <td>6.559004</td>\n",
       "      <td>6.987860</td>\n",
       "      <td>6.773643</td>\n",
       "      <td>7.035062</td>\n",
       "      <td>6.863475</td>\n",
       "      <td>6.393593</td>\n",
       "      <td>6.617142</td>\n",
       "      <td>6.617166</td>\n",
       "      <td>6.459524</td>\n",
       "      <td>6.614485</td>\n",
       "      <td>7.546407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Na</td>\n",
       "      <td>2.672384</td>\n",
       "      <td>2.769542</td>\n",
       "      <td>2.602539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.494917</td>\n",
       "      <td>2.821174</td>\n",
       "      <td>2.441333</td>\n",
       "      <td>2.415947</td>\n",
       "      <td>2.542292</td>\n",
       "      <td>2.875124</td>\n",
       "      <td>2.996601</td>\n",
       "      <td>2.674243</td>\n",
       "      <td>2.872144</td>\n",
       "      <td>2.633774</td>\n",
       "      <td>2.208561</td>\n",
       "      <td>2.123646</td>\n",
       "      <td>2.379234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PaCO2</td>\n",
       "      <td>4.967971</td>\n",
       "      <td>5.076210</td>\n",
       "      <td>4.715042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.413070</td>\n",
       "      <td>4.981651</td>\n",
       "      <td>4.321863</td>\n",
       "      <td>4.352053</td>\n",
       "      <td>6.070209</td>\n",
       "      <td>4.347840</td>\n",
       "      <td>5.831317</td>\n",
       "      <td>5.472330</td>\n",
       "      <td>4.507438</td>\n",
       "      <td>4.505633</td>\n",
       "      <td>4.120564</td>\n",
       "      <td>3.895674</td>\n",
       "      <td>5.058675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PaO2</td>\n",
       "      <td>43.050423</td>\n",
       "      <td>42.619913</td>\n",
       "      <td>41.131272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.616914</td>\n",
       "      <td>43.217058</td>\n",
       "      <td>44.119486</td>\n",
       "      <td>40.452805</td>\n",
       "      <td>41.993705</td>\n",
       "      <td>42.284284</td>\n",
       "      <td>46.238682</td>\n",
       "      <td>50.544774</td>\n",
       "      <td>41.696367</td>\n",
       "      <td>40.346382</td>\n",
       "      <td>40.372656</td>\n",
       "      <td>29.534833</td>\n",
       "      <td>33.947740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Platelets</td>\n",
       "      <td>40.793265</td>\n",
       "      <td>42.525995</td>\n",
       "      <td>38.203296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.371299</td>\n",
       "      <td>40.099692</td>\n",
       "      <td>36.798819</td>\n",
       "      <td>37.461673</td>\n",
       "      <td>48.761128</td>\n",
       "      <td>36.571258</td>\n",
       "      <td>44.374709</td>\n",
       "      <td>47.097318</td>\n",
       "      <td>37.030077</td>\n",
       "      <td>43.549062</td>\n",
       "      <td>40.398868</td>\n",
       "      <td>42.935501</td>\n",
       "      <td>27.924109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RespRate</td>\n",
       "      <td>2.670932</td>\n",
       "      <td>2.736726</td>\n",
       "      <td>2.644809</td>\n",
       "      <td>0.174608</td>\n",
       "      <td>2.569035</td>\n",
       "      <td>2.632417</td>\n",
       "      <td>2.601214</td>\n",
       "      <td>2.289578</td>\n",
       "      <td>2.844393</td>\n",
       "      <td>2.790671</td>\n",
       "      <td>2.728389</td>\n",
       "      <td>3.696697</td>\n",
       "      <td>2.502556</td>\n",
       "      <td>2.413531</td>\n",
       "      <td>2.553265</td>\n",
       "      <td>2.611844</td>\n",
       "      <td>2.521425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SaO2</td>\n",
       "      <td>1.359402</td>\n",
       "      <td>2.037315</td>\n",
       "      <td>1.407910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467290</td>\n",
       "      <td>2.140035</td>\n",
       "      <td>1.884904</td>\n",
       "      <td>1.209799</td>\n",
       "      <td>5.449911</td>\n",
       "      <td>1.561281</td>\n",
       "      <td>2.234493</td>\n",
       "      <td>1.374787</td>\n",
       "      <td>1.379427</td>\n",
       "      <td>1.424463</td>\n",
       "      <td>1.358058</td>\n",
       "      <td>1.507972</td>\n",
       "      <td>1.686622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SysABP</td>\n",
       "      <td>6.740217</td>\n",
       "      <td>6.637011</td>\n",
       "      <td>6.686289</td>\n",
       "      <td>4.437422</td>\n",
       "      <td>6.738621</td>\n",
       "      <td>6.144654</td>\n",
       "      <td>7.138861</td>\n",
       "      <td>6.303302</td>\n",
       "      <td>6.422521</td>\n",
       "      <td>6.585868</td>\n",
       "      <td>6.678920</td>\n",
       "      <td>6.510742</td>\n",
       "      <td>6.282462</td>\n",
       "      <td>6.607206</td>\n",
       "      <td>5.830505</td>\n",
       "      <td>6.208069</td>\n",
       "      <td>6.254736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Temp</td>\n",
       "      <td>0.466368</td>\n",
       "      <td>0.500463</td>\n",
       "      <td>0.496633</td>\n",
       "      <td>0.196135</td>\n",
       "      <td>0.497527</td>\n",
       "      <td>0.504793</td>\n",
       "      <td>0.560734</td>\n",
       "      <td>0.363337</td>\n",
       "      <td>0.712791</td>\n",
       "      <td>0.552139</td>\n",
       "      <td>0.566915</td>\n",
       "      <td>0.394675</td>\n",
       "      <td>0.446483</td>\n",
       "      <td>0.407501</td>\n",
       "      <td>0.370067</td>\n",
       "      <td>0.465328</td>\n",
       "      <td>0.422670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TroponinI</td>\n",
       "      <td>6.362012</td>\n",
       "      <td>4.426368</td>\n",
       "      <td>5.835877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.108636</td>\n",
       "      <td>5.154131</td>\n",
       "      <td>15.672864</td>\n",
       "      <td>5.958891</td>\n",
       "      <td>3.802148</td>\n",
       "      <td>9.534473</td>\n",
       "      <td>6.168008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.053101</td>\n",
       "      <td>8.658358</td>\n",
       "      <td>5.787554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.924161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TroponinT</td>\n",
       "      <td>1.693757</td>\n",
       "      <td>1.843798</td>\n",
       "      <td>2.156500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.689169</td>\n",
       "      <td>2.129632</td>\n",
       "      <td>1.865677</td>\n",
       "      <td>2.117408</td>\n",
       "      <td>1.621257</td>\n",
       "      <td>1.709294</td>\n",
       "      <td>1.585811</td>\n",
       "      <td>0.585628</td>\n",
       "      <td>1.236493</td>\n",
       "      <td>1.963694</td>\n",
       "      <td>1.242526</td>\n",
       "      <td>4.370945</td>\n",
       "      <td>3.122294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Urine</td>\n",
       "      <td>71.261287</td>\n",
       "      <td>66.061741</td>\n",
       "      <td>73.192831</td>\n",
       "      <td>65.789238</td>\n",
       "      <td>63.168425</td>\n",
       "      <td>86.330015</td>\n",
       "      <td>81.368626</td>\n",
       "      <td>67.909313</td>\n",
       "      <td>69.538475</td>\n",
       "      <td>77.175518</td>\n",
       "      <td>74.291728</td>\n",
       "      <td>77.240789</td>\n",
       "      <td>60.722856</td>\n",
       "      <td>67.791088</td>\n",
       "      <td>77.218000</td>\n",
       "      <td>69.161830</td>\n",
       "      <td>86.801906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WBC</td>\n",
       "      <td>6.241772</td>\n",
       "      <td>6.099460</td>\n",
       "      <td>6.976084</td>\n",
       "      <td>0.779596</td>\n",
       "      <td>6.962048</td>\n",
       "      <td>6.254304</td>\n",
       "      <td>4.886107</td>\n",
       "      <td>7.378177</td>\n",
       "      <td>6.319079</td>\n",
       "      <td>6.221232</td>\n",
       "      <td>6.886531</td>\n",
       "      <td>7.850301</td>\n",
       "      <td>6.844688</td>\n",
       "      <td>5.863044</td>\n",
       "      <td>9.258225</td>\n",
       "      <td>6.525281</td>\n",
       "      <td>4.523911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Weight</td>\n",
       "      <td>3.566410</td>\n",
       "      <td>3.700160</td>\n",
       "      <td>3.267497</td>\n",
       "      <td>3.149892</td>\n",
       "      <td>3.149132</td>\n",
       "      <td>3.962618</td>\n",
       "      <td>3.278340</td>\n",
       "      <td>3.836552</td>\n",
       "      <td>3.383119</td>\n",
       "      <td>3.872834</td>\n",
       "      <td>3.426406</td>\n",
       "      <td>6.006028</td>\n",
       "      <td>3.229866</td>\n",
       "      <td>2.386197</td>\n",
       "      <td>3.300715</td>\n",
       "      <td>3.484257</td>\n",
       "      <td>7.298356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ph</td>\n",
       "      <td>0.184695</td>\n",
       "      <td>0.115544</td>\n",
       "      <td>0.125340</td>\n",
       "      <td>0.039608</td>\n",
       "      <td>0.112644</td>\n",
       "      <td>0.113526</td>\n",
       "      <td>0.128245</td>\n",
       "      <td>0.131825</td>\n",
       "      <td>0.113775</td>\n",
       "      <td>0.108418</td>\n",
       "      <td>0.118106</td>\n",
       "      <td>0.122447</td>\n",
       "      <td>0.114938</td>\n",
       "      <td>0.113485</td>\n",
       "      <td>1.129916</td>\n",
       "      <td>0.106379</td>\n",
       "      <td>0.114143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     General      Female        Male  Undefined Gender  \\\n",
       "0           ALP   46.051781   51.660965   50.391101          0.000000   \n",
       "1           ALT  406.211277  314.161357  297.518824          0.000000   \n",
       "2           AST  496.992056  451.364648  359.312336          0.000000   \n",
       "3       Albumin    0.385179    0.400118    0.408109          0.000000   \n",
       "4           BUN    5.380332    5.788564    5.229164          0.000000   \n",
       "5     Bilirubin    1.673677    1.727891    1.428961          0.000000   \n",
       "6   Cholesterol   38.400731   64.665720   40.927007          0.000000   \n",
       "7    Creatinine    0.350400    0.336804    0.324496          0.000000   \n",
       "8       DiasABP    3.286241    3.337108    3.081119          2.338437   \n",
       "9          FiO2    0.091048    0.085020    0.097313          0.000000   \n",
       "10          GCS    1.111560    1.066320    1.171348          0.544286   \n",
       "11      Glucose   35.675561   35.932910   37.808749         15.417404   \n",
       "12         HCO3    1.997330    2.040713    2.049113          1.013312   \n",
       "13          HCT    2.749176    2.836203    2.690640          0.000000   \n",
       "14           HR    4.954846    5.055318    4.895126          4.929430   \n",
       "15            K    0.418811    0.431528    0.401318          0.439721   \n",
       "16      Lactate    0.957856    0.889468    1.081914          0.000000   \n",
       "17          MAP    4.378979    4.452709    4.690102          4.312929   \n",
       "18     MechVent    0.016132    0.014552    0.014512          0.000000   \n",
       "19           Mg    0.263525    0.269840    0.271036          0.187056   \n",
       "20    NIDiasABP    3.570238    3.962779    3.644148          3.308968   \n",
       "21        NIMAP    2.641346    2.822390    2.527404          1.663565   \n",
       "22     NISysABP    6.786346    6.968757    6.942146          3.254864   \n",
       "23           Na    2.672384    2.769542    2.602539          0.000000   \n",
       "24        PaCO2    4.967971    5.076210    4.715042          0.000000   \n",
       "25         PaO2   43.050423   42.619913   41.131272          0.000000   \n",
       "26    Platelets   40.793265   42.525995   38.203296          0.000000   \n",
       "27     RespRate    2.670932    2.736726    2.644809          0.174608   \n",
       "28         SaO2    1.359402    2.037315    1.407910          0.000000   \n",
       "29       SysABP    6.740217    6.637011    6.686289          4.437422   \n",
       "30         Temp    0.466368    0.500463    0.496633          0.196135   \n",
       "31    TroponinI    6.362012    4.426368    5.835877          0.000000   \n",
       "32    TroponinT    1.693757    1.843798    2.156500          0.000000   \n",
       "33        Urine   71.261287   66.061741   73.192831         65.789238   \n",
       "34          WBC    6.241772    6.099460    6.976084          0.779596   \n",
       "35       Weight    3.566410    3.700160    3.267497          3.149892   \n",
       "36           Ph    0.184695    0.115544    0.125340          0.039608   \n",
       "\n",
       "           +65         -65   ICUType 1   ICUType 2   ICUType 3   ICUType 4  \\\n",
       "0    42.976266   95.976280   28.805102   25.384391   93.671351   65.502523   \n",
       "1   328.757487  387.086329  348.481409  412.199358  365.521484  319.837259   \n",
       "2   361.902289  426.691253  596.861957  479.165045  536.706173  537.244698   \n",
       "3     0.352356    0.432542    0.321838    0.471058    0.465325    0.495207   \n",
       "4     5.537895    5.581244    4.873432    5.092528    6.735940    4.790106   \n",
       "5     1.328168    1.927334    1.148139    1.823300    1.606364    1.534762   \n",
       "6    32.812929   52.741440   44.662291    0.000000   40.378078   32.301643   \n",
       "7     0.308442    0.325636    0.298301    0.282373    0.409005    0.335304   \n",
       "8     3.467723    2.961258    3.835419    2.828557    3.201445    3.089303   \n",
       "9     0.092311    0.094241    0.090704    0.087931    0.095330    0.082386   \n",
       "10    1.104704    1.095373    1.057828    1.625264    1.067594    0.983383   \n",
       "11   37.783916   39.641572   38.862338   29.817379   43.838713   28.403849   \n",
       "12    2.184270    2.109203    1.939059    1.646092    2.299890    1.890032   \n",
       "13    2.554819    2.645722    2.826863    2.472529    2.957552    2.587723   \n",
       "14    4.986115    4.879796    4.821826    4.232061    5.101064    5.287386   \n",
       "15    0.383908    0.426004    0.392062    0.453842    0.480025    0.355890   \n",
       "16    0.875934    0.990148    1.249445    0.903555    1.247109    1.081465   \n",
       "17    4.751582    4.530798    4.618477    4.457792    4.346825    4.380908   \n",
       "18    0.012942    0.018278    0.012980    0.012379    0.015399    0.016767   \n",
       "19    0.261926    0.259076    0.237843    0.294052    0.290116    0.263472   \n",
       "20    3.880349    3.554298    3.418196    3.646251    3.815104    3.872947   \n",
       "21    2.637725    2.706873    2.431636    2.421383    2.684392    2.787510   \n",
       "22    7.182070    6.263310    6.559004    6.987860    6.773643    7.035062   \n",
       "23    2.494917    2.821174    2.441333    2.415947    2.542292    2.875124   \n",
       "24    4.413070    4.981651    4.321863    4.352053    6.070209    4.347840   \n",
       "25   38.616914   43.217058   44.119486   40.452805   41.993705   42.284284   \n",
       "26   40.371299   40.099692   36.798819   37.461673   48.761128   36.571258   \n",
       "27    2.569035    2.632417    2.601214    2.289578    2.844393    2.790671   \n",
       "28    1.467290    2.140035    1.884904    1.209799    5.449911    1.561281   \n",
       "29    6.738621    6.144654    7.138861    6.303302    6.422521    6.585868   \n",
       "30    0.497527    0.504793    0.560734    0.363337    0.712791    0.552139   \n",
       "31   10.108636    5.154131   15.672864    5.958891    3.802148    9.534473   \n",
       "32    1.689169    2.129632    1.865677    2.117408    1.621257    1.709294   \n",
       "33   63.168425   86.330015   81.368626   67.909313   69.538475   77.175518   \n",
       "34    6.962048    6.254304    4.886107    7.378177    6.319079    6.221232   \n",
       "35    3.149132    3.962618    3.278340    3.836552    3.383119    3.872834   \n",
       "36    0.112644    0.113526    0.128245    0.131825    0.113775    0.108418   \n",
       "\n",
       "    Undefined classification   Low Weight  Normal Weight  Overweight  \\\n",
       "0                  38.470790    28.032644      36.798283   30.579934   \n",
       "1                 382.291076   280.105937     295.352176  359.007188   \n",
       "2                 429.105660  1227.220856     384.279929  558.798516   \n",
       "3                   0.509711     0.769713       0.551819    0.535161   \n",
       "4                   5.892070     4.585910       5.016969    5.384336   \n",
       "5                   1.787523     5.466168       2.251319    1.391699   \n",
       "6                  27.556951     3.825745      31.399719   54.914536   \n",
       "7                   0.360020     0.273227       0.350594    0.318273   \n",
       "8                   3.206805     3.154283       3.147369    3.272900   \n",
       "9                   0.086553     0.089609       0.082583    0.077830   \n",
       "10                  0.968040     1.259802       1.304571    1.326264   \n",
       "11                 38.257792    62.828565      34.208764   35.980538   \n",
       "12                  2.255663     2.729447       2.036791    1.682571   \n",
       "13                  2.825440     2.737536       2.476707    2.739891   \n",
       "14                  5.080465     5.852353       4.850859    4.602111   \n",
       "15                  0.415551     0.382642       0.406445    0.444354   \n",
       "16                  1.015313     1.883991       0.902787    1.147012   \n",
       "17                  4.659014     4.010374       4.436943    4.725435   \n",
       "18                  0.015603     0.011590       0.013785    0.014702   \n",
       "19                  0.238837     0.233769       0.283492    0.251284   \n",
       "20                  3.788726     4.275811       3.541497    3.643103   \n",
       "21                  2.741174     3.738670       2.643183    2.517836   \n",
       "22                  6.863475     6.393593       6.617142    6.617166   \n",
       "23                  2.996601     2.674243       2.872144    2.633774   \n",
       "24                  5.831317     5.472330       4.507438    4.505633   \n",
       "25                 46.238682    50.544774      41.696367   40.346382   \n",
       "26                 44.374709    47.097318      37.030077   43.549062   \n",
       "27                  2.728389     3.696697       2.502556    2.413531   \n",
       "28                  2.234493     1.374787       1.379427    1.424463   \n",
       "29                  6.678920     6.510742       6.282462    6.607206   \n",
       "30                  0.566915     0.394675       0.446483    0.407501   \n",
       "31                  6.168008     0.000000       6.053101    8.658358   \n",
       "32                  1.585811     0.585628       1.236493    1.963694   \n",
       "33                 74.291728    77.240789      60.722856   67.791088   \n",
       "34                  6.886531     7.850301       6.844688    5.863044   \n",
       "35                  3.426406     6.006028       3.229866    2.386197   \n",
       "36                  0.118106     0.122447       0.114938    0.113485   \n",
       "\n",
       "     Obesity 1   Obesity 2   Obesity 3  \n",
       "0    23.837229   44.978069   34.543202  \n",
       "1   322.465492  373.911811  966.493927  \n",
       "2   364.348525  417.221308  367.442741  \n",
       "3     0.307209    0.440125    0.479202  \n",
       "4     4.807965    7.067806    5.749823  \n",
       "5     1.920443    1.776509    1.241894  \n",
       "6    23.766652   15.478622    0.000000  \n",
       "7     0.277424    0.297009    0.297080  \n",
       "8     3.221336    3.014249    3.200419  \n",
       "9     0.095487    0.094203    0.107812  \n",
       "10    1.153910    1.198873    1.180866  \n",
       "11   29.028685   37.027772   40.448167  \n",
       "12    1.799330    1.470391    2.239728  \n",
       "13    2.719506    2.694744    2.938093  \n",
       "14    4.461947    4.558286    4.731497  \n",
       "15    0.360935    0.361885    0.407590  \n",
       "16    1.121165    0.966507    1.037675  \n",
       "17    4.566593    4.189926    4.439701  \n",
       "18    0.014115    0.014136    0.021396  \n",
       "19    0.234670    0.314494    0.266337  \n",
       "20    3.228744    3.631663    4.783361  \n",
       "21    2.539368    2.409625    3.321727  \n",
       "22    6.459524    6.614485    7.546407  \n",
       "23    2.208561    2.123646    2.379234  \n",
       "24    4.120564    3.895674    5.058675  \n",
       "25   40.372656   29.534833   33.947740  \n",
       "26   40.398868   42.935501   27.924109  \n",
       "27    2.553265    2.611844    2.521425  \n",
       "28    1.358058    1.507972    1.686622  \n",
       "29    5.830505    6.208069    6.254736  \n",
       "30    0.370067    0.465328    0.422670  \n",
       "31    5.787554    0.000000    2.924161  \n",
       "32    1.242526    4.370945    3.122294  \n",
       "33   77.218000   69.161830   86.801906  \n",
       "34    9.258225    6.525281    4.523911  \n",
       "35    3.300715    3.484257    7.298356  \n",
       "36    1.129916    0.106379    0.114143  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saits_mae_standard_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "MechVent\n",
      "0.01613150967851928\n",
      "--------------------\n",
      "Female\n",
      "MechVent\n",
      "0.01455234083128562\n",
      "--------------------\n",
      "Male\n",
      "MechVent\n",
      "0.01451199920706472\n",
      "--------------------\n",
      "Undefined Gender\n",
      "TroponinT\n",
      "0.0\n",
      "--------------------\n",
      "+65\n",
      "MechVent\n",
      "0.012942019615087409\n",
      "--------------------\n",
      "-65\n",
      "MechVent\n",
      "0.01827777949621049\n",
      "--------------------\n",
      "ICUType 1\n",
      "MechVent\n",
      "0.01297977845445802\n",
      "--------------------\n",
      "ICUType 2\n",
      "Cholesterol\n",
      "0.0\n",
      "--------------------\n",
      "ICUType 3\n",
      "MechVent\n",
      "0.015398822799574146\n",
      "--------------------\n",
      "ICUType 4\n",
      "MechVent\n",
      "0.016766555871361\n",
      "--------------------\n",
      "Undefined classification\n",
      "MechVent\n",
      "0.015602763073054438\n",
      "--------------------\n",
      "Low Weight\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Normal Weight\n",
      "MechVent\n",
      "0.01378469708507042\n",
      "--------------------\n",
      "Overweight\n",
      "MechVent\n",
      "0.01470213082798736\n",
      "--------------------\n",
      "Obesity 1\n",
      "MechVent\n",
      "0.014115385917010151\n",
      "--------------------\n",
      "Obesity 2\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Obesity 3\n",
      "Cholesterol\n",
      "0.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.min_value_in_subgroup(df_saits_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "AST\n",
      "496.992055665294\n",
      "--------------------\n",
      "Female\n",
      "AST\n",
      "451.3646475829517\n",
      "--------------------\n",
      "Male\n",
      "AST\n",
      "359.3123356241838\n",
      "--------------------\n",
      "Undefined Gender\n",
      "Urine\n",
      "65.78923797600842\n",
      "--------------------\n",
      "+65\n",
      "AST\n",
      "361.9022886474364\n",
      "--------------------\n",
      "-65\n",
      "AST\n",
      "426.6912528872452\n",
      "--------------------\n",
      "ICUType 1\n",
      "AST\n",
      "596.8619574737311\n",
      "--------------------\n",
      "ICUType 2\n",
      "AST\n",
      "479.1650450333097\n",
      "--------------------\n",
      "ICUType 3\n",
      "AST\n",
      "536.7061729221494\n",
      "--------------------\n",
      "ICUType 4\n",
      "AST\n",
      "537.2446980476282\n",
      "--------------------\n",
      "Undefined classification\n",
      "AST\n",
      "429.10566036324303\n",
      "--------------------\n",
      "Low Weight\n",
      "AST\n",
      "1227.220855712584\n",
      "--------------------\n",
      "Normal Weight\n",
      "AST\n",
      "384.2799291610597\n",
      "--------------------\n",
      "Overweight\n",
      "AST\n",
      "558.7985157739283\n",
      "--------------------\n",
      "Obesity 1\n",
      "AST\n",
      "364.3485252380189\n",
      "--------------------\n",
      "Obesity 2\n",
      "AST\n",
      "417.2213083901604\n",
      "--------------------\n",
      "Obesity 3\n",
      "ALT\n",
      "966.4939270016309\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.max_value_in_subgroup(df_saits_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAITS - MAE\n",
      "************\n",
      "General\n",
      "-------------\n",
      "ALP : 0.040219907134693526\n",
      "ALT : 0.017229766448887466\n",
      "AST : 0.015468583460460567\n",
      "Albumin : 0.45373933730150834\n",
      "BUN : 0.11917518228086253\n",
      "Bilirubin : 0.04461143951106635\n",
      "Cholesterol : 0.38712575286624884\n",
      "Creatinine : 0.06330636563215755\n",
      "DiasABP : 0.2222883897032941\n",
      "FiO2 : 0.4212544503171473\n",
      "GCS : 0.7013794357043793\n",
      "Glucose : 0.08704999971751116\n",
      "HCO3 : 0.38295172317766624\n",
      "HCT : 0.45079366319242514\n",
      "HR : 0.43347807748668743\n",
      "K : 0.12318451615964963\n",
      "Lactate : 0.08926922807197073\n",
      "MAP : 0.2694956399200512\n",
      "MechVent : 0.0\n",
      "Mg : 0.06937552593284982\n",
      "NIDiasABP : 0.34968573475266584\n",
      "NIMAP : 0.34092245932783305\n",
      "NISysABP : 0.45429145035202567\n",
      "Na : 0.4986517947522833\n",
      "PaCO2 : 0.4095278180110242\n",
      "PaO2 : 0.29658351907452635\n",
      "Platelets : 0.08119226622028226\n",
      "RespRate : 0.19828358249685923\n",
      "SaO2 : 0.9685085946654024\n",
      "SysABP : 0.42148474257749563\n",
      "Temp : 0.9335184893617962\n",
      "TroponinI : 0.16517311730421255\n",
      "TroponinT : 0.02879763797807984\n",
      "Urine : 0.010554451556121645\n",
      "WBC : 0.0019617856545559084\n",
      "Weight : 0.2781759731419437\n",
      "Ph : 0.07438949385076159\n",
      "Female\n",
      "-------------\n",
      "ALP : 0.041253007527389196\n",
      "ALT : 0.0069411088291121805\n",
      "AST : 0.017081113733614473\n",
      "Albumin : 0.42399730608946423\n",
      "BUN : 0.11502335977735291\n",
      "Bilirubin : 0.03464755074055929\n",
      "Cholesterol : 0.4730538815258987\n",
      "Creatinine : 0.04878269874642024\n",
      "DiasABP : 0.2212575231029587\n",
      "FiO2 : 0.4079546838217121\n",
      "GCS : 0.7023907811695204\n",
      "Glucose : 0.08708719000514671\n",
      "HCO3 : 0.3874236953374724\n",
      "HCT : 0.44855294804009266\n",
      "HR : 0.43543538155961137\n",
      "K : 0.11922598260821982\n",
      "Lactate : 0.08573017613457593\n",
      "MAP : 0.27097373096588906\n",
      "MechVent : 0.0\n",
      "Mg : 0.06800714125526842\n",
      "NIDiasABP : 0.34033534047107256\n",
      "NIMAP : 0.33257236090490444\n",
      "NISysABP : 0.45345923719580616\n",
      "Na : 0.49990243788560096\n",
      "PaCO2 : 0.4032464607917196\n",
      "PaO2 : 0.29394454534618536\n",
      "Platelets : 0.08775004390713584\n",
      "RespRate : 0.20054272045501548\n",
      "SaO2 : 0.9617415842380362\n",
      "SysABP : 0.42227976041291315\n",
      "Temp : 0.9318503706009829\n",
      "TroponinI : 0.09938900219275554\n",
      "TroponinT : 0.03446973771637911\n",
      "Urine : 0.009314027052929913\n",
      "WBC : 0.0019868603482145583\n",
      "Weight : 0.24873120330351559\n",
      "Ph : 0.07373614576157189\n",
      "Male\n",
      "-------------\n",
      "ALP : 0.042668923567462734\n",
      "ALT : 0.016567172909647692\n",
      "AST : 0.00677323114683121\n",
      "Albumin : 0.4500850842493282\n",
      "BUN : 0.13481296141967827\n",
      "Bilirubin : 0.03611811241463962\n",
      "Cholesterol : 0.389912486076325\n",
      "Creatinine : 0.07207926358712037\n",
      "DiasABP : 0.2235516228700771\n",
      "FiO2 : 0.44147361223849446\n",
      "GCS : 0.7036860506001456\n",
      "Glucose : 0.08990698521275324\n",
      "HCO3 : 0.3915744685232631\n",
      "HCT : 0.45299057082866984\n",
      "HR : 0.4315613519268091\n",
      "K : 0.126710613871673\n",
      "Lactate : 0.09346714446114134\n",
      "MAP : 0.26880577698813846\n",
      "MechVent : 0.0\n",
      "Mg : 0.0711525847650984\n",
      "NIDiasABP : 0.35781996331464744\n",
      "NIMAP : 0.34337200471421103\n",
      "NISysABP : 0.45048147283221424\n",
      "Na : 0.497247150197254\n",
      "PaCO2 : 0.40779974433232274\n",
      "PaO2 : 0.296289962962214\n",
      "Platelets : 0.07872296428922315\n",
      "RespRate : 0.1947304958767616\n",
      "SaO2 : 0.9675699026353866\n",
      "SysABP : 0.42416671276905604\n",
      "Temp : 0.9337515185646613\n",
      "TroponinI : 0.11065852246245388\n",
      "TroponinT : 0.03795986584583738\n",
      "Urine : 0.0112151963643603\n",
      "WBC : 0.002097486768299558\n",
      "Weight : 0.2974652455049292\n",
      "Ph : 0.0737617306852589\n",
      "Undefined Gender\n",
      "-------------\n",
      "ALP : 0.0\n",
      "ALT : 0.0\n",
      "AST : 0.0\n",
      "Albumin : 0.0\n",
      "BUN : 0.0\n",
      "Bilirubin : 0.0\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.0\n",
      "DiasABP : 0.22970085777339447\n",
      "FiO2 : 0.0\n",
      "GCS : 0.9375000149009268\n",
      "Glucose : 0.08460013568392875\n",
      "HCO3 : 0.4148936271665406\n",
      "HCT : 0.0\n",
      "HR : 0.36593750119204715\n",
      "K : 0.14485980570301826\n",
      "Lactate : 0.0\n",
      "MAP : 0.2550335563718635\n",
      "MechVent : 0.0\n",
      "Mg : 0.06206896528595439\n",
      "NIDiasABP : 0.3713235370813872\n",
      "NIMAP : 0.34793859720218506\n",
      "NISysABP : 0.3669201582668377\n",
      "Na : 0.0\n",
      "PaCO2 : 0.0\n",
      "PaO2 : 0.0\n",
      "Platelets : 0.0\n",
      "RespRate : 0.20153060555437913\n",
      "SaO2 : 0.0\n",
      "SysABP : 0.3510233908891239\n",
      "Temp : 0.9345238208766079\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.0\n",
      "Urine : 0.012727272696780352\n",
      "WBC : 0.0017354724113820756\n",
      "Weight : 0.33410853147501146\n",
      "Ph : 0.07460000365965058\n",
      "+65\n",
      "-------------\n",
      "ALP : 0.045370370705818334\n",
      "ALT : 0.012877128198442092\n",
      "AST : 0.006469063976002386\n",
      "Albumin : 0.428798982643912\n",
      "BUN : 0.14714972557857817\n",
      "Bilirubin : 0.02631955632370764\n",
      "Cholesterol : 0.3630239553749259\n",
      "Creatinine : 0.057819904964770186\n",
      "DiasABP : 0.21217337469374653\n",
      "FiO2 : 0.43162533989572455\n",
      "GCS : 0.7107294321117053\n",
      "Glucose : 0.09212396700088266\n",
      "HCO3 : 0.3825070749041546\n",
      "HCT : 0.4549880310893051\n",
      "HR : 0.41870028212537835\n",
      "K : 0.1226991177372963\n",
      "Lactate : 0.08516129041534054\n",
      "MAP : 0.2631209267869792\n",
      "MechVent : 0.0\n",
      "Mg : 0.07128044394537407\n",
      "NIDiasABP : 0.3284783756665337\n",
      "NIMAP : 0.3276358896759961\n",
      "NISysABP : 0.45358703936340705\n",
      "Na : 0.5012709182933555\n",
      "PaCO2 : 0.3972002819327603\n",
      "PaO2 : 0.29531586848333163\n",
      "Platelets : 0.08055704174945057\n",
      "RespRate : 0.20403307131547296\n",
      "SaO2 : 0.9675195857723378\n",
      "SysABP : 0.4234958352626579\n",
      "Temp : 0.9318350844650769\n",
      "TroponinI : 0.2612016308509217\n",
      "TroponinT : 0.03175937328495252\n",
      "Urine : 0.009261554083765174\n",
      "WBC : 0.0021923172391482494\n",
      "Weight : 0.2619146227310987\n",
      "Ph : 0.07375993553863346\n",
      "-65\n",
      "-------------\n",
      "ALP : 0.05836832208913651\n",
      "ALT : 0.02374350841789369\n",
      "AST : 0.011711819809144175\n",
      "Albumin : 0.450342000319672\n",
      "BUN : 0.10292022843373752\n",
      "Bilirubin : 0.05307243673420396\n",
      "Cholesterol : 0.23802395537483712\n",
      "Creatinine : 0.06033653839124137\n",
      "DiasABP : 0.235334791184502\n",
      "FiO2 : 0.42281526924740814\n",
      "GCS : 0.7056760014597636\n",
      "Glucose : 0.09057882816042705\n",
      "HCO3 : 0.3739108417892731\n",
      "HCT : 0.4469815976917735\n",
      "HR : 0.4488097328649768\n",
      "K : 0.12291450290768204\n",
      "Lactate : 0.08543369163241614\n",
      "MAP : 0.2770417583268938\n",
      "MechVent : 0.0\n",
      "Mg : 0.06829432017394209\n",
      "NIDiasABP : 0.3714812807357017\n",
      "NIMAP : 0.3484406746861661\n",
      "NISysABP : 0.4566653433352638\n",
      "Na : 0.49664845253936574\n",
      "PaCO2 : 0.40965354311184554\n",
      "PaO2 : 0.309624937684647\n",
      "Platelets : 0.0831572635522295\n",
      "RespRate : 0.1911806982481771\n",
      "SaO2 : 0.9603608350471073\n",
      "SysABP : 0.42134578770333153\n",
      "Temp : 0.9347425765847246\n",
      "TroponinI : 0.029531567881345783\n",
      "TroponinT : 0.027938365947711947\n",
      "Urine : 0.012570309098613223\n",
      "WBC : 0.0019320810814057634\n",
      "Weight : 0.2910875769149665\n",
      "Ph : 0.0737080655387312\n",
      "ICUType 1\n",
      "-------------\n",
      "ALP : 0.02926034581932022\n",
      "ALT : 0.018795437205698335\n",
      "AST : 0.018681174444499613\n",
      "Albumin : 0.54418604820964\n",
      "BUN : 0.13692605960392107\n",
      "Bilirubin : 0.018808373818173255\n",
      "Cholesterol : 0.43044680127727425\n",
      "Creatinine : 0.07954545417339084\n",
      "DiasABP : 0.2173568804572405\n",
      "FiO2 : 0.4154640049406651\n",
      "GCS : 0.8087657227549894\n",
      "Glucose : 0.09004254834144307\n",
      "HCO3 : 0.3895688191451823\n",
      "HCT : 0.48916816314061123\n",
      "HR : 0.40782203010322354\n",
      "K : 0.12439919859170825\n",
      "Lactate : 0.10457856484478183\n",
      "MAP : 0.27004272929009226\n",
      "MechVent : 0.0\n",
      "Mg : 0.07056349894501272\n",
      "NIDiasABP : 0.34238943948477296\n",
      "NIMAP : 0.32552843921714314\n",
      "NISysABP : 0.43097455472849416\n",
      "Na : 0.49199999928474036\n",
      "PaCO2 : 0.39610236179171276\n",
      "PaO2 : 0.30716666678022925\n",
      "Platelets : 0.09090437500153481\n",
      "RespRate : 0.1970665624320613\n",
      "SaO2 : 0.964683557990219\n",
      "SysABP : 0.40293670137539256\n",
      "Temp : 0.9303357952003217\n",
      "TroponinI : 0.37805498787197217\n",
      "TroponinT : 0.08106433288604878\n",
      "Urine : 0.0124505818517602\n",
      "WBC : 0.001892806403338892\n",
      "Weight : 0.27516416295873125\n",
      "Ph : 0.073885080855219\n",
      "ICUType 2\n",
      "-------------\n",
      "ALP : 0.01800716854631829\n",
      "ALT : 0.029334818360725053\n",
      "AST : 0.008077810751273878\n",
      "Albumin : 0.3920265808700955\n",
      "BUN : 0.09297082263590897\n",
      "Bilirubin : 0.037439613399326326\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.05109090849136279\n",
      "DiasABP : 0.2083781641511032\n",
      "FiO2 : 0.45435799580933445\n",
      "GCS : 0.7205333020724612\n",
      "Glucose : 0.08042548635067015\n",
      "HCO3 : 0.38667900314241965\n",
      "HCT : 0.4296229795725241\n",
      "HR : 0.4295433686690959\n",
      "K : 0.13905699282395922\n",
      "Lactate : 0.09875575999555244\n",
      "MAP : 0.25205464468544386\n",
      "MechVent : 0.0\n",
      "Mg : 0.07427772650545464\n",
      "NIDiasABP : 0.31401052510196453\n",
      "NIMAP : 0.31225747188736186\n",
      "NISysABP : 0.4237538781211777\n",
      "Na : 0.4879816178826283\n",
      "PaCO2 : 0.40511122407120026\n",
      "PaO2 : 0.3375206160782761\n",
      "Platelets : 0.06921222569943226\n",
      "RespRate : 0.20489604340892156\n",
      "SaO2 : 0.9694730503594138\n",
      "SysABP : 0.40215872384848855\n",
      "Temp : 0.9367179108571396\n",
      "TroponinI : 0.2052953146397657\n",
      "TroponinT : 0.012560386255951237\n",
      "Urine : 0.009956702513148953\n",
      "WBC : 0.002243804649595238\n",
      "Weight : 0.2898721086531682\n",
      "Ph : 0.07379809535351108\n",
      "ICUType 3\n",
      "-------------\n",
      "ALP : 0.06329818700611964\n",
      "ALT : 0.023351968604276203\n",
      "AST : 0.013382101005485524\n",
      "Albumin : 0.41972993530572705\n",
      "BUN : 0.15864541178340394\n",
      "Bilirubin : 0.039131998116132036\n",
      "Cholesterol : 0.300149705260917\n",
      "Creatinine : 0.08606654753622393\n",
      "DiasABP : 0.22627656326999634\n",
      "FiO2 : 0.43808562014724095\n",
      "GCS : 0.716312057884008\n",
      "Glucose : 0.08961132848027901\n",
      "HCO3 : 0.3684777092421416\n",
      "HCT : 0.45838026653413777\n",
      "HR : 0.44849412840628905\n",
      "K : 0.1230576937299184\n",
      "Lactate : 0.10109083152278016\n",
      "MAP : 0.2744539813503581\n",
      "MechVent : 0.0\n",
      "Mg : 0.06981561322593002\n",
      "NIDiasABP : 0.35350058867815753\n",
      "NIMAP : 0.3414212650088259\n",
      "NISysABP : 0.45627029755102816\n",
      "Na : 0.498093267751049\n",
      "PaCO2 : 0.41836148752151164\n",
      "PaO2 : 0.24595808364941613\n",
      "Platelets : 0.07839231190623336\n",
      "RespRate : 0.20217784134500574\n",
      "SaO2 : 0.9190000039338881\n",
      "SysABP : 0.4217341888604001\n",
      "Temp : 0.9274873339532732\n",
      "TroponinI : 0.052953157211751485\n",
      "TroponinT : 0.01424522404075513\n",
      "Urine : 0.010544399666614478\n",
      "WBC : 0.0019731005804678194\n",
      "Weight : 0.27311171228069553\n",
      "Ph : 0.07353313606695289\n",
      "ICUType 4\n",
      "-------------\n",
      "ALP : 0.040523514920619244\n",
      "ALT : 0.016529093646791445\n",
      "AST : 0.01720968343645557\n",
      "Albumin : 0.41167738906879764\n",
      "BUN : 0.09148997406308046\n",
      "Bilirubin : 0.045804258414405344\n",
      "Cholesterol : 0.35628743221356496\n",
      "Creatinine : 0.04697573313348541\n",
      "DiasABP : 0.2314681028410168\n",
      "FiO2 : 0.39944275999639833\n",
      "GCS : 0.6579927631331506\n",
      "Glucose : 0.08342810381611074\n",
      "HCO3 : 0.38407451483568755\n",
      "HCT : 0.45524095009359805\n",
      "HR : 0.4346676898666187\n",
      "K : 0.11938417865229461\n",
      "Lactate : 0.09302640199101543\n",
      "MAP : 0.2849046886630729\n",
      "MechVent : 0.0\n",
      "Mg : 0.06695862104743693\n",
      "NIDiasABP : 0.3557906137666193\n",
      "NIMAP : 0.3497956175431742\n",
      "NISysABP : 0.47911127757096084\n",
      "Na : 0.5045000445578792\n",
      "PaCO2 : 0.3954534001029098\n",
      "PaO2 : 0.291142858023332\n",
      "Platelets : 0.08101774986371142\n",
      "RespRate : 0.19326932441361883\n",
      "SaO2 : 0.9657142966512496\n",
      "SysABP : 0.44784303024891864\n",
      "Temp : 0.9327390101557221\n",
      "TroponinI : 0.15071283048015044\n",
      "TroponinT : 0.014966555456339225\n",
      "Urine : 0.01110275175328473\n",
      "WBC : 0.001972700301121214\n",
      "Weight : 0.27181533739803093\n",
      "Ph : 0.07380082572476464\n",
      "Undefined classification\n",
      "-------------\n",
      "ALP : 0.03699005274834551\n",
      "ALT : 0.017182390236876146\n",
      "AST : 0.007567922496213805\n",
      "Albumin : 0.4045915362448977\n",
      "BUN : 0.12575549518223822\n",
      "Bilirubin : 0.044419113936404506\n",
      "Cholesterol : 0.4271457095940433\n",
      "Creatinine : 0.0650300714756069\n",
      "DiasABP : 0.23091582098427813\n",
      "FiO2 : 0.3991706722778415\n",
      "GCS : 0.7185448583386655\n",
      "Glucose : 0.08650369868486456\n",
      "HCO3 : 0.3852321568858144\n",
      "HCT : 0.45837122258363244\n",
      "HR : 0.43308079588709447\n",
      "K : 0.1231259733360879\n",
      "Lactate : 0.08446312294354702\n",
      "MAP : 0.28238178257383106\n",
      "MechVent : 0.0\n",
      "Mg : 0.06729442993035666\n",
      "NIDiasABP : 0.3558100276819062\n",
      "NIMAP : 0.34119140114588997\n",
      "NISysABP : 0.46084065178194916\n",
      "Na : 0.5040847747360606\n",
      "PaCO2 : 0.413135592733254\n",
      "PaO2 : 0.26211649705940904\n",
      "Platelets : 0.08722456246764243\n",
      "RespRate : 0.20185009479967506\n",
      "SaO2 : 0.9598484974015699\n",
      "SysABP : 0.43786591119979573\n",
      "Temp : 0.930932478500845\n",
      "TroponinI : 0.1167685004571879\n",
      "TroponinT : 0.019794291230189432\n",
      "Urine : 0.011216486651871244\n",
      "WBC : 0.00209303268241654\n",
      "Weight : 0.2695347559547523\n",
      "Ph : 0.07372282354270693\n",
      "Low Weight\n",
      "-------------\n",
      "ALP : 0.12849462591105235\n",
      "ALT : 0.012010166113029627\n",
      "AST : 0.024315859831397862\n",
      "Albumin : 0.4883721073467712\n",
      "BUN : 0.14350961730814754\n",
      "Bilirubin : 0.13333333833140448\n",
      "Cholesterol : 0.3772455155845684\n",
      "Creatinine : 0.041558441506429664\n",
      "DiasABP : 0.21849725496275818\n",
      "FiO2 : 0.37480704522713465\n",
      "GCS : 0.6744791665114358\n",
      "Glucose : 0.10730193360991634\n",
      "HCO3 : 0.44148936246828535\n",
      "HCT : 0.441090669482928\n",
      "HR : 0.4432283493212445\n",
      "K : 0.11479073487545635\n",
      "Lactate : 0.14709677305071076\n",
      "MAP : 0.28126142255496017\n",
      "MechVent : 0.0\n",
      "Mg : 0.07310344912111028\n",
      "NIDiasABP : 0.34386710988150226\n",
      "NIMAP : 0.3336672643562373\n",
      "NISysABP : 0.46001293121928255\n",
      "Na : 0.4609756022691266\n",
      "PaCO2 : 0.3980882378185379\n",
      "PaO2 : 0.3882368410888369\n",
      "Platelets : 0.07176928933370545\n",
      "RespRate : 0.2191448002343982\n",
      "SaO2 : 0.966071435383319\n",
      "SysABP : 0.4354597494957264\n",
      "Temp : 0.9317392693324417\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.09297658503046274\n",
      "Urine : 0.010726357455874703\n",
      "WBC : 0.0019360268443749731\n",
      "Weight : 0.18868461124977665\n",
      "Ph : 0.07372073211320837\n",
      "Normal Weight\n",
      "-------------\n",
      "ALP : 0.039368713214511526\n",
      "ALT : 0.009226032189048742\n",
      "AST : 0.004763572927003985\n",
      "Albumin : 0.3806609581959675\n",
      "BUN : 0.12807830663652423\n",
      "Bilirubin : 0.055638847113906195\n",
      "Cholesterol : 0.2589820325373308\n",
      "Creatinine : 0.06824583859628831\n",
      "DiasABP : 0.21524044339851928\n",
      "FiO2 : 0.413955667047041\n",
      "GCS : 0.6680555556034821\n",
      "Glucose : 0.08563250735833063\n",
      "HCO3 : 0.38104448964198145\n",
      "HCT : 0.4529393812284801\n",
      "HR : 0.4308102133979586\n",
      "K : 0.12405788306627444\n",
      "Lactate : 0.09353287840405246\n",
      "MAP : 0.25903742372914756\n",
      "MechVent : 0.0\n",
      "Mg : 0.0708158120391809\n",
      "NIDiasABP : 0.3310879640393515\n",
      "NIMAP : 0.32778467119477184\n",
      "NISysABP : 0.4350025314727163\n",
      "Na : 0.49855465072172295\n",
      "PaCO2 : 0.39832764380824304\n",
      "PaO2 : 0.32062610966850924\n",
      "Platelets : 0.07315669132383126\n",
      "RespRate : 0.18697378937607526\n",
      "SaO2 : 0.9706250113936478\n",
      "SysABP : 0.40503043072453104\n",
      "Temp : 0.9330817678865039\n",
      "TroponinI : 0.16994795079030692\n",
      "TroponinT : 0.029558847266125995\n",
      "Urine : 0.009560062119363529\n",
      "WBC : 0.0021119781102228734\n",
      "Weight : 0.2255991635454612\n",
      "Ph : 0.07379047633433802\n",
      "Overweight\n",
      "-------------\n",
      "ALP : 0.032560483858104707\n",
      "ALT : 0.037874907253554795\n",
      "AST : 0.018235912850025973\n",
      "Albumin : 0.47001224050393897\n",
      "BUN : 0.12255973215073707\n",
      "Bilirubin : 0.025541241209591132\n",
      "Cholesterol : 0.5337895665849196\n",
      "Creatinine : 0.05656934261702667\n",
      "DiasABP : 0.2160167390482559\n",
      "FiO2 : 0.41296175382117695\n",
      "GCS : 0.7123519459553366\n",
      "Glucose : 0.08623080851784913\n",
      "HCO3 : 0.37035003448686055\n",
      "HCT : 0.4457667074470856\n",
      "HR : 0.42761740631392875\n",
      "K : 0.13470453986032635\n",
      "Lactate : 0.09630532588722983\n",
      "MAP : 0.2639913092989946\n",
      "MechVent : 0.0\n",
      "Mg : 0.06942368017418353\n",
      "NIDiasABP : 0.33853139641905977\n",
      "NIMAP : 0.3318021753520671\n",
      "NISysABP : 0.439777413632954\n",
      "Na : 0.48860941661728413\n",
      "PaCO2 : 0.40217234198891955\n",
      "PaO2 : 0.30056231935386984\n",
      "Platelets : 0.07274071909602181\n",
      "RespRate : 0.19702470804868044\n",
      "SaO2 : 0.9674803281393501\n",
      "SysABP : 0.41325499514244807\n",
      "Temp : 0.9349696859643665\n",
      "TroponinI : 0.19837066829200592\n",
      "TroponinT : 0.05196809748079801\n",
      "Urine : 0.009661902907219293\n",
      "WBC : 0.0019569994751087063\n",
      "Weight : 0.2785071155491657\n",
      "Ph : 0.07376130929748904\n",
      "Obesity 1\n",
      "-------------\n",
      "ALP : 0.03353814660970414\n",
      "ALT : 0.014711271555278647\n",
      "AST : 0.0105712166172447\n",
      "Albumin : 0.4612403089801086\n",
      "BUN : 0.1408974360674601\n",
      "Bilirubin : 0.04176060141374389\n",
      "Cholesterol : 0.31137725214153084\n",
      "Creatinine : 0.05744400485486141\n",
      "DiasABP : 0.2205008176350557\n",
      "FiO2 : 0.4716148861128851\n",
      "GCS : 0.7014124303045896\n",
      "Glucose : 0.08194478274437275\n",
      "HCO3 : 0.38760407139425135\n",
      "HCT : 0.4602542329313459\n",
      "HR : 0.4346733966175416\n",
      "K : 0.1202567273144966\n",
      "Lactate : 0.10637992877099012\n",
      "MAP : 0.2653687129674014\n",
      "MechVent : 0.0\n",
      "Mg : 0.06773399045424705\n",
      "NIDiasABP : 0.3512441020692524\n",
      "NIMAP : 0.329776092656587\n",
      "NISysABP : 0.45112277082759056\n",
      "Na : 0.48227896355091776\n",
      "PaCO2 : 0.39360605984022884\n",
      "PaO2 : 0.2620813960512695\n",
      "Platelets : 0.08242238771268304\n",
      "RespRate : 0.20034680720053336\n",
      "SaO2 : 0.9677868954470741\n",
      "SysABP : 0.4212843536295819\n",
      "Temp : 0.9368506019974472\n",
      "TroponinI : 0.15274948906145802\n",
      "TroponinT : 0.0374581942054282\n",
      "Urine : 0.01090334983193011\n",
      "WBC : 0.0025744304885385043\n",
      "Weight : 0.3259233741311846\n",
      "Ph : 0.07897253280411841\n",
      "Obesity 2\n",
      "-------------\n",
      "ALP : 0.019912385278275713\n",
      "ALT : 0.045586956157675156\n",
      "AST : 0.002252994900723789\n",
      "Albumin : 0.4069767445325173\n",
      "BUN : 0.1495037224263866\n",
      "Bilirubin : 0.02648058753563832\n",
      "Cholesterol : 0.4071856141086321\n",
      "Creatinine : 0.05939393937587541\n",
      "DiasABP : 0.22219662335663795\n",
      "FiO2 : 0.4440200834952475\n",
      "GCS : 0.731250000745052\n",
      "Glucose : 0.08238279026361939\n",
      "HCO3 : 0.42234042584893977\n",
      "HCT : 0.4684399468333974\n",
      "HR : 0.44575138935517766\n",
      "K : 0.12257368833972387\n",
      "Lactate : 0.09516128884361644\n",
      "MAP : 0.26644822472675794\n",
      "MechVent : 0.0\n",
      "Mg : 0.06717752244461217\n",
      "NIDiasABP : 0.341124487307033\n",
      "NIMAP : 0.33253627719769696\n",
      "NISysABP : 0.4309972355311535\n",
      "Na : 0.5007390957889983\n",
      "PaCO2 : 0.39861538456035883\n",
      "PaO2 : 0.30098461726536657\n",
      "Platelets : 0.07418182979409227\n",
      "RespRate : 0.21115491844035883\n",
      "SaO2 : 0.962777776850541\n",
      "SysABP : 0.40635532188415363\n",
      "Temp : 0.9350564766451258\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.16755852320536352\n",
      "Urine : 0.009949277467533777\n",
      "WBC : 0.002079883853213855\n",
      "Weight : 0.3575346656953492\n",
      "Ph : 0.07353928578751297\n",
      "Obesity 3\n",
      "-------------\n",
      "ALP : 0.03134571515362086\n",
      "ALT : 0.099730087094907\n",
      "AST : 0.0011333662696412724\n",
      "Albumin : 0.39069767594329646\n",
      "BUN : 0.14148351745235815\n",
      "Bilirubin : 0.03569511541475257\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.07653958891187698\n",
      "DiasABP : 0.2212343264322769\n",
      "FiO2 : 0.47952891190300406\n",
      "GCS : 0.7373633887313369\n",
      "Glucose : 0.10372847143341207\n",
      "HCO3 : 0.35607008635996723\n",
      "HCT : 0.45750317340942753\n",
      "HR : 0.43886712589724397\n",
      "K : 0.13271028076608535\n",
      "Lactate : 0.11043906796516337\n",
      "MAP : 0.2638823023620035\n",
      "MechVent : 0.0\n",
      "Mg : 0.07024630579565051\n",
      "NIDiasABP : 0.34011312946676947\n",
      "NIMAP : 0.33154343944546116\n",
      "NISysABP : 0.4360485397505005\n",
      "Na : 0.46818663762961304\n",
      "PaCO2 : 0.4118604628845655\n",
      "PaO2 : 0.24244194795910962\n",
      "Platelets : 0.07038423692574504\n",
      "RespRate : 0.2020030247944298\n",
      "SaO2 : 0.9603703794655621\n",
      "SysABP : 0.412675698881046\n",
      "Temp : 0.9374104923608726\n",
      "TroponinI : 0.14052952826009046\n",
      "TroponinT : 0.1296321103116238\n",
      "Urine : 0.009860508063457502\n",
      "WBC : 0.001903906057123037\n",
      "Weight : 0.4232498811158732\n",
      "Ph : 0.07365277792430484\n"
     ]
    }
   ],
   "source": [
    "print(\"SAITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_saits_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saits_mae_minmax = toolkits.create_table(testing_mae_saits_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>General</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Undefined Gender</th>\n",
       "      <th>+65</th>\n",
       "      <th>-65</th>\n",
       "      <th>ICUType 1</th>\n",
       "      <th>ICUType 2</th>\n",
       "      <th>ICUType 3</th>\n",
       "      <th>ICUType 4</th>\n",
       "      <th>Undefined classification</th>\n",
       "      <th>Low Weight</th>\n",
       "      <th>Normal Weight</th>\n",
       "      <th>Overweight</th>\n",
       "      <th>Obesity 1</th>\n",
       "      <th>Obesity 2</th>\n",
       "      <th>Obesity 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALP</td>\n",
       "      <td>0.040220</td>\n",
       "      <td>0.041253</td>\n",
       "      <td>0.042669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045370</td>\n",
       "      <td>0.058368</td>\n",
       "      <td>0.029260</td>\n",
       "      <td>0.018007</td>\n",
       "      <td>0.063298</td>\n",
       "      <td>0.040524</td>\n",
       "      <td>0.036990</td>\n",
       "      <td>0.128495</td>\n",
       "      <td>0.039369</td>\n",
       "      <td>0.032560</td>\n",
       "      <td>0.033538</td>\n",
       "      <td>0.019912</td>\n",
       "      <td>0.031346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALT</td>\n",
       "      <td>0.017230</td>\n",
       "      <td>0.006941</td>\n",
       "      <td>0.016567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012877</td>\n",
       "      <td>0.023744</td>\n",
       "      <td>0.018795</td>\n",
       "      <td>0.029335</td>\n",
       "      <td>0.023352</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>0.017182</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>0.009226</td>\n",
       "      <td>0.037875</td>\n",
       "      <td>0.014711</td>\n",
       "      <td>0.045587</td>\n",
       "      <td>0.099730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AST</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.017081</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006469</td>\n",
       "      <td>0.011712</td>\n",
       "      <td>0.018681</td>\n",
       "      <td>0.008078</td>\n",
       "      <td>0.013382</td>\n",
       "      <td>0.017210</td>\n",
       "      <td>0.007568</td>\n",
       "      <td>0.024316</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.018236</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.001133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albumin</td>\n",
       "      <td>0.453739</td>\n",
       "      <td>0.423997</td>\n",
       "      <td>0.450085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428799</td>\n",
       "      <td>0.450342</td>\n",
       "      <td>0.544186</td>\n",
       "      <td>0.392027</td>\n",
       "      <td>0.419730</td>\n",
       "      <td>0.411677</td>\n",
       "      <td>0.404592</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.380661</td>\n",
       "      <td>0.470012</td>\n",
       "      <td>0.461240</td>\n",
       "      <td>0.406977</td>\n",
       "      <td>0.390698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUN</td>\n",
       "      <td>0.119175</td>\n",
       "      <td>0.115023</td>\n",
       "      <td>0.134813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147150</td>\n",
       "      <td>0.102920</td>\n",
       "      <td>0.136926</td>\n",
       "      <td>0.092971</td>\n",
       "      <td>0.158645</td>\n",
       "      <td>0.091490</td>\n",
       "      <td>0.125755</td>\n",
       "      <td>0.143510</td>\n",
       "      <td>0.128078</td>\n",
       "      <td>0.122560</td>\n",
       "      <td>0.140897</td>\n",
       "      <td>0.149504</td>\n",
       "      <td>0.141484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bilirubin</td>\n",
       "      <td>0.044611</td>\n",
       "      <td>0.034648</td>\n",
       "      <td>0.036118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026320</td>\n",
       "      <td>0.053072</td>\n",
       "      <td>0.018808</td>\n",
       "      <td>0.037440</td>\n",
       "      <td>0.039132</td>\n",
       "      <td>0.045804</td>\n",
       "      <td>0.044419</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.055639</td>\n",
       "      <td>0.025541</td>\n",
       "      <td>0.041761</td>\n",
       "      <td>0.026481</td>\n",
       "      <td>0.035695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cholesterol</td>\n",
       "      <td>0.387126</td>\n",
       "      <td>0.473054</td>\n",
       "      <td>0.389912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363024</td>\n",
       "      <td>0.238024</td>\n",
       "      <td>0.430447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300150</td>\n",
       "      <td>0.356287</td>\n",
       "      <td>0.427146</td>\n",
       "      <td>0.377246</td>\n",
       "      <td>0.258982</td>\n",
       "      <td>0.533790</td>\n",
       "      <td>0.311377</td>\n",
       "      <td>0.407186</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Creatinine</td>\n",
       "      <td>0.063306</td>\n",
       "      <td>0.048783</td>\n",
       "      <td>0.072079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057820</td>\n",
       "      <td>0.060337</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.051091</td>\n",
       "      <td>0.086067</td>\n",
       "      <td>0.046976</td>\n",
       "      <td>0.065030</td>\n",
       "      <td>0.041558</td>\n",
       "      <td>0.068246</td>\n",
       "      <td>0.056569</td>\n",
       "      <td>0.057444</td>\n",
       "      <td>0.059394</td>\n",
       "      <td>0.076540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DiasABP</td>\n",
       "      <td>0.222288</td>\n",
       "      <td>0.221258</td>\n",
       "      <td>0.223552</td>\n",
       "      <td>0.229701</td>\n",
       "      <td>0.212173</td>\n",
       "      <td>0.235335</td>\n",
       "      <td>0.217357</td>\n",
       "      <td>0.208378</td>\n",
       "      <td>0.226277</td>\n",
       "      <td>0.231468</td>\n",
       "      <td>0.230916</td>\n",
       "      <td>0.218497</td>\n",
       "      <td>0.215240</td>\n",
       "      <td>0.216017</td>\n",
       "      <td>0.220501</td>\n",
       "      <td>0.222197</td>\n",
       "      <td>0.221234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FiO2</td>\n",
       "      <td>0.421254</td>\n",
       "      <td>0.407955</td>\n",
       "      <td>0.441474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431625</td>\n",
       "      <td>0.422815</td>\n",
       "      <td>0.415464</td>\n",
       "      <td>0.454358</td>\n",
       "      <td>0.438086</td>\n",
       "      <td>0.399443</td>\n",
       "      <td>0.399171</td>\n",
       "      <td>0.374807</td>\n",
       "      <td>0.413956</td>\n",
       "      <td>0.412962</td>\n",
       "      <td>0.471615</td>\n",
       "      <td>0.444020</td>\n",
       "      <td>0.479529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GCS</td>\n",
       "      <td>0.701379</td>\n",
       "      <td>0.702391</td>\n",
       "      <td>0.703686</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.710729</td>\n",
       "      <td>0.705676</td>\n",
       "      <td>0.808766</td>\n",
       "      <td>0.720533</td>\n",
       "      <td>0.716312</td>\n",
       "      <td>0.657993</td>\n",
       "      <td>0.718545</td>\n",
       "      <td>0.674479</td>\n",
       "      <td>0.668056</td>\n",
       "      <td>0.712352</td>\n",
       "      <td>0.701412</td>\n",
       "      <td>0.731250</td>\n",
       "      <td>0.737363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.087050</td>\n",
       "      <td>0.087087</td>\n",
       "      <td>0.089907</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.092124</td>\n",
       "      <td>0.090579</td>\n",
       "      <td>0.090043</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.089611</td>\n",
       "      <td>0.083428</td>\n",
       "      <td>0.086504</td>\n",
       "      <td>0.107302</td>\n",
       "      <td>0.085633</td>\n",
       "      <td>0.086231</td>\n",
       "      <td>0.081945</td>\n",
       "      <td>0.082383</td>\n",
       "      <td>0.103728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HCO3</td>\n",
       "      <td>0.382952</td>\n",
       "      <td>0.387424</td>\n",
       "      <td>0.391574</td>\n",
       "      <td>0.414894</td>\n",
       "      <td>0.382507</td>\n",
       "      <td>0.373911</td>\n",
       "      <td>0.389569</td>\n",
       "      <td>0.386679</td>\n",
       "      <td>0.368478</td>\n",
       "      <td>0.384075</td>\n",
       "      <td>0.385232</td>\n",
       "      <td>0.441489</td>\n",
       "      <td>0.381044</td>\n",
       "      <td>0.370350</td>\n",
       "      <td>0.387604</td>\n",
       "      <td>0.422340</td>\n",
       "      <td>0.356070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HCT</td>\n",
       "      <td>0.450794</td>\n",
       "      <td>0.448553</td>\n",
       "      <td>0.452991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.454988</td>\n",
       "      <td>0.446982</td>\n",
       "      <td>0.489168</td>\n",
       "      <td>0.429623</td>\n",
       "      <td>0.458380</td>\n",
       "      <td>0.455241</td>\n",
       "      <td>0.458371</td>\n",
       "      <td>0.441091</td>\n",
       "      <td>0.452939</td>\n",
       "      <td>0.445767</td>\n",
       "      <td>0.460254</td>\n",
       "      <td>0.468440</td>\n",
       "      <td>0.457503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HR</td>\n",
       "      <td>0.433478</td>\n",
       "      <td>0.435435</td>\n",
       "      <td>0.431561</td>\n",
       "      <td>0.365938</td>\n",
       "      <td>0.418700</td>\n",
       "      <td>0.448810</td>\n",
       "      <td>0.407822</td>\n",
       "      <td>0.429543</td>\n",
       "      <td>0.448494</td>\n",
       "      <td>0.434668</td>\n",
       "      <td>0.433081</td>\n",
       "      <td>0.443228</td>\n",
       "      <td>0.430810</td>\n",
       "      <td>0.427617</td>\n",
       "      <td>0.434673</td>\n",
       "      <td>0.445751</td>\n",
       "      <td>0.438867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>K</td>\n",
       "      <td>0.123185</td>\n",
       "      <td>0.119226</td>\n",
       "      <td>0.126711</td>\n",
       "      <td>0.144860</td>\n",
       "      <td>0.122699</td>\n",
       "      <td>0.122915</td>\n",
       "      <td>0.124399</td>\n",
       "      <td>0.139057</td>\n",
       "      <td>0.123058</td>\n",
       "      <td>0.119384</td>\n",
       "      <td>0.123126</td>\n",
       "      <td>0.114791</td>\n",
       "      <td>0.124058</td>\n",
       "      <td>0.134705</td>\n",
       "      <td>0.120257</td>\n",
       "      <td>0.122574</td>\n",
       "      <td>0.132710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lactate</td>\n",
       "      <td>0.089269</td>\n",
       "      <td>0.085730</td>\n",
       "      <td>0.093467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085161</td>\n",
       "      <td>0.085434</td>\n",
       "      <td>0.104579</td>\n",
       "      <td>0.098756</td>\n",
       "      <td>0.101091</td>\n",
       "      <td>0.093026</td>\n",
       "      <td>0.084463</td>\n",
       "      <td>0.147097</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.096305</td>\n",
       "      <td>0.106380</td>\n",
       "      <td>0.095161</td>\n",
       "      <td>0.110439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MAP</td>\n",
       "      <td>0.269496</td>\n",
       "      <td>0.270974</td>\n",
       "      <td>0.268806</td>\n",
       "      <td>0.255034</td>\n",
       "      <td>0.263121</td>\n",
       "      <td>0.277042</td>\n",
       "      <td>0.270043</td>\n",
       "      <td>0.252055</td>\n",
       "      <td>0.274454</td>\n",
       "      <td>0.284905</td>\n",
       "      <td>0.282382</td>\n",
       "      <td>0.281261</td>\n",
       "      <td>0.259037</td>\n",
       "      <td>0.263991</td>\n",
       "      <td>0.265369</td>\n",
       "      <td>0.266448</td>\n",
       "      <td>0.263882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MechVent</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mg</td>\n",
       "      <td>0.069376</td>\n",
       "      <td>0.068007</td>\n",
       "      <td>0.071153</td>\n",
       "      <td>0.062069</td>\n",
       "      <td>0.071280</td>\n",
       "      <td>0.068294</td>\n",
       "      <td>0.070563</td>\n",
       "      <td>0.074278</td>\n",
       "      <td>0.069816</td>\n",
       "      <td>0.066959</td>\n",
       "      <td>0.067294</td>\n",
       "      <td>0.073103</td>\n",
       "      <td>0.070816</td>\n",
       "      <td>0.069424</td>\n",
       "      <td>0.067734</td>\n",
       "      <td>0.067178</td>\n",
       "      <td>0.070246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NIDiasABP</td>\n",
       "      <td>0.349686</td>\n",
       "      <td>0.340335</td>\n",
       "      <td>0.357820</td>\n",
       "      <td>0.371324</td>\n",
       "      <td>0.328478</td>\n",
       "      <td>0.371481</td>\n",
       "      <td>0.342389</td>\n",
       "      <td>0.314011</td>\n",
       "      <td>0.353501</td>\n",
       "      <td>0.355791</td>\n",
       "      <td>0.355810</td>\n",
       "      <td>0.343867</td>\n",
       "      <td>0.331088</td>\n",
       "      <td>0.338531</td>\n",
       "      <td>0.351244</td>\n",
       "      <td>0.341124</td>\n",
       "      <td>0.340113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NIMAP</td>\n",
       "      <td>0.340922</td>\n",
       "      <td>0.332572</td>\n",
       "      <td>0.343372</td>\n",
       "      <td>0.347939</td>\n",
       "      <td>0.327636</td>\n",
       "      <td>0.348441</td>\n",
       "      <td>0.325528</td>\n",
       "      <td>0.312257</td>\n",
       "      <td>0.341421</td>\n",
       "      <td>0.349796</td>\n",
       "      <td>0.341191</td>\n",
       "      <td>0.333667</td>\n",
       "      <td>0.327785</td>\n",
       "      <td>0.331802</td>\n",
       "      <td>0.329776</td>\n",
       "      <td>0.332536</td>\n",
       "      <td>0.331543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NISysABP</td>\n",
       "      <td>0.454291</td>\n",
       "      <td>0.453459</td>\n",
       "      <td>0.450481</td>\n",
       "      <td>0.366920</td>\n",
       "      <td>0.453587</td>\n",
       "      <td>0.456665</td>\n",
       "      <td>0.430975</td>\n",
       "      <td>0.423754</td>\n",
       "      <td>0.456270</td>\n",
       "      <td>0.479111</td>\n",
       "      <td>0.460841</td>\n",
       "      <td>0.460013</td>\n",
       "      <td>0.435003</td>\n",
       "      <td>0.439777</td>\n",
       "      <td>0.451123</td>\n",
       "      <td>0.430997</td>\n",
       "      <td>0.436049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Na</td>\n",
       "      <td>0.498652</td>\n",
       "      <td>0.499902</td>\n",
       "      <td>0.497247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501271</td>\n",
       "      <td>0.496648</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.487982</td>\n",
       "      <td>0.498093</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.504085</td>\n",
       "      <td>0.460976</td>\n",
       "      <td>0.498555</td>\n",
       "      <td>0.488609</td>\n",
       "      <td>0.482279</td>\n",
       "      <td>0.500739</td>\n",
       "      <td>0.468187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PaCO2</td>\n",
       "      <td>0.409528</td>\n",
       "      <td>0.403246</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397200</td>\n",
       "      <td>0.409654</td>\n",
       "      <td>0.396102</td>\n",
       "      <td>0.405111</td>\n",
       "      <td>0.418361</td>\n",
       "      <td>0.395453</td>\n",
       "      <td>0.413136</td>\n",
       "      <td>0.398088</td>\n",
       "      <td>0.398328</td>\n",
       "      <td>0.402172</td>\n",
       "      <td>0.393606</td>\n",
       "      <td>0.398615</td>\n",
       "      <td>0.411860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PaO2</td>\n",
       "      <td>0.296584</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>0.296290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295316</td>\n",
       "      <td>0.309625</td>\n",
       "      <td>0.307167</td>\n",
       "      <td>0.337521</td>\n",
       "      <td>0.245958</td>\n",
       "      <td>0.291143</td>\n",
       "      <td>0.262116</td>\n",
       "      <td>0.388237</td>\n",
       "      <td>0.320626</td>\n",
       "      <td>0.300562</td>\n",
       "      <td>0.262081</td>\n",
       "      <td>0.300985</td>\n",
       "      <td>0.242442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Platelets</td>\n",
       "      <td>0.081192</td>\n",
       "      <td>0.087750</td>\n",
       "      <td>0.078723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080557</td>\n",
       "      <td>0.083157</td>\n",
       "      <td>0.090904</td>\n",
       "      <td>0.069212</td>\n",
       "      <td>0.078392</td>\n",
       "      <td>0.081018</td>\n",
       "      <td>0.087225</td>\n",
       "      <td>0.071769</td>\n",
       "      <td>0.073157</td>\n",
       "      <td>0.072741</td>\n",
       "      <td>0.082422</td>\n",
       "      <td>0.074182</td>\n",
       "      <td>0.070384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RespRate</td>\n",
       "      <td>0.198284</td>\n",
       "      <td>0.200543</td>\n",
       "      <td>0.194730</td>\n",
       "      <td>0.201531</td>\n",
       "      <td>0.204033</td>\n",
       "      <td>0.191181</td>\n",
       "      <td>0.197067</td>\n",
       "      <td>0.204896</td>\n",
       "      <td>0.202178</td>\n",
       "      <td>0.193269</td>\n",
       "      <td>0.201850</td>\n",
       "      <td>0.219145</td>\n",
       "      <td>0.186974</td>\n",
       "      <td>0.197025</td>\n",
       "      <td>0.200347</td>\n",
       "      <td>0.211155</td>\n",
       "      <td>0.202003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SaO2</td>\n",
       "      <td>0.968509</td>\n",
       "      <td>0.961742</td>\n",
       "      <td>0.967570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.967520</td>\n",
       "      <td>0.960361</td>\n",
       "      <td>0.964684</td>\n",
       "      <td>0.969473</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>0.965714</td>\n",
       "      <td>0.959848</td>\n",
       "      <td>0.966071</td>\n",
       "      <td>0.970625</td>\n",
       "      <td>0.967480</td>\n",
       "      <td>0.967787</td>\n",
       "      <td>0.962778</td>\n",
       "      <td>0.960370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SysABP</td>\n",
       "      <td>0.421485</td>\n",
       "      <td>0.422280</td>\n",
       "      <td>0.424167</td>\n",
       "      <td>0.351023</td>\n",
       "      <td>0.423496</td>\n",
       "      <td>0.421346</td>\n",
       "      <td>0.402937</td>\n",
       "      <td>0.402159</td>\n",
       "      <td>0.421734</td>\n",
       "      <td>0.447843</td>\n",
       "      <td>0.437866</td>\n",
       "      <td>0.435460</td>\n",
       "      <td>0.405030</td>\n",
       "      <td>0.413255</td>\n",
       "      <td>0.421284</td>\n",
       "      <td>0.406355</td>\n",
       "      <td>0.412676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Temp</td>\n",
       "      <td>0.933518</td>\n",
       "      <td>0.931850</td>\n",
       "      <td>0.933752</td>\n",
       "      <td>0.934524</td>\n",
       "      <td>0.931835</td>\n",
       "      <td>0.934743</td>\n",
       "      <td>0.930336</td>\n",
       "      <td>0.936718</td>\n",
       "      <td>0.927487</td>\n",
       "      <td>0.932739</td>\n",
       "      <td>0.930932</td>\n",
       "      <td>0.931739</td>\n",
       "      <td>0.933082</td>\n",
       "      <td>0.934970</td>\n",
       "      <td>0.936851</td>\n",
       "      <td>0.935056</td>\n",
       "      <td>0.937410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TroponinI</td>\n",
       "      <td>0.165173</td>\n",
       "      <td>0.099389</td>\n",
       "      <td>0.110659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261202</td>\n",
       "      <td>0.029532</td>\n",
       "      <td>0.378055</td>\n",
       "      <td>0.205295</td>\n",
       "      <td>0.052953</td>\n",
       "      <td>0.150713</td>\n",
       "      <td>0.116769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169948</td>\n",
       "      <td>0.198371</td>\n",
       "      <td>0.152749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TroponinT</td>\n",
       "      <td>0.028798</td>\n",
       "      <td>0.034470</td>\n",
       "      <td>0.037960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031759</td>\n",
       "      <td>0.027938</td>\n",
       "      <td>0.081064</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.014245</td>\n",
       "      <td>0.014967</td>\n",
       "      <td>0.019794</td>\n",
       "      <td>0.092977</td>\n",
       "      <td>0.029559</td>\n",
       "      <td>0.051968</td>\n",
       "      <td>0.037458</td>\n",
       "      <td>0.167559</td>\n",
       "      <td>0.129632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Urine</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.009314</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>0.012727</td>\n",
       "      <td>0.009262</td>\n",
       "      <td>0.012570</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.009957</td>\n",
       "      <td>0.010544</td>\n",
       "      <td>0.011103</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.010726</td>\n",
       "      <td>0.009560</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.010903</td>\n",
       "      <td>0.009949</td>\n",
       "      <td>0.009861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WBC</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.001904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Weight</td>\n",
       "      <td>0.278176</td>\n",
       "      <td>0.248731</td>\n",
       "      <td>0.297465</td>\n",
       "      <td>0.334109</td>\n",
       "      <td>0.261915</td>\n",
       "      <td>0.291088</td>\n",
       "      <td>0.275164</td>\n",
       "      <td>0.289872</td>\n",
       "      <td>0.273112</td>\n",
       "      <td>0.271815</td>\n",
       "      <td>0.269535</td>\n",
       "      <td>0.188685</td>\n",
       "      <td>0.225599</td>\n",
       "      <td>0.278507</td>\n",
       "      <td>0.325923</td>\n",
       "      <td>0.357535</td>\n",
       "      <td>0.423250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ph</td>\n",
       "      <td>0.074389</td>\n",
       "      <td>0.073736</td>\n",
       "      <td>0.073762</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.073760</td>\n",
       "      <td>0.073708</td>\n",
       "      <td>0.073885</td>\n",
       "      <td>0.073798</td>\n",
       "      <td>0.073533</td>\n",
       "      <td>0.073801</td>\n",
       "      <td>0.073723</td>\n",
       "      <td>0.073721</td>\n",
       "      <td>0.073790</td>\n",
       "      <td>0.073761</td>\n",
       "      <td>0.078973</td>\n",
       "      <td>0.073539</td>\n",
       "      <td>0.073653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0   General    Female      Male  Undefined Gender       +65  \\\n",
       "0           ALP  0.040220  0.041253  0.042669          0.000000  0.045370   \n",
       "1           ALT  0.017230  0.006941  0.016567          0.000000  0.012877   \n",
       "2           AST  0.015469  0.017081  0.006773          0.000000  0.006469   \n",
       "3       Albumin  0.453739  0.423997  0.450085          0.000000  0.428799   \n",
       "4           BUN  0.119175  0.115023  0.134813          0.000000  0.147150   \n",
       "5     Bilirubin  0.044611  0.034648  0.036118          0.000000  0.026320   \n",
       "6   Cholesterol  0.387126  0.473054  0.389912          0.000000  0.363024   \n",
       "7    Creatinine  0.063306  0.048783  0.072079          0.000000  0.057820   \n",
       "8       DiasABP  0.222288  0.221258  0.223552          0.229701  0.212173   \n",
       "9          FiO2  0.421254  0.407955  0.441474          0.000000  0.431625   \n",
       "10          GCS  0.701379  0.702391  0.703686          0.937500  0.710729   \n",
       "11      Glucose  0.087050  0.087087  0.089907          0.084600  0.092124   \n",
       "12         HCO3  0.382952  0.387424  0.391574          0.414894  0.382507   \n",
       "13          HCT  0.450794  0.448553  0.452991          0.000000  0.454988   \n",
       "14           HR  0.433478  0.435435  0.431561          0.365938  0.418700   \n",
       "15            K  0.123185  0.119226  0.126711          0.144860  0.122699   \n",
       "16      Lactate  0.089269  0.085730  0.093467          0.000000  0.085161   \n",
       "17          MAP  0.269496  0.270974  0.268806          0.255034  0.263121   \n",
       "18     MechVent  0.000000  0.000000  0.000000          0.000000  0.000000   \n",
       "19           Mg  0.069376  0.068007  0.071153          0.062069  0.071280   \n",
       "20    NIDiasABP  0.349686  0.340335  0.357820          0.371324  0.328478   \n",
       "21        NIMAP  0.340922  0.332572  0.343372          0.347939  0.327636   \n",
       "22     NISysABP  0.454291  0.453459  0.450481          0.366920  0.453587   \n",
       "23           Na  0.498652  0.499902  0.497247          0.000000  0.501271   \n",
       "24        PaCO2  0.409528  0.403246  0.407800          0.000000  0.397200   \n",
       "25         PaO2  0.296584  0.293945  0.296290          0.000000  0.295316   \n",
       "26    Platelets  0.081192  0.087750  0.078723          0.000000  0.080557   \n",
       "27     RespRate  0.198284  0.200543  0.194730          0.201531  0.204033   \n",
       "28         SaO2  0.968509  0.961742  0.967570          0.000000  0.967520   \n",
       "29       SysABP  0.421485  0.422280  0.424167          0.351023  0.423496   \n",
       "30         Temp  0.933518  0.931850  0.933752          0.934524  0.931835   \n",
       "31    TroponinI  0.165173  0.099389  0.110659          0.000000  0.261202   \n",
       "32    TroponinT  0.028798  0.034470  0.037960          0.000000  0.031759   \n",
       "33        Urine  0.010554  0.009314  0.011215          0.012727  0.009262   \n",
       "34          WBC  0.001962  0.001987  0.002097          0.001735  0.002192   \n",
       "35       Weight  0.278176  0.248731  0.297465          0.334109  0.261915   \n",
       "36           Ph  0.074389  0.073736  0.073762          0.074600  0.073760   \n",
       "\n",
       "         -65  ICUType 1  ICUType 2  ICUType 3  ICUType 4  \\\n",
       "0   0.058368   0.029260   0.018007   0.063298   0.040524   \n",
       "1   0.023744   0.018795   0.029335   0.023352   0.016529   \n",
       "2   0.011712   0.018681   0.008078   0.013382   0.017210   \n",
       "3   0.450342   0.544186   0.392027   0.419730   0.411677   \n",
       "4   0.102920   0.136926   0.092971   0.158645   0.091490   \n",
       "5   0.053072   0.018808   0.037440   0.039132   0.045804   \n",
       "6   0.238024   0.430447   0.000000   0.300150   0.356287   \n",
       "7   0.060337   0.079545   0.051091   0.086067   0.046976   \n",
       "8   0.235335   0.217357   0.208378   0.226277   0.231468   \n",
       "9   0.422815   0.415464   0.454358   0.438086   0.399443   \n",
       "10  0.705676   0.808766   0.720533   0.716312   0.657993   \n",
       "11  0.090579   0.090043   0.080425   0.089611   0.083428   \n",
       "12  0.373911   0.389569   0.386679   0.368478   0.384075   \n",
       "13  0.446982   0.489168   0.429623   0.458380   0.455241   \n",
       "14  0.448810   0.407822   0.429543   0.448494   0.434668   \n",
       "15  0.122915   0.124399   0.139057   0.123058   0.119384   \n",
       "16  0.085434   0.104579   0.098756   0.101091   0.093026   \n",
       "17  0.277042   0.270043   0.252055   0.274454   0.284905   \n",
       "18  0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "19  0.068294   0.070563   0.074278   0.069816   0.066959   \n",
       "20  0.371481   0.342389   0.314011   0.353501   0.355791   \n",
       "21  0.348441   0.325528   0.312257   0.341421   0.349796   \n",
       "22  0.456665   0.430975   0.423754   0.456270   0.479111   \n",
       "23  0.496648   0.492000   0.487982   0.498093   0.504500   \n",
       "24  0.409654   0.396102   0.405111   0.418361   0.395453   \n",
       "25  0.309625   0.307167   0.337521   0.245958   0.291143   \n",
       "26  0.083157   0.090904   0.069212   0.078392   0.081018   \n",
       "27  0.191181   0.197067   0.204896   0.202178   0.193269   \n",
       "28  0.960361   0.964684   0.969473   0.919000   0.965714   \n",
       "29  0.421346   0.402937   0.402159   0.421734   0.447843   \n",
       "30  0.934743   0.930336   0.936718   0.927487   0.932739   \n",
       "31  0.029532   0.378055   0.205295   0.052953   0.150713   \n",
       "32  0.027938   0.081064   0.012560   0.014245   0.014967   \n",
       "33  0.012570   0.012451   0.009957   0.010544   0.011103   \n",
       "34  0.001932   0.001893   0.002244   0.001973   0.001973   \n",
       "35  0.291088   0.275164   0.289872   0.273112   0.271815   \n",
       "36  0.073708   0.073885   0.073798   0.073533   0.073801   \n",
       "\n",
       "    Undefined classification  Low Weight  Normal Weight  Overweight  \\\n",
       "0                   0.036990    0.128495       0.039369    0.032560   \n",
       "1                   0.017182    0.012010       0.009226    0.037875   \n",
       "2                   0.007568    0.024316       0.004764    0.018236   \n",
       "3                   0.404592    0.488372       0.380661    0.470012   \n",
       "4                   0.125755    0.143510       0.128078    0.122560   \n",
       "5                   0.044419    0.133333       0.055639    0.025541   \n",
       "6                   0.427146    0.377246       0.258982    0.533790   \n",
       "7                   0.065030    0.041558       0.068246    0.056569   \n",
       "8                   0.230916    0.218497       0.215240    0.216017   \n",
       "9                   0.399171    0.374807       0.413956    0.412962   \n",
       "10                  0.718545    0.674479       0.668056    0.712352   \n",
       "11                  0.086504    0.107302       0.085633    0.086231   \n",
       "12                  0.385232    0.441489       0.381044    0.370350   \n",
       "13                  0.458371    0.441091       0.452939    0.445767   \n",
       "14                  0.433081    0.443228       0.430810    0.427617   \n",
       "15                  0.123126    0.114791       0.124058    0.134705   \n",
       "16                  0.084463    0.147097       0.093533    0.096305   \n",
       "17                  0.282382    0.281261       0.259037    0.263991   \n",
       "18                  0.000000    0.000000       0.000000    0.000000   \n",
       "19                  0.067294    0.073103       0.070816    0.069424   \n",
       "20                  0.355810    0.343867       0.331088    0.338531   \n",
       "21                  0.341191    0.333667       0.327785    0.331802   \n",
       "22                  0.460841    0.460013       0.435003    0.439777   \n",
       "23                  0.504085    0.460976       0.498555    0.488609   \n",
       "24                  0.413136    0.398088       0.398328    0.402172   \n",
       "25                  0.262116    0.388237       0.320626    0.300562   \n",
       "26                  0.087225    0.071769       0.073157    0.072741   \n",
       "27                  0.201850    0.219145       0.186974    0.197025   \n",
       "28                  0.959848    0.966071       0.970625    0.967480   \n",
       "29                  0.437866    0.435460       0.405030    0.413255   \n",
       "30                  0.930932    0.931739       0.933082    0.934970   \n",
       "31                  0.116769    0.000000       0.169948    0.198371   \n",
       "32                  0.019794    0.092977       0.029559    0.051968   \n",
       "33                  0.011216    0.010726       0.009560    0.009662   \n",
       "34                  0.002093    0.001936       0.002112    0.001957   \n",
       "35                  0.269535    0.188685       0.225599    0.278507   \n",
       "36                  0.073723    0.073721       0.073790    0.073761   \n",
       "\n",
       "    Obesity 1  Obesity 2  Obesity 3  \n",
       "0    0.033538   0.019912   0.031346  \n",
       "1    0.014711   0.045587   0.099730  \n",
       "2    0.010571   0.002253   0.001133  \n",
       "3    0.461240   0.406977   0.390698  \n",
       "4    0.140897   0.149504   0.141484  \n",
       "5    0.041761   0.026481   0.035695  \n",
       "6    0.311377   0.407186   0.000000  \n",
       "7    0.057444   0.059394   0.076540  \n",
       "8    0.220501   0.222197   0.221234  \n",
       "9    0.471615   0.444020   0.479529  \n",
       "10   0.701412   0.731250   0.737363  \n",
       "11   0.081945   0.082383   0.103728  \n",
       "12   0.387604   0.422340   0.356070  \n",
       "13   0.460254   0.468440   0.457503  \n",
       "14   0.434673   0.445751   0.438867  \n",
       "15   0.120257   0.122574   0.132710  \n",
       "16   0.106380   0.095161   0.110439  \n",
       "17   0.265369   0.266448   0.263882  \n",
       "18   0.000000   0.000000   0.000000  \n",
       "19   0.067734   0.067178   0.070246  \n",
       "20   0.351244   0.341124   0.340113  \n",
       "21   0.329776   0.332536   0.331543  \n",
       "22   0.451123   0.430997   0.436049  \n",
       "23   0.482279   0.500739   0.468187  \n",
       "24   0.393606   0.398615   0.411860  \n",
       "25   0.262081   0.300985   0.242442  \n",
       "26   0.082422   0.074182   0.070384  \n",
       "27   0.200347   0.211155   0.202003  \n",
       "28   0.967787   0.962778   0.960370  \n",
       "29   0.421284   0.406355   0.412676  \n",
       "30   0.936851   0.935056   0.937410  \n",
       "31   0.152749   0.000000   0.140530  \n",
       "32   0.037458   0.167559   0.129632  \n",
       "33   0.010903   0.009949   0.009861  \n",
       "34   0.002574   0.002080   0.001904  \n",
       "35   0.325923   0.357535   0.423250  \n",
       "36   0.078973   0.073539   0.073653  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saits_mae_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "Female\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "Male\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "Undefined Gender\n",
      "TroponinT\n",
      "0.0\n",
      "--------------------\n",
      "+65\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "-65\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "ICUType 1\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "ICUType 2\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "ICUType 3\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "ICUType 4\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "Undefined classification\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "Low Weight\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Normal Weight\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "Overweight\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "Obesity 1\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n",
      "Obesity 2\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Obesity 3\n",
      "MechVent\n",
      "0.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.min_value_in_subgroup(df_saits_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Maximum MAE value in each subgroup </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "SaO2\n",
      "0.9685085946654024\n",
      "--------------------\n",
      "Female\n",
      "SaO2\n",
      "0.9617415842380362\n",
      "--------------------\n",
      "Male\n",
      "SaO2\n",
      "0.9675699026353866\n",
      "--------------------\n",
      "Undefined Gender\n",
      "GCS\n",
      "0.9375000149009268\n",
      "--------------------\n",
      "+65\n",
      "SaO2\n",
      "0.9675195857723378\n",
      "--------------------\n",
      "-65\n",
      "SaO2\n",
      "0.9603608350471073\n",
      "--------------------\n",
      "ICUType 1\n",
      "SaO2\n",
      "0.964683557990219\n",
      "--------------------\n",
      "ICUType 2\n",
      "SaO2\n",
      "0.9694730503594138\n",
      "--------------------\n",
      "ICUType 3\n",
      "Temp\n",
      "0.9274873339532732\n",
      "--------------------\n",
      "ICUType 4\n",
      "SaO2\n",
      "0.9657142966512496\n",
      "--------------------\n",
      "Undefined classification\n",
      "SaO2\n",
      "0.9598484974015699\n",
      "--------------------\n",
      "Low Weight\n",
      "SaO2\n",
      "0.966071435383319\n",
      "--------------------\n",
      "Normal Weight\n",
      "SaO2\n",
      "0.9706250113936478\n",
      "--------------------\n",
      "Overweight\n",
      "SaO2\n",
      "0.9674803281393501\n",
      "--------------------\n",
      "Obesity 1\n",
      "SaO2\n",
      "0.9677868954470741\n",
      "--------------------\n",
      "Obesity 2\n",
      "SaO2\n",
      "0.962777776850541\n",
      "--------------------\n",
      "Obesity 3\n",
      "SaO2\n",
      "0.9603703794655621\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.max_value_in_subgroup(df_saits_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAITS - MAE\n",
      "************\n",
      "General\n",
      "-------------\n",
      "ALP : 120.21354016661581\n",
      "ALT : 292.51042278607537\n",
      "AST : 566.9945693689813\n",
      "Albumin : 2.9510791413217787\n",
      "BUN : 25.788438117812007\n",
      "Bilirubin : 2.7703703989979305\n",
      "Cholesterol : 157.29999999999214\n",
      "Creatinine : 1.4927400290198274\n",
      "DiasABP : 59.684729736472974\n",
      "FiO2 : 0.5427910102481897\n",
      "GCS : 11.416553277655188\n",
      "Glucose : 139.70664855678496\n",
      "HCO3 : 22.998730468265872\n",
      "HCT : 30.609207015520653\n",
      "HR : 86.69561557043218\n",
      "K : 4.13614858243916\n",
      "Lactate : 2.767346140115878\n",
      "MAP : 80.30970073528962\n",
      "MechVent : 0.9999999999999996\n",
      "Mg : 2.011890264982128\n",
      "NIDiasABP : 58.44698860910438\n",
      "NIMAP : 77.73032076927599\n",
      "NISysABP : 119.4786514127118\n",
      "Na : 138.88944723618073\n",
      "PaCO2 : 40.95278192858044\n",
      "PaO2 : 148.29175956176658\n",
      "Platelets : 190.6867094356714\n",
      "RespRate : 19.431791012219442\n",
      "SaO2 : 96.8508583690985\n",
      "SysABP : 120.12315150566705\n",
      "Temp : 37.09088744652194\n",
      "TroponinI : 8.210000050067492\n",
      "TroponinT : 0.8710493570492538\n",
      "Urine : 116.09896736077816\n",
      "WBC : 12.26488773373189\n",
      "Weight : 82.7366097717606\n",
      "Ph : 7.43894935678957\n",
      "Female\n",
      "-------------\n",
      "ALP : 123.09589082900624\n",
      "ALT : 118.43662048393406\n",
      "AST : 625.6842090706995\n",
      "Albumin : 2.823188412016676\n",
      "BUN : 24.92485890684821\n",
      "Bilirubin : 2.1516128864179147\n",
      "Cholesterol : 185.99999389644717\n",
      "Creatinine : 1.1732193701117772\n",
      "DiasABP : 59.40330312211117\n",
      "FiO2 : 0.5322841955806464\n",
      "GCS : 11.428689406604583\n",
      "Glucose : 139.76291745774245\n",
      "HCO3 : 23.20891323195852\n",
      "HCT : 30.484399137570815\n",
      "HR : 87.08707636694399\n",
      "K : 4.0514359704508065\n",
      "Lactate : 2.657635525235976\n",
      "MAP : 80.75017183196017\n",
      "MechVent : 0.9999999999999986\n",
      "Mg : 1.9722071103244072\n",
      "NIDiasABP : 56.857936920286534\n",
      "NIMAP : 75.82649832001862\n",
      "NISysABP : 119.2597793553718\n",
      "Na : 138.99199999999962\n",
      "PaCO2 : 40.32464615074356\n",
      "PaO2 : 146.97227254526067\n",
      "Platelets : 205.68434699587172\n",
      "RespRate : 19.65318653605362\n",
      "SaO2 : 96.17415730337025\n",
      "SysABP : 120.34973163640119\n",
      "Temp : 36.99280200646472\n",
      "TroponinI : 4.98000006079624\n",
      "TroponinT : 1.0406451424463454\n",
      "Urine : 102.45429780167657\n",
      "WBC : 12.421652221355712\n",
      "Weight : 73.87801045546404\n",
      "Ph : 7.373614547519433\n",
      "Male\n",
      "-------------\n",
      "ALP : 127.04629590776115\n",
      "ALT : 281.3000011026836\n",
      "AST : 250.5185204611861\n",
      "Albumin : 2.9353658426098352\n",
      "BUN : 29.041096133184148\n",
      "Bilirubin : 2.242934758167528\n",
      "Cholesterol : 158.23076923075706\n",
      "Creatinine : 1.6857437889191695\n",
      "DiasABP : 60.02959244863548\n",
      "FiO2 : 0.558764148292676\n",
      "GCS : 11.444232654753353\n",
      "Glucose : 144.02926779956317\n",
      "HCO3 : 23.403999533653213\n",
      "HCT : 30.73157472985931\n",
      "HR : 86.31227045082214\n",
      "K : 4.211607071615393\n",
      "Lactate : 2.8974815479031206\n",
      "MAP : 80.10412163524481\n",
      "MechVent : 0.999999999999999\n",
      "Mg : 2.063424968316233\n",
      "NIDiasABP : 59.830911759461415\n",
      "NIMAP : 78.28881712967546\n",
      "NISysABP : 118.47662729107307\n",
      "Na : 138.77426636568816\n",
      "PaCO2 : 40.7799745433184\n",
      "PaO2 : 148.14498146463632\n",
      "Platelets : 185.03941588975542\n",
      "RespRate : 19.083588507450603\n",
      "SaO2 : 96.75698923910785\n",
      "SysABP : 120.8875130478721\n",
      "Temp : 37.11854486865465\n",
      "TroponinI : 5.533333490291309\n",
      "TroponinT : 1.1449999794559893\n",
      "Urine : 123.36716051185522\n",
      "WBC : 13.113277497046269\n",
      "Weight : 88.5412417349066\n",
      "Ph : 7.3761730448847915\n",
      "Undefined Gender\n",
      "-------------\n",
      "ALP : 0.0\n",
      "ALT : 0.0\n",
      "AST : 0.0\n",
      "Albumin : 0.0\n",
      "BUN : 0.0\n",
      "Bilirubin : 0.0\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 0.0\n",
      "DiasABP : 61.708333492271336\n",
      "FiO2 : 0.0\n",
      "GCS : 14.250000715252174\n",
      "Glucose : 135.999999999864\n",
      "HCO3 : 24.49999999998775\n",
      "HCT : 0.0\n",
      "HR : 73.18749999999085\n",
      "K : 4.599999904627968\n",
      "Lactate : 0.0\n",
      "MAP : 75.999999999981\n",
      "MechVent : 0.0\n",
      "Mg : 1.799999952314484\n",
      "NIDiasABP : 62.12499999998447\n",
      "NIMAP : 79.33000183102824\n",
      "NISysABP : 96.49999999995174\n",
      "Na : 0.0\n",
      "PaCO2 : 0.0\n",
      "PaO2 : 0.0\n",
      "Platelets : 0.0\n",
      "RespRate : 19.749999999980247\n",
      "SaO2 : 0.0\n",
      "SysABP : 100.04166603087128\n",
      "Temp : 37.1499996185117\n",
      "TroponinI : 0.0\n",
      "TroponinT : 0.0\n",
      "Urine : 139.99999999986\n",
      "WBC : 10.849999904627143\n",
      "Weight : 99.56666310626595\n",
      "Ph : 7.46000051497667\n",
      "+65\n",
      "-------------\n",
      "ALP : 134.58333338631337\n",
      "ALT : 218.8681298821813\n",
      "AST : 239.44805549026773\n",
      "Albumin : 2.843835628195946\n",
      "BUN : 31.60714305915261\n",
      "Bilirubin : 1.6344444332851122\n",
      "Cholesterol : 149.24999999998755\n",
      "Creatinine : 1.37203790727667\n",
      "DiasABP : 56.92333064940073\n",
      "FiO2 : 0.5509840135837525\n",
      "GCS : 11.528753238173229\n",
      "Glucose : 147.38356123623743\n",
      "HCO3 : 22.9778320284312\n",
      "HCT : 30.842833375930734\n",
      "HR : 83.74005648661858\n",
      "K : 4.1257610482968445\n",
      "Lactate : 2.640000080778473\n",
      "MAP : 78.41003617123543\n",
      "MechVent : 0.9999999999999989\n",
      "Mg : 2.0671328920306533\n",
      "NIDiasABP : 54.84293846593251\n",
      "NIMAP : 74.70098286153004\n",
      "NISysABP : 119.29339133907601\n",
      "Na : 139.10421547454158\n",
      "PaCO2 : 39.72002827599954\n",
      "PaO2 : 147.65793416600005\n",
      "Platelets : 189.23395111842146\n",
      "RespRate : 19.995240901071856\n",
      "SaO2 : 96.75195727908279\n",
      "SysABP : 120.6963129861179\n",
      "Temp : 37.00658968920163\n",
      "TroponinI : 12.925000001985744\n",
      "TroponinT : 0.9596052598208064\n",
      "Urine : 101.87709531103178\n",
      "WBC : 13.70614814802449\n",
      "Weight : 77.84188421062525\n",
      "Ph : 7.375993523525159\n",
      "-65\n",
      "-------------\n",
      "ALP : 170.8476185934868\n",
      "ALT : 402.7164166987802\n",
      "AST : 430.2633963482682\n",
      "Albumin : 2.9364705899182146\n",
      "BUN : 22.407407635818238\n",
      "Bilirubin : 3.2957983256137395\n",
      "Cholesterol : 107.49999999994624\n",
      "Creatinine : 1.4274038370841935\n",
      "DiasABP : 63.24639737374103\n",
      "FiO2 : 0.5440240574521423\n",
      "GCS : 11.468112058820434\n",
      "Glucose : 145.04576622375873\n",
      "HCO3 : 22.57380903107774\n",
      "HCT : 30.39687492325896\n",
      "HR : 89.76194659125372\n",
      "K : 4.130370306674335\n",
      "Lactate : 2.6484445124202187\n",
      "MAP : 82.55844403135909\n",
      "MechVent : 0.9999999999999988\n",
      "Mg : 1.9805352983683515\n",
      "NIDiasABP : 62.15372363131515\n",
      "NIMAP : 79.44447386962122\n",
      "NISysABP : 120.10298521469146\n",
      "Na : 138.72517321016133\n",
      "PaCO2 : 40.965354435838044\n",
      "PaO2 : 154.81246861936796\n",
      "Platelets : 195.18065821912097\n",
      "RespRate : 18.735708352541018\n",
      "SaO2 : 96.0360824742263\n",
      "SysABP : 120.08354939651238\n",
      "Temp : 37.181719728445586\n",
      "TroponinI : 1.5499999672172515\n",
      "TroponinT : 0.8453571377987007\n",
      "Urine : 138.27340037797455\n",
      "WBC : 12.079177701662926\n",
      "Weight : 86.6275885794342\n",
      "Ph : 7.370806538492749\n",
      "ICUType 1\n",
      "-------------\n",
      "ALP : 89.63636398315023\n",
      "ALT : 319.0000019073327\n",
      "AST : 683.9200496291841\n",
      "Albumin : 3.3399999737737898\n",
      "BUN : 29.480620421180202\n",
      "Bilirubin : 1.1680000156163703\n",
      "Cholesterol : 171.76922900858804\n",
      "Creatinine : 1.849999986657292\n",
      "DiasABP : 58.33842763481674\n",
      "FiO2 : 0.5382165582316663\n",
      "GCS : 12.705188700612956\n",
      "Glucose : 144.2343749999989\n",
      "HCO3 : 23.30973402377758\n",
      "HCT : 32.74666667514353\n",
      "HR : 81.56440610734238\n",
      "K : 4.162142789363831\n",
      "Lactate : 3.2419355800073872\n",
      "MAP : 80.4727334238057\n",
      "MechVent : 0.9999999999999929\n",
      "Mg : 2.0463414831859144\n",
      "NIDiasABP : 57.20620430767199\n",
      "NIMAP : 74.22048429601323\n",
      "NISysABP : 113.34630789714456\n",
      "Na : 138.34399999999889\n",
      "PaCO2 : 39.610236235490625\n",
      "PaO2 : 153.58333327553373\n",
      "Platelets : 212.8983021752291\n",
      "RespRate : 19.312523055032656\n",
      "SaO2 : 96.46835443037853\n",
      "SysABP : 114.83695982709325\n",
      "Temp : 36.90374511203677\n",
      "TroponinI : 18.662499450144818\n",
      "TroponinT : 2.433823494569272\n",
      "Urine : 136.95640062706872\n",
      "WBC : 11.833636316386029\n",
      "Weight : 81.84006050718769\n",
      "Ph : 7.388508066054254\n",
      "ICUType 2\n",
      "-------------\n",
      "ALP : 58.239999847409784\n",
      "ALT : 497.31578862037054\n",
      "AST : 298.0000010780537\n",
      "Albumin : 2.685714295931841\n",
      "BUN : 20.337931251525738\n",
      "Bilirubin : 2.3250000197438934\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 1.2239999824762262\n",
      "DiasABP : 55.887238211319065\n",
      "FiO2 : 0.5689428114998677\n",
      "GCS : 11.646399650109837\n",
      "Glucose : 129.68375964042357\n",
      "HCO3 : 23.173912522215137\n",
      "HCT : 29.42999998728423\n",
      "HR : 85.90867378770199\n",
      "K : 4.475819575981979\n",
      "Lactate : 3.0614286379499416\n",
      "MAP : 75.11228422088696\n",
      "MechVent : 0.9999999999999974\n",
      "Mg : 2.1540540730630884\n",
      "NIDiasABP : 52.38178922826584\n",
      "NIMAP : 71.19470374340712\n",
      "NISysABP : 111.44726991600109\n",
      "Na : 138.0144927536222\n",
      "PaCO2 : 40.51112254884934\n",
      "PaO2 : 168.76030747195674\n",
      "Platelets : 163.28835696265725\n",
      "RespRate : 20.07981216403772\n",
      "SaO2 : 96.94730394026776\n",
      "SysABP : 114.61523617752768\n",
      "Temp : 37.27901341241118\n",
      "TroponinI : 10.180000114438881\n",
      "TroponinT : 0.3855555281042624\n",
      "Urine : 109.52372798504028\n",
      "WBC : 14.028042288684267\n",
      "Weight : 86.25298464291166\n",
      "Ph : 7.3798095029943\n",
      "ICUType 3\n",
      "-------------\n",
      "ALP : 184.60194148831977\n",
      "ALT : 396.0919550972378\n",
      "AST : 491.05494833516013\n",
      "Albumin : 2.804838734288324\n",
      "BUN : 33.99824563076609\n",
      "Bilirubin : 2.4300970831542106\n",
      "Cholesterol : 128.24999999996794\n",
      "Creatinine : 1.9934640331011126\n",
      "DiasABP : 60.77350118772809\n",
      "FiO2 : 0.556087635399216\n",
      "GCS : 11.59574474040056\n",
      "Glucose : 143.5819397993306\n",
      "HCO3 : 22.318451879989464\n",
      "HCT : 31.031780817737232\n",
      "HR : 89.69882572189515\n",
      "K : 4.133434594221015\n",
      "Lactate : 3.1338158669440164\n",
      "MAP : 81.7872864814867\n",
      "MechVent : 0.9999999999999983\n",
      "Mg : 2.024652801661021\n",
      "NIDiasABP : 59.09596199228146\n",
      "NIMAP : 77.84404846346212\n",
      "NISysABP : 119.99908820791582\n",
      "Na : 138.84364818361715\n",
      "PaCO2 : 41.83614885484835\n",
      "PaO2 : 122.97904193900979\n",
      "Platelets : 184.28321395553843\n",
      "RespRate : 19.813428369416663\n",
      "SaO2 : 91.89999999999816\n",
      "SysABP : 120.19424387102839\n",
      "Temp : 36.768098063737604\n",
      "TroponinI : 2.699999994701509\n",
      "TroponinT : 0.4359321938985411\n",
      "Urine : 115.98839683532711\n",
      "WBC : 12.33562751651288\n",
      "Weight : 81.21035621218734\n",
      "Ph : 7.353313575834895\n",
      "ICUType 4\n",
      "-------------\n",
      "ALP : 121.06060704317777\n",
      "ALT : 280.6557306227091\n",
      "AST : 630.3636355139878\n",
      "Albumin : 2.7702127517537276\n",
      "BUN : 20.02991485799471\n",
      "Bilirubin : 2.8444444589592264\n",
      "Cholesterol : 146.9999999999755\n",
      "Creatinine : 1.13346611764801\n",
      "DiasABP : 62.1907915071831\n",
      "FiO2 : 0.525559775508645\n",
      "GCS : 10.895913207820238\n",
      "Glucose : 134.22672053966392\n",
      "HCO3 : 23.051501679318047\n",
      "HCT : 30.856920980464178\n",
      "HR : 86.93353806854414\n",
      "K : 4.054821360962717\n",
      "Lactate : 2.8838185334989026\n",
      "MAP : 84.9015973258039\n",
      "MechVent : 0.9999999999999984\n",
      "Mg : 1.9418000202178878\n",
      "NIDiasABP : 59.487900318816365\n",
      "NIMAP : 79.75340070012265\n",
      "NISysABP : 126.00626599873202\n",
      "Na : 139.36900369003638\n",
      "PaCO2 : 39.545340160879036\n",
      "PaO2 : 145.5714285331856\n",
      "Platelets : 190.28759094467662\n",
      "RespRate : 18.940393720266883\n",
      "SaO2 : 96.57142857142703\n",
      "SysABP : 127.63526359476064\n",
      "Temp : 37.10740079395405\n",
      "TroponinI : 7.500000193713221\n",
      "TroponinT : 0.4575000104183762\n",
      "Urine : 122.13026955095468\n",
      "WBC : 12.333124995231577\n",
      "Weight : 80.82322622709763\n",
      "Ph : 7.380082555536936\n",
      "Undefined classification\n",
      "-------------\n",
      "ALP : 111.20224766248943\n",
      "ALT : 291.7088629807061\n",
      "AST : 279.4421094492833\n",
      "Albumin : 2.739743607166451\n",
      "BUN : 27.157143039930368\n",
      "Bilirubin : 2.7584269657898175\n",
      "Cholesterol : 170.6666649712267\n",
      "Creatinine : 1.5306615558744348\n",
      "DiasABP : 62.04001848988648\n",
      "FiO2 : 0.5253448272903851\n",
      "GCS : 11.622538356864473\n",
      "Glucose : 138.8800956854142\n",
      "HCO3 : 23.105910953042486\n",
      "HCT : 31.031276956781095\n",
      "HR : 86.61615923397423\n",
      "K : 4.134895770251742\n",
      "Lactate : 2.618356880727496\n",
      "MAP : 84.14977130915793\n",
      "MechVent : 0.9999999999999987\n",
      "Mg : 1.9515384796338153\n",
      "NIDiasABP : 59.48770409995857\n",
      "NIMAP : 77.79163943956353\n",
      "NISysABP : 121.2010913430792\n",
      "Na : 139.33495141927443\n",
      "PaCO2 : 41.31355941901762\n",
      "PaO2 : 131.05824827661286\n",
      "Platelets : 204.4825708834154\n",
      "RespRate : 19.78130921748146\n",
      "SaO2 : 95.98484848484703\n",
      "SysABP : 124.79178462349446\n",
      "Temp : 36.93883000696941\n",
      "TroponinI : 5.833333333332361\n",
      "TroponinT : 0.6018493108400254\n",
      "Urine : 123.38135347328762\n",
      "WBC : 13.08543102327602\n",
      "Weight : 80.14319066815386\n",
      "Ph : 7.372282332252036\n",
      "Low Weight\n",
      "-------------\n",
      "ALP : 366.49999999981674\n",
      "ALT : 204.19999999995915\n",
      "AST : 889.0000677106542\n",
      "Albumin : 3.100000023841341\n",
      "BUN : 30.850000286100755\n",
      "Bilirubin : 8.280000007150901\n",
      "Cholesterol : 153.99999999984598\n",
      "Creatinine : 1.0142857155628842\n",
      "DiasABP : 58.64975007136022\n",
      "FiO2 : 0.5060975624293692\n",
      "GCS : 11.093750007450408\n",
      "Glucose : 170.3478231015339\n",
      "HCO3 : 25.749999682106417\n",
      "HCT : 30.068749904631318\n",
      "HR : 88.64566976912853\n",
      "K : 3.9565216043719356\n",
      "Lactate : 4.56000005602791\n",
      "MAP : 83.81590396165782\n",
      "MechVent : 0.9999999999999678\n",
      "Mg : 2.1200000286100176\n",
      "NIDiasABP : 57.457408507664205\n",
      "NIMAP : 76.07613635372708\n",
      "NISysABP : 120.98340090899231\n",
      "Na : 135.79999999998643\n",
      "PaCO2 : 39.808823866001525\n",
      "PaO2 : 194.11842105262647\n",
      "Platelets : 169.1363615556043\n",
      "RespRate : 21.47619043077639\n",
      "SaO2 : 96.60714285713595\n",
      "SysABP : 124.1060285635865\n",
      "Temp : 36.98626934398263\n",
      "TroponinI : 0.0\n",
      "TroponinT : 2.789999961850237\n",
      "Urine : 117.98993227145738\n",
      "WBC : 12.10384623820872\n",
      "Weight : 55.79406718884437\n",
      "Ph : 7.372073185152944\n",
      "Normal Weight\n",
      "-------------\n",
      "ALP : 117.83871041574406\n",
      "ALT : 157.09524000257926\n",
      "AST : 177.37499982118052\n",
      "Albumin : 2.6368421378887197\n",
      "BUN : 27.640288095679875\n",
      "Bilirubin : 3.4551723982238403\n",
      "Cholesterol : 114.49999237054821\n",
      "Creatinine : 1.6014084398326627\n",
      "DiasABP : 57.76064030732224\n",
      "FiO2 : 0.5370249722019451\n",
      "GCS : 11.016666685070888\n",
      "Glucose : 137.56198290359762\n",
      "HCO3 : 22.90909054062566\n",
      "HCT : 30.72872364774647\n",
      "HR : 86.1620427014138\n",
      "K : 4.154838627384522\n",
      "Lactate : 2.899519312840214\n",
      "MAP : 77.19315231744962\n",
      "MechVent : 0.9999999999999969\n",
      "Mg : 2.0536585677929846\n",
      "NIDiasABP : 55.284953725996836\n",
      "NIMAP : 74.73490517552231\n",
      "NISysABP : 114.4056657321788\n",
      "Na : 138.88148148148045\n",
      "PaCO2 : 39.83276454417765\n",
      "PaO2 : 160.31305445101717\n",
      "Platelets : 172.30934999314877\n",
      "RespRate : 18.32343131359484\n",
      "SaO2 : 97.06249999999908\n",
      "SysABP : 115.43367258292807\n",
      "Temp : 37.06520819041565\n",
      "TroponinI : 8.444444550407319\n",
      "TroponinT : 0.8938095269812999\n",
      "Urine : 105.16068367160482\n",
      "WBC : 13.203875901162984\n",
      "Weight : 66.9053476384954\n",
      "Ph : 7.379047604645165\n",
      "Overweight\n",
      "-------------\n",
      "ALP : 98.84375023841548\n",
      "ALT : 641.8055382172088\n",
      "AST : 667.7142724082266\n",
      "Albumin : 3.0210525926789153\n",
      "BUN : 26.49242446278061\n",
      "Bilirubin : 1.5861110765900435\n",
      "Cholesterol : 206.2857121058578\n",
      "Creatinine : 1.3445255269099228\n",
      "DiasABP : 57.9725691945707\n",
      "FiO2 : 0.5362397804247246\n",
      "GCS : 11.548223412379778\n",
      "Glucose : 138.46721258319678\n",
      "HCO3 : 22.406451053003966\n",
      "HCT : 30.329205637780163\n",
      "HR : 85.52348124530441\n",
      "K : 4.382677070737789\n",
      "Lactate : 2.9854651772698344\n",
      "MAP : 78.66941025770825\n",
      "MechVent : 0.999999999999997\n",
      "Mg : 2.0132867372952834\n",
      "NIDiasABP : 56.550336975610456\n",
      "NIMAP : 75.65089601458912\n",
      "NISysABP : 115.66145973658907\n",
      "Na : 138.06597222222126\n",
      "PaCO2 : 40.217234301493136\n",
      "PaO2 : 150.2811594424036\n",
      "Platelets : 171.3580221953205\n",
      "RespRate : 19.308421317111165\n",
      "SaO2 : 96.74803149606224\n",
      "SysABP : 117.77767338535995\n",
      "Temp : 37.17621774796087\n",
      "TroponinI : 9.839999675748764\n",
      "TroponinT : 1.5638460903785494\n",
      "Urine : 106.28093231747964\n",
      "WBC : 12.234964963439376\n",
      "Weight : 82.8306388319315\n",
      "Ph : 7.376130891697725\n",
      "Obesity 1\n",
      "-------------\n",
      "ALP : 101.57142938885644\n",
      "ALT : 249.90000133513158\n",
      "AST : 388.75000405309646\n",
      "Albumin : 2.9833333690958717\n",
      "BUN : 30.30666673024455\n",
      "Bilirubin : 2.5933333277700603\n",
      "Cholesterol : 131.999999999956\n",
      "Creatinine : 1.3637680994427526\n",
      "DiasABP : 59.19672265569193\n",
      "FiO2 : 0.5825757534816982\n",
      "GCS : 11.416949225280204\n",
      "Glucose : 131.9824538649151\n",
      "HCO3 : 23.21739082059965\n",
      "HCT : 31.13616081646483\n",
      "HR : 86.93467938458468\n",
      "K : 4.073493908686761\n",
      "Lactate : 3.297777867317126\n",
      "MAP : 79.07987651984328\n",
      "MechVent : 0.9999999999999944\n",
      "Mg : 1.9642857432365137\n",
      "NIDiasABP : 58.71149696375257\n",
      "NIMAP : 75.18894930047477\n",
      "NISysABP : 118.64528871669559\n",
      "Na : 137.54687499999787\n",
      "PaCO2 : 39.36060617620271\n",
      "PaO2 : 131.0406977187748\n",
      "Platelets : 193.49999738994146\n",
      "RespRate : 19.633986984202938\n",
      "SaO2 : 96.77868852458857\n",
      "SysABP : 120.06604052649587\n",
      "Temp : 37.28681568741102\n",
      "TroponinI : 7.5999998748264455\n",
      "TroponinT : 1.1300000172446047\n",
      "Urine : 119.93684820085748\n",
      "WBC : 16.095082040692674\n",
      "Weight : 97.10293181533095\n",
      "Ph : 7.897253254938348\n",
      "Obesity 2\n",
      "-------------\n",
      "ALP : 63.55555597940374\n",
      "ALT : 772.2857140131256\n",
      "AST : 86.00000127153709\n",
      "Albumin : 2.749999960263112\n",
      "BUN : 32.0967743166021\n",
      "Bilirubin : 1.6444445004064003\n",
      "Cholesterol : 163.99998474104692\n",
      "Creatinine : 1.406666653354916\n",
      "DiasABP : 59.65967760512106\n",
      "FiO2 : 0.5607758611440562\n",
      "GCS : 11.775000051657262\n",
      "Glucose : 132.64516196711944\n",
      "HCO3 : 24.8499995231616\n",
      "HCT : 31.592104911803368\n",
      "HR : 89.15027794471129\n",
      "K : 4.123076842381246\n",
      "Lactate : 2.9500000434260305\n",
      "MAP : 79.40157111746333\n",
      "MechVent : 0.9999999999999836\n",
      "Mg : 1.948148175522061\n",
      "NIDiasABP : 56.99116250240399\n",
      "NIMAP : 75.81827120016492\n",
      "NISysABP : 113.35227284286876\n",
      "Na : 139.06060606060183\n",
      "PaCO2 : 39.86153860825698\n",
      "PaO2 : 150.49230780968068\n",
      "Platelets : 174.65384145882743\n",
      "RespRate : 20.693181991576676\n",
      "SaO2 : 96.27777777777244\n",
      "SysABP : 115.81126669311477\n",
      "Temp : 37.18132122227383\n",
      "TroponinI : 0.0\n",
      "TroponinT : 5.0199996829016165\n",
      "Urine : 109.4420520390892\n",
      "WBC : 13.003225834138513\n",
      "Weight : 106.61793074400471\n",
      "Ph : 7.353928514889176\n",
      "Obesity 3\n",
      "-------------\n",
      "ALP : 95.45454268021149\n",
      "ALT : 1688.3333333327705\n",
      "AST : 45.24999952315153\n",
      "Albumin : 2.6799999713892344\n",
      "BUN : 30.428571519396108\n",
      "Bilirubin : 2.2166666885213338\n",
      "Cholesterol : 0.0\n",
      "Creatinine : 1.7838709508218498\n",
      "DiasABP : 59.396970476422794\n",
      "FiO2 : 0.5888278304220491\n",
      "GCS : 11.848360702639624\n",
      "Glucose : 164.94117882671992\n",
      "HCO3 : 21.735293556661528\n",
      "HCT : 30.98292667109716\n",
      "HR : 87.77342518893134\n",
      "K : 4.339999914169167\n",
      "Lactate : 3.423611212107775\n",
      "MAP : 78.63692615194532\n",
      "MechVent : 0.999999999999988\n",
      "Mg : 2.037142876216285\n",
      "NIDiasABP : 56.819231502825964\n",
      "NIMAP : 75.5919045499848\n",
      "NISysABP : 114.6807662501471\n",
      "Na : 136.39130434782015\n",
      "PaCO2 : 41.18604651162743\n",
      "PaO2 : 121.22097366847214\n",
      "Platelets : 165.96874713897185\n",
      "RespRate : 19.796296437580647\n",
      "SaO2 : 96.03703703703349\n",
      "SysABP : 117.61257398340582\n",
      "Temp : 37.319737375067575\n",
      "TroponinI : 6.999999999992999\n",
      "TroponinT : 3.8860000684849596\n",
      "Urine : 108.46558934383147\n",
      "WBC : 11.903030265460954\n",
      "Weight : 126.39821168354523\n",
      "Ph : 7.3652777539358185\n"
     ]
    }
   ],
   "source": [
    "print(\"SAITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_saits_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_saits_mae_minmax_ori = toolkits.create_table(testing_mae_saits_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>General</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>Undefined Gender</th>\n",
       "      <th>+65</th>\n",
       "      <th>-65</th>\n",
       "      <th>ICUType 1</th>\n",
       "      <th>ICUType 2</th>\n",
       "      <th>ICUType 3</th>\n",
       "      <th>ICUType 4</th>\n",
       "      <th>Undefined classification</th>\n",
       "      <th>Low Weight</th>\n",
       "      <th>Normal Weight</th>\n",
       "      <th>Overweight</th>\n",
       "      <th>Obesity 1</th>\n",
       "      <th>Obesity 2</th>\n",
       "      <th>Obesity 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALP</td>\n",
       "      <td>120.213540</td>\n",
       "      <td>123.095891</td>\n",
       "      <td>127.046296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>134.583333</td>\n",
       "      <td>170.847619</td>\n",
       "      <td>89.636364</td>\n",
       "      <td>58.240000</td>\n",
       "      <td>184.601941</td>\n",
       "      <td>121.060607</td>\n",
       "      <td>111.202248</td>\n",
       "      <td>366.500000</td>\n",
       "      <td>117.838710</td>\n",
       "      <td>98.843750</td>\n",
       "      <td>101.571429</td>\n",
       "      <td>63.555556</td>\n",
       "      <td>95.454543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALT</td>\n",
       "      <td>292.510423</td>\n",
       "      <td>118.436620</td>\n",
       "      <td>281.300001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>218.868130</td>\n",
       "      <td>402.716417</td>\n",
       "      <td>319.000002</td>\n",
       "      <td>497.315789</td>\n",
       "      <td>396.091955</td>\n",
       "      <td>280.655731</td>\n",
       "      <td>291.708863</td>\n",
       "      <td>204.200000</td>\n",
       "      <td>157.095240</td>\n",
       "      <td>641.805538</td>\n",
       "      <td>249.900001</td>\n",
       "      <td>772.285714</td>\n",
       "      <td>1688.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AST</td>\n",
       "      <td>566.994569</td>\n",
       "      <td>625.684209</td>\n",
       "      <td>250.518520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>239.448055</td>\n",
       "      <td>430.263396</td>\n",
       "      <td>683.920050</td>\n",
       "      <td>298.000001</td>\n",
       "      <td>491.054948</td>\n",
       "      <td>630.363636</td>\n",
       "      <td>279.442109</td>\n",
       "      <td>889.000068</td>\n",
       "      <td>177.375000</td>\n",
       "      <td>667.714272</td>\n",
       "      <td>388.750004</td>\n",
       "      <td>86.000001</td>\n",
       "      <td>45.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albumin</td>\n",
       "      <td>2.951079</td>\n",
       "      <td>2.823188</td>\n",
       "      <td>2.935366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.843836</td>\n",
       "      <td>2.936471</td>\n",
       "      <td>3.340000</td>\n",
       "      <td>2.685714</td>\n",
       "      <td>2.804839</td>\n",
       "      <td>2.770213</td>\n",
       "      <td>2.739744</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>2.636842</td>\n",
       "      <td>3.021053</td>\n",
       "      <td>2.983333</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BUN</td>\n",
       "      <td>25.788438</td>\n",
       "      <td>24.924859</td>\n",
       "      <td>29.041096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.607143</td>\n",
       "      <td>22.407408</td>\n",
       "      <td>29.480620</td>\n",
       "      <td>20.337931</td>\n",
       "      <td>33.998246</td>\n",
       "      <td>20.029915</td>\n",
       "      <td>27.157143</td>\n",
       "      <td>30.850000</td>\n",
       "      <td>27.640288</td>\n",
       "      <td>26.492424</td>\n",
       "      <td>30.306667</td>\n",
       "      <td>32.096774</td>\n",
       "      <td>30.428572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bilirubin</td>\n",
       "      <td>2.770370</td>\n",
       "      <td>2.151613</td>\n",
       "      <td>2.242935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.634444</td>\n",
       "      <td>3.295798</td>\n",
       "      <td>1.168000</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>2.430097</td>\n",
       "      <td>2.844444</td>\n",
       "      <td>2.758427</td>\n",
       "      <td>8.280000</td>\n",
       "      <td>3.455172</td>\n",
       "      <td>1.586111</td>\n",
       "      <td>2.593333</td>\n",
       "      <td>1.644445</td>\n",
       "      <td>2.216667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cholesterol</td>\n",
       "      <td>157.300000</td>\n",
       "      <td>185.999994</td>\n",
       "      <td>158.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>149.250000</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>171.769229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>128.250000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>170.666665</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>114.499992</td>\n",
       "      <td>206.285712</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>163.999985</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Creatinine</td>\n",
       "      <td>1.492740</td>\n",
       "      <td>1.173219</td>\n",
       "      <td>1.685744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.372038</td>\n",
       "      <td>1.427404</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.224000</td>\n",
       "      <td>1.993464</td>\n",
       "      <td>1.133466</td>\n",
       "      <td>1.530662</td>\n",
       "      <td>1.014286</td>\n",
       "      <td>1.601408</td>\n",
       "      <td>1.344526</td>\n",
       "      <td>1.363768</td>\n",
       "      <td>1.406667</td>\n",
       "      <td>1.783871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DiasABP</td>\n",
       "      <td>59.684730</td>\n",
       "      <td>59.403303</td>\n",
       "      <td>60.029592</td>\n",
       "      <td>61.708333</td>\n",
       "      <td>56.923331</td>\n",
       "      <td>63.246397</td>\n",
       "      <td>58.338428</td>\n",
       "      <td>55.887238</td>\n",
       "      <td>60.773501</td>\n",
       "      <td>62.190792</td>\n",
       "      <td>62.040018</td>\n",
       "      <td>58.649750</td>\n",
       "      <td>57.760640</td>\n",
       "      <td>57.972569</td>\n",
       "      <td>59.196723</td>\n",
       "      <td>59.659678</td>\n",
       "      <td>59.396970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FiO2</td>\n",
       "      <td>0.542791</td>\n",
       "      <td>0.532284</td>\n",
       "      <td>0.558764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550984</td>\n",
       "      <td>0.544024</td>\n",
       "      <td>0.538217</td>\n",
       "      <td>0.568943</td>\n",
       "      <td>0.556088</td>\n",
       "      <td>0.525560</td>\n",
       "      <td>0.525345</td>\n",
       "      <td>0.506098</td>\n",
       "      <td>0.537025</td>\n",
       "      <td>0.536240</td>\n",
       "      <td>0.582576</td>\n",
       "      <td>0.560776</td>\n",
       "      <td>0.588828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GCS</td>\n",
       "      <td>11.416553</td>\n",
       "      <td>11.428689</td>\n",
       "      <td>11.444233</td>\n",
       "      <td>14.250001</td>\n",
       "      <td>11.528753</td>\n",
       "      <td>11.468112</td>\n",
       "      <td>12.705189</td>\n",
       "      <td>11.646400</td>\n",
       "      <td>11.595745</td>\n",
       "      <td>10.895913</td>\n",
       "      <td>11.622538</td>\n",
       "      <td>11.093750</td>\n",
       "      <td>11.016667</td>\n",
       "      <td>11.548223</td>\n",
       "      <td>11.416949</td>\n",
       "      <td>11.775000</td>\n",
       "      <td>11.848361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Glucose</td>\n",
       "      <td>139.706649</td>\n",
       "      <td>139.762917</td>\n",
       "      <td>144.029268</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>147.383561</td>\n",
       "      <td>145.045766</td>\n",
       "      <td>144.234375</td>\n",
       "      <td>129.683760</td>\n",
       "      <td>143.581940</td>\n",
       "      <td>134.226721</td>\n",
       "      <td>138.880096</td>\n",
       "      <td>170.347823</td>\n",
       "      <td>137.561983</td>\n",
       "      <td>138.467213</td>\n",
       "      <td>131.982454</td>\n",
       "      <td>132.645162</td>\n",
       "      <td>164.941179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HCO3</td>\n",
       "      <td>22.998730</td>\n",
       "      <td>23.208913</td>\n",
       "      <td>23.404000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>22.977832</td>\n",
       "      <td>22.573809</td>\n",
       "      <td>23.309734</td>\n",
       "      <td>23.173913</td>\n",
       "      <td>22.318452</td>\n",
       "      <td>23.051502</td>\n",
       "      <td>23.105911</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>22.909091</td>\n",
       "      <td>22.406451</td>\n",
       "      <td>23.217391</td>\n",
       "      <td>24.850000</td>\n",
       "      <td>21.735294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HCT</td>\n",
       "      <td>30.609207</td>\n",
       "      <td>30.484399</td>\n",
       "      <td>30.731575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.842833</td>\n",
       "      <td>30.396875</td>\n",
       "      <td>32.746667</td>\n",
       "      <td>29.430000</td>\n",
       "      <td>31.031781</td>\n",
       "      <td>30.856921</td>\n",
       "      <td>31.031277</td>\n",
       "      <td>30.068750</td>\n",
       "      <td>30.728724</td>\n",
       "      <td>30.329206</td>\n",
       "      <td>31.136161</td>\n",
       "      <td>31.592105</td>\n",
       "      <td>30.982927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HR</td>\n",
       "      <td>86.695616</td>\n",
       "      <td>87.087076</td>\n",
       "      <td>86.312270</td>\n",
       "      <td>73.187500</td>\n",
       "      <td>83.740056</td>\n",
       "      <td>89.761947</td>\n",
       "      <td>81.564406</td>\n",
       "      <td>85.908674</td>\n",
       "      <td>89.698826</td>\n",
       "      <td>86.933538</td>\n",
       "      <td>86.616159</td>\n",
       "      <td>88.645670</td>\n",
       "      <td>86.162043</td>\n",
       "      <td>85.523481</td>\n",
       "      <td>86.934679</td>\n",
       "      <td>89.150278</td>\n",
       "      <td>87.773425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>K</td>\n",
       "      <td>4.136149</td>\n",
       "      <td>4.051436</td>\n",
       "      <td>4.211607</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.125761</td>\n",
       "      <td>4.130370</td>\n",
       "      <td>4.162143</td>\n",
       "      <td>4.475820</td>\n",
       "      <td>4.133435</td>\n",
       "      <td>4.054821</td>\n",
       "      <td>4.134896</td>\n",
       "      <td>3.956522</td>\n",
       "      <td>4.154839</td>\n",
       "      <td>4.382677</td>\n",
       "      <td>4.073494</td>\n",
       "      <td>4.123077</td>\n",
       "      <td>4.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lactate</td>\n",
       "      <td>2.767346</td>\n",
       "      <td>2.657636</td>\n",
       "      <td>2.897482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.640000</td>\n",
       "      <td>2.648445</td>\n",
       "      <td>3.241936</td>\n",
       "      <td>3.061429</td>\n",
       "      <td>3.133816</td>\n",
       "      <td>2.883819</td>\n",
       "      <td>2.618357</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>2.899519</td>\n",
       "      <td>2.985465</td>\n",
       "      <td>3.297778</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>3.423611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MAP</td>\n",
       "      <td>80.309701</td>\n",
       "      <td>80.750172</td>\n",
       "      <td>80.104122</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>78.410036</td>\n",
       "      <td>82.558444</td>\n",
       "      <td>80.472733</td>\n",
       "      <td>75.112284</td>\n",
       "      <td>81.787286</td>\n",
       "      <td>84.901597</td>\n",
       "      <td>84.149771</td>\n",
       "      <td>83.815904</td>\n",
       "      <td>77.193152</td>\n",
       "      <td>78.669410</td>\n",
       "      <td>79.079877</td>\n",
       "      <td>79.401571</td>\n",
       "      <td>78.636926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MechVent</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mg</td>\n",
       "      <td>2.011890</td>\n",
       "      <td>1.972207</td>\n",
       "      <td>2.063425</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.067133</td>\n",
       "      <td>1.980535</td>\n",
       "      <td>2.046341</td>\n",
       "      <td>2.154054</td>\n",
       "      <td>2.024653</td>\n",
       "      <td>1.941800</td>\n",
       "      <td>1.951538</td>\n",
       "      <td>2.120000</td>\n",
       "      <td>2.053659</td>\n",
       "      <td>2.013287</td>\n",
       "      <td>1.964286</td>\n",
       "      <td>1.948148</td>\n",
       "      <td>2.037143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NIDiasABP</td>\n",
       "      <td>58.446989</td>\n",
       "      <td>56.857937</td>\n",
       "      <td>59.830912</td>\n",
       "      <td>62.125000</td>\n",
       "      <td>54.842938</td>\n",
       "      <td>62.153724</td>\n",
       "      <td>57.206204</td>\n",
       "      <td>52.381789</td>\n",
       "      <td>59.095962</td>\n",
       "      <td>59.487900</td>\n",
       "      <td>59.487704</td>\n",
       "      <td>57.457409</td>\n",
       "      <td>55.284954</td>\n",
       "      <td>56.550337</td>\n",
       "      <td>58.711497</td>\n",
       "      <td>56.991163</td>\n",
       "      <td>56.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NIMAP</td>\n",
       "      <td>77.730321</td>\n",
       "      <td>75.826498</td>\n",
       "      <td>78.288817</td>\n",
       "      <td>79.330002</td>\n",
       "      <td>74.700983</td>\n",
       "      <td>79.444474</td>\n",
       "      <td>74.220484</td>\n",
       "      <td>71.194704</td>\n",
       "      <td>77.844048</td>\n",
       "      <td>79.753401</td>\n",
       "      <td>77.791639</td>\n",
       "      <td>76.076136</td>\n",
       "      <td>74.734905</td>\n",
       "      <td>75.650896</td>\n",
       "      <td>75.188949</td>\n",
       "      <td>75.818271</td>\n",
       "      <td>75.591905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NISysABP</td>\n",
       "      <td>119.478651</td>\n",
       "      <td>119.259779</td>\n",
       "      <td>118.476627</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>119.293391</td>\n",
       "      <td>120.102985</td>\n",
       "      <td>113.346308</td>\n",
       "      <td>111.447270</td>\n",
       "      <td>119.999088</td>\n",
       "      <td>126.006266</td>\n",
       "      <td>121.201091</td>\n",
       "      <td>120.983401</td>\n",
       "      <td>114.405666</td>\n",
       "      <td>115.661460</td>\n",
       "      <td>118.645289</td>\n",
       "      <td>113.352273</td>\n",
       "      <td>114.680766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Na</td>\n",
       "      <td>138.889447</td>\n",
       "      <td>138.992000</td>\n",
       "      <td>138.774266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>139.104215</td>\n",
       "      <td>138.725173</td>\n",
       "      <td>138.344000</td>\n",
       "      <td>138.014493</td>\n",
       "      <td>138.843648</td>\n",
       "      <td>139.369004</td>\n",
       "      <td>139.334951</td>\n",
       "      <td>135.800000</td>\n",
       "      <td>138.881481</td>\n",
       "      <td>138.065972</td>\n",
       "      <td>137.546875</td>\n",
       "      <td>139.060606</td>\n",
       "      <td>136.391304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>PaCO2</td>\n",
       "      <td>40.952782</td>\n",
       "      <td>40.324646</td>\n",
       "      <td>40.779975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.720028</td>\n",
       "      <td>40.965354</td>\n",
       "      <td>39.610236</td>\n",
       "      <td>40.511123</td>\n",
       "      <td>41.836149</td>\n",
       "      <td>39.545340</td>\n",
       "      <td>41.313559</td>\n",
       "      <td>39.808824</td>\n",
       "      <td>39.832765</td>\n",
       "      <td>40.217234</td>\n",
       "      <td>39.360606</td>\n",
       "      <td>39.861539</td>\n",
       "      <td>41.186047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>PaO2</td>\n",
       "      <td>148.291760</td>\n",
       "      <td>146.972273</td>\n",
       "      <td>148.144981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.657934</td>\n",
       "      <td>154.812469</td>\n",
       "      <td>153.583333</td>\n",
       "      <td>168.760307</td>\n",
       "      <td>122.979042</td>\n",
       "      <td>145.571429</td>\n",
       "      <td>131.058248</td>\n",
       "      <td>194.118421</td>\n",
       "      <td>160.313054</td>\n",
       "      <td>150.281159</td>\n",
       "      <td>131.040698</td>\n",
       "      <td>150.492308</td>\n",
       "      <td>121.220974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Platelets</td>\n",
       "      <td>190.686709</td>\n",
       "      <td>205.684347</td>\n",
       "      <td>185.039416</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>189.233951</td>\n",
       "      <td>195.180658</td>\n",
       "      <td>212.898302</td>\n",
       "      <td>163.288357</td>\n",
       "      <td>184.283214</td>\n",
       "      <td>190.287591</td>\n",
       "      <td>204.482571</td>\n",
       "      <td>169.136362</td>\n",
       "      <td>172.309350</td>\n",
       "      <td>171.358022</td>\n",
       "      <td>193.499997</td>\n",
       "      <td>174.653841</td>\n",
       "      <td>165.968747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RespRate</td>\n",
       "      <td>19.431791</td>\n",
       "      <td>19.653187</td>\n",
       "      <td>19.083589</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>19.995241</td>\n",
       "      <td>18.735708</td>\n",
       "      <td>19.312523</td>\n",
       "      <td>20.079812</td>\n",
       "      <td>19.813428</td>\n",
       "      <td>18.940394</td>\n",
       "      <td>19.781309</td>\n",
       "      <td>21.476190</td>\n",
       "      <td>18.323431</td>\n",
       "      <td>19.308421</td>\n",
       "      <td>19.633987</td>\n",
       "      <td>20.693182</td>\n",
       "      <td>19.796296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>SaO2</td>\n",
       "      <td>96.850858</td>\n",
       "      <td>96.174157</td>\n",
       "      <td>96.756989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96.751957</td>\n",
       "      <td>96.036082</td>\n",
       "      <td>96.468354</td>\n",
       "      <td>96.947304</td>\n",
       "      <td>91.900000</td>\n",
       "      <td>96.571429</td>\n",
       "      <td>95.984848</td>\n",
       "      <td>96.607143</td>\n",
       "      <td>97.062500</td>\n",
       "      <td>96.748031</td>\n",
       "      <td>96.778689</td>\n",
       "      <td>96.277778</td>\n",
       "      <td>96.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>SysABP</td>\n",
       "      <td>120.123152</td>\n",
       "      <td>120.349732</td>\n",
       "      <td>120.887513</td>\n",
       "      <td>100.041666</td>\n",
       "      <td>120.696313</td>\n",
       "      <td>120.083549</td>\n",
       "      <td>114.836960</td>\n",
       "      <td>114.615236</td>\n",
       "      <td>120.194244</td>\n",
       "      <td>127.635264</td>\n",
       "      <td>124.791785</td>\n",
       "      <td>124.106029</td>\n",
       "      <td>115.433673</td>\n",
       "      <td>117.777673</td>\n",
       "      <td>120.066041</td>\n",
       "      <td>115.811267</td>\n",
       "      <td>117.612574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Temp</td>\n",
       "      <td>37.090887</td>\n",
       "      <td>36.992802</td>\n",
       "      <td>37.118545</td>\n",
       "      <td>37.150000</td>\n",
       "      <td>37.006590</td>\n",
       "      <td>37.181720</td>\n",
       "      <td>36.903745</td>\n",
       "      <td>37.279013</td>\n",
       "      <td>36.768098</td>\n",
       "      <td>37.107401</td>\n",
       "      <td>36.938830</td>\n",
       "      <td>36.986269</td>\n",
       "      <td>37.065208</td>\n",
       "      <td>37.176218</td>\n",
       "      <td>37.286816</td>\n",
       "      <td>37.181321</td>\n",
       "      <td>37.319737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>TroponinI</td>\n",
       "      <td>8.210000</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>5.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.925000</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>18.662499</td>\n",
       "      <td>10.180000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.444445</td>\n",
       "      <td>9.840000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TroponinT</td>\n",
       "      <td>0.871049</td>\n",
       "      <td>1.040645</td>\n",
       "      <td>1.145000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.959605</td>\n",
       "      <td>0.845357</td>\n",
       "      <td>2.433823</td>\n",
       "      <td>0.385556</td>\n",
       "      <td>0.435932</td>\n",
       "      <td>0.457500</td>\n",
       "      <td>0.601849</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>0.893810</td>\n",
       "      <td>1.563846</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>5.020000</td>\n",
       "      <td>3.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Urine</td>\n",
       "      <td>116.098967</td>\n",
       "      <td>102.454298</td>\n",
       "      <td>123.367161</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>101.877095</td>\n",
       "      <td>138.273400</td>\n",
       "      <td>136.956401</td>\n",
       "      <td>109.523728</td>\n",
       "      <td>115.988397</td>\n",
       "      <td>122.130270</td>\n",
       "      <td>123.381353</td>\n",
       "      <td>117.989932</td>\n",
       "      <td>105.160684</td>\n",
       "      <td>106.280932</td>\n",
       "      <td>119.936848</td>\n",
       "      <td>109.442052</td>\n",
       "      <td>108.465589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WBC</td>\n",
       "      <td>12.264888</td>\n",
       "      <td>12.421652</td>\n",
       "      <td>13.113277</td>\n",
       "      <td>10.850000</td>\n",
       "      <td>13.706148</td>\n",
       "      <td>12.079178</td>\n",
       "      <td>11.833636</td>\n",
       "      <td>14.028042</td>\n",
       "      <td>12.335628</td>\n",
       "      <td>12.333125</td>\n",
       "      <td>13.085431</td>\n",
       "      <td>12.103846</td>\n",
       "      <td>13.203876</td>\n",
       "      <td>12.234965</td>\n",
       "      <td>16.095082</td>\n",
       "      <td>13.003226</td>\n",
       "      <td>11.903030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Weight</td>\n",
       "      <td>82.736610</td>\n",
       "      <td>73.878010</td>\n",
       "      <td>88.541242</td>\n",
       "      <td>99.566663</td>\n",
       "      <td>77.841884</td>\n",
       "      <td>86.627589</td>\n",
       "      <td>81.840061</td>\n",
       "      <td>86.252985</td>\n",
       "      <td>81.210356</td>\n",
       "      <td>80.823226</td>\n",
       "      <td>80.143191</td>\n",
       "      <td>55.794067</td>\n",
       "      <td>66.905348</td>\n",
       "      <td>82.830639</td>\n",
       "      <td>97.102932</td>\n",
       "      <td>106.617931</td>\n",
       "      <td>126.398212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ph</td>\n",
       "      <td>7.438949</td>\n",
       "      <td>7.373615</td>\n",
       "      <td>7.376173</td>\n",
       "      <td>7.460001</td>\n",
       "      <td>7.375994</td>\n",
       "      <td>7.370807</td>\n",
       "      <td>7.388508</td>\n",
       "      <td>7.379810</td>\n",
       "      <td>7.353314</td>\n",
       "      <td>7.380083</td>\n",
       "      <td>7.372282</td>\n",
       "      <td>7.372073</td>\n",
       "      <td>7.379048</td>\n",
       "      <td>7.376131</td>\n",
       "      <td>7.897253</td>\n",
       "      <td>7.353929</td>\n",
       "      <td>7.365278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0     General      Female        Male  Undefined Gender  \\\n",
       "0           ALP  120.213540  123.095891  127.046296          0.000000   \n",
       "1           ALT  292.510423  118.436620  281.300001          0.000000   \n",
       "2           AST  566.994569  625.684209  250.518520          0.000000   \n",
       "3       Albumin    2.951079    2.823188    2.935366          0.000000   \n",
       "4           BUN   25.788438   24.924859   29.041096          0.000000   \n",
       "5     Bilirubin    2.770370    2.151613    2.242935          0.000000   \n",
       "6   Cholesterol  157.300000  185.999994  158.230769          0.000000   \n",
       "7    Creatinine    1.492740    1.173219    1.685744          0.000000   \n",
       "8       DiasABP   59.684730   59.403303   60.029592         61.708333   \n",
       "9          FiO2    0.542791    0.532284    0.558764          0.000000   \n",
       "10          GCS   11.416553   11.428689   11.444233         14.250001   \n",
       "11      Glucose  139.706649  139.762917  144.029268        136.000000   \n",
       "12         HCO3   22.998730   23.208913   23.404000         24.500000   \n",
       "13          HCT   30.609207   30.484399   30.731575          0.000000   \n",
       "14           HR   86.695616   87.087076   86.312270         73.187500   \n",
       "15            K    4.136149    4.051436    4.211607          4.600000   \n",
       "16      Lactate    2.767346    2.657636    2.897482          0.000000   \n",
       "17          MAP   80.309701   80.750172   80.104122         76.000000   \n",
       "18     MechVent    1.000000    1.000000    1.000000          0.000000   \n",
       "19           Mg    2.011890    1.972207    2.063425          1.800000   \n",
       "20    NIDiasABP   58.446989   56.857937   59.830912         62.125000   \n",
       "21        NIMAP   77.730321   75.826498   78.288817         79.330002   \n",
       "22     NISysABP  119.478651  119.259779  118.476627         96.500000   \n",
       "23           Na  138.889447  138.992000  138.774266          0.000000   \n",
       "24        PaCO2   40.952782   40.324646   40.779975          0.000000   \n",
       "25         PaO2  148.291760  146.972273  148.144981          0.000000   \n",
       "26    Platelets  190.686709  205.684347  185.039416          0.000000   \n",
       "27     RespRate   19.431791   19.653187   19.083589         19.750000   \n",
       "28         SaO2   96.850858   96.174157   96.756989          0.000000   \n",
       "29       SysABP  120.123152  120.349732  120.887513        100.041666   \n",
       "30         Temp   37.090887   36.992802   37.118545         37.150000   \n",
       "31    TroponinI    8.210000    4.980000    5.533333          0.000000   \n",
       "32    TroponinT    0.871049    1.040645    1.145000          0.000000   \n",
       "33        Urine  116.098967  102.454298  123.367161        140.000000   \n",
       "34          WBC   12.264888   12.421652   13.113277         10.850000   \n",
       "35       Weight   82.736610   73.878010   88.541242         99.566663   \n",
       "36           Ph    7.438949    7.373615    7.376173          7.460001   \n",
       "\n",
       "           +65         -65   ICUType 1   ICUType 2   ICUType 3   ICUType 4  \\\n",
       "0   134.583333  170.847619   89.636364   58.240000  184.601941  121.060607   \n",
       "1   218.868130  402.716417  319.000002  497.315789  396.091955  280.655731   \n",
       "2   239.448055  430.263396  683.920050  298.000001  491.054948  630.363636   \n",
       "3     2.843836    2.936471    3.340000    2.685714    2.804839    2.770213   \n",
       "4    31.607143   22.407408   29.480620   20.337931   33.998246   20.029915   \n",
       "5     1.634444    3.295798    1.168000    2.325000    2.430097    2.844444   \n",
       "6   149.250000  107.500000  171.769229    0.000000  128.250000  147.000000   \n",
       "7     1.372038    1.427404    1.850000    1.224000    1.993464    1.133466   \n",
       "8    56.923331   63.246397   58.338428   55.887238   60.773501   62.190792   \n",
       "9     0.550984    0.544024    0.538217    0.568943    0.556088    0.525560   \n",
       "10   11.528753   11.468112   12.705189   11.646400   11.595745   10.895913   \n",
       "11  147.383561  145.045766  144.234375  129.683760  143.581940  134.226721   \n",
       "12   22.977832   22.573809   23.309734   23.173913   22.318452   23.051502   \n",
       "13   30.842833   30.396875   32.746667   29.430000   31.031781   30.856921   \n",
       "14   83.740056   89.761947   81.564406   85.908674   89.698826   86.933538   \n",
       "15    4.125761    4.130370    4.162143    4.475820    4.133435    4.054821   \n",
       "16    2.640000    2.648445    3.241936    3.061429    3.133816    2.883819   \n",
       "17   78.410036   82.558444   80.472733   75.112284   81.787286   84.901597   \n",
       "18    1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "19    2.067133    1.980535    2.046341    2.154054    2.024653    1.941800   \n",
       "20   54.842938   62.153724   57.206204   52.381789   59.095962   59.487900   \n",
       "21   74.700983   79.444474   74.220484   71.194704   77.844048   79.753401   \n",
       "22  119.293391  120.102985  113.346308  111.447270  119.999088  126.006266   \n",
       "23  139.104215  138.725173  138.344000  138.014493  138.843648  139.369004   \n",
       "24   39.720028   40.965354   39.610236   40.511123   41.836149   39.545340   \n",
       "25  147.657934  154.812469  153.583333  168.760307  122.979042  145.571429   \n",
       "26  189.233951  195.180658  212.898302  163.288357  184.283214  190.287591   \n",
       "27   19.995241   18.735708   19.312523   20.079812   19.813428   18.940394   \n",
       "28   96.751957   96.036082   96.468354   96.947304   91.900000   96.571429   \n",
       "29  120.696313  120.083549  114.836960  114.615236  120.194244  127.635264   \n",
       "30   37.006590   37.181720   36.903745   37.279013   36.768098   37.107401   \n",
       "31   12.925000    1.550000   18.662499   10.180000    2.700000    7.500000   \n",
       "32    0.959605    0.845357    2.433823    0.385556    0.435932    0.457500   \n",
       "33  101.877095  138.273400  136.956401  109.523728  115.988397  122.130270   \n",
       "34   13.706148   12.079178   11.833636   14.028042   12.335628   12.333125   \n",
       "35   77.841884   86.627589   81.840061   86.252985   81.210356   80.823226   \n",
       "36    7.375994    7.370807    7.388508    7.379810    7.353314    7.380083   \n",
       "\n",
       "    Undefined classification  Low Weight  Normal Weight  Overweight  \\\n",
       "0                 111.202248  366.500000     117.838710   98.843750   \n",
       "1                 291.708863  204.200000     157.095240  641.805538   \n",
       "2                 279.442109  889.000068     177.375000  667.714272   \n",
       "3                   2.739744    3.100000       2.636842    3.021053   \n",
       "4                  27.157143   30.850000      27.640288   26.492424   \n",
       "5                   2.758427    8.280000       3.455172    1.586111   \n",
       "6                 170.666665  154.000000     114.499992  206.285712   \n",
       "7                   1.530662    1.014286       1.601408    1.344526   \n",
       "8                  62.040018   58.649750      57.760640   57.972569   \n",
       "9                   0.525345    0.506098       0.537025    0.536240   \n",
       "10                 11.622538   11.093750      11.016667   11.548223   \n",
       "11                138.880096  170.347823     137.561983  138.467213   \n",
       "12                 23.105911   25.750000      22.909091   22.406451   \n",
       "13                 31.031277   30.068750      30.728724   30.329206   \n",
       "14                 86.616159   88.645670      86.162043   85.523481   \n",
       "15                  4.134896    3.956522       4.154839    4.382677   \n",
       "16                  2.618357    4.560000       2.899519    2.985465   \n",
       "17                 84.149771   83.815904      77.193152   78.669410   \n",
       "18                  1.000000    1.000000       1.000000    1.000000   \n",
       "19                  1.951538    2.120000       2.053659    2.013287   \n",
       "20                 59.487704   57.457409      55.284954   56.550337   \n",
       "21                 77.791639   76.076136      74.734905   75.650896   \n",
       "22                121.201091  120.983401     114.405666  115.661460   \n",
       "23                139.334951  135.800000     138.881481  138.065972   \n",
       "24                 41.313559   39.808824      39.832765   40.217234   \n",
       "25                131.058248  194.118421     160.313054  150.281159   \n",
       "26                204.482571  169.136362     172.309350  171.358022   \n",
       "27                 19.781309   21.476190      18.323431   19.308421   \n",
       "28                 95.984848   96.607143      97.062500   96.748031   \n",
       "29                124.791785  124.106029     115.433673  117.777673   \n",
       "30                 36.938830   36.986269      37.065208   37.176218   \n",
       "31                  5.833333    0.000000       8.444445    9.840000   \n",
       "32                  0.601849    2.790000       0.893810    1.563846   \n",
       "33                123.381353  117.989932     105.160684  106.280932   \n",
       "34                 13.085431   12.103846      13.203876   12.234965   \n",
       "35                 80.143191   55.794067      66.905348   82.830639   \n",
       "36                  7.372282    7.372073       7.379048    7.376131   \n",
       "\n",
       "     Obesity 1   Obesity 2    Obesity 3  \n",
       "0   101.571429   63.555556    95.454543  \n",
       "1   249.900001  772.285714  1688.333333  \n",
       "2   388.750004   86.000001    45.250000  \n",
       "3     2.983333    2.750000     2.680000  \n",
       "4    30.306667   32.096774    30.428572  \n",
       "5     2.593333    1.644445     2.216667  \n",
       "6   132.000000  163.999985     0.000000  \n",
       "7     1.363768    1.406667     1.783871  \n",
       "8    59.196723   59.659678    59.396970  \n",
       "9     0.582576    0.560776     0.588828  \n",
       "10   11.416949   11.775000    11.848361  \n",
       "11  131.982454  132.645162   164.941179  \n",
       "12   23.217391   24.850000    21.735294  \n",
       "13   31.136161   31.592105    30.982927  \n",
       "14   86.934679   89.150278    87.773425  \n",
       "15    4.073494    4.123077     4.340000  \n",
       "16    3.297778    2.950000     3.423611  \n",
       "17   79.079877   79.401571    78.636926  \n",
       "18    1.000000    1.000000     1.000000  \n",
       "19    1.964286    1.948148     2.037143  \n",
       "20   58.711497   56.991163    56.819232  \n",
       "21   75.188949   75.818271    75.591905  \n",
       "22  118.645289  113.352273   114.680766  \n",
       "23  137.546875  139.060606   136.391304  \n",
       "24   39.360606   39.861539    41.186047  \n",
       "25  131.040698  150.492308   121.220974  \n",
       "26  193.499997  174.653841   165.968747  \n",
       "27   19.633987   20.693182    19.796296  \n",
       "28   96.778689   96.277778    96.037037  \n",
       "29  120.066041  115.811267   117.612574  \n",
       "30   37.286816   37.181321    37.319737  \n",
       "31    7.600000    0.000000     7.000000  \n",
       "32    1.130000    5.020000     3.886000  \n",
       "33  119.936848  109.442052   108.465589  \n",
       "34   16.095082   13.003226    11.903030  \n",
       "35   97.102932  106.617931   126.398212  \n",
       "36    7.897253    7.353929     7.365278  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_saits_mae_minmax_ori "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "FiO2\n",
      "0.5427910102481897\n",
      "--------------------\n",
      "Female\n",
      "FiO2\n",
      "0.5322841955806464\n",
      "--------------------\n",
      "Male\n",
      "FiO2\n",
      "0.558764148292676\n",
      "--------------------\n",
      "Undefined Gender\n",
      "TroponinT\n",
      "0.0\n",
      "--------------------\n",
      "+65\n",
      "FiO2\n",
      "0.5509840135837525\n",
      "--------------------\n",
      "-65\n",
      "FiO2\n",
      "0.5440240574521423\n",
      "--------------------\n",
      "ICUType 1\n",
      "FiO2\n",
      "0.5382165582316663\n",
      "--------------------\n",
      "ICUType 2\n",
      "Cholesterol\n",
      "0.0\n",
      "--------------------\n",
      "ICUType 3\n",
      "TroponinT\n",
      "0.4359321938985411\n",
      "--------------------\n",
      "ICUType 4\n",
      "TroponinT\n",
      "0.4575000104183762\n",
      "--------------------\n",
      "Undefined classification\n",
      "FiO2\n",
      "0.5253448272903851\n",
      "--------------------\n",
      "Low Weight\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Normal Weight\n",
      "FiO2\n",
      "0.5370249722019451\n",
      "--------------------\n",
      "Overweight\n",
      "FiO2\n",
      "0.5362397804247246\n",
      "--------------------\n",
      "Obesity 1\n",
      "FiO2\n",
      "0.5825757534816982\n",
      "--------------------\n",
      "Obesity 2\n",
      "TroponinI\n",
      "0.0\n",
      "--------------------\n",
      "Obesity 3\n",
      "Cholesterol\n",
      "0.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.min_value_in_subgroup(df_saits_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "AST\n",
      "566.9945693689813\n",
      "--------------------\n",
      "Female\n",
      "AST\n",
      "625.6842090706995\n",
      "--------------------\n",
      "Male\n",
      "ALT\n",
      "281.3000011026836\n",
      "--------------------\n",
      "Undefined Gender\n",
      "Urine\n",
      "139.99999999986\n",
      "--------------------\n",
      "+65\n",
      "AST\n",
      "239.44805549026773\n",
      "--------------------\n",
      "-65\n",
      "AST\n",
      "430.2633963482682\n",
      "--------------------\n",
      "ICUType 1\n",
      "AST\n",
      "683.9200496291841\n",
      "--------------------\n",
      "ICUType 2\n",
      "ALT\n",
      "497.31578862037054\n",
      "--------------------\n",
      "ICUType 3\n",
      "AST\n",
      "491.05494833516013\n",
      "--------------------\n",
      "ICUType 4\n",
      "AST\n",
      "630.3636355139878\n",
      "--------------------\n",
      "Undefined classification\n",
      "ALT\n",
      "291.7088629807061\n",
      "--------------------\n",
      "Low Weight\n",
      "AST\n",
      "889.0000677106542\n",
      "--------------------\n",
      "Normal Weight\n",
      "AST\n",
      "177.37499982118052\n",
      "--------------------\n",
      "Overweight\n",
      "AST\n",
      "667.7142724082266\n",
      "--------------------\n",
      "Obesity 1\n",
      "AST\n",
      "388.75000405309646\n",
      "--------------------\n",
      "Obesity 2\n",
      "ALT\n",
      "772.2857140131256\n",
      "--------------------\n",
      "Obesity 3\n",
      "ALT\n",
      "1688.3333333327705\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "toolkits.max_value_in_subgroup(df_saits_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BRITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BRITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_brits_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_standard = toolkits.create_table(testing_mae_brits_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_standard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_brits_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_brits_mae_standard, subgroups, variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BRITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_brits_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_standard_ori = toolkits.create_table(testing_mae_brits_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_standard_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_brits_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_brits_mae_standard_ori, subgroups, variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> MinMax Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BRITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_brits_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_minmax = toolkits.create_table(testing_mae_brits_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_minmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_brits_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_brits_mae_minmax, subgroups, variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> MinMax Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BRITS - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_brits_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_minmax_ori = toolkits.create_table(testing_mae_brits_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brits_mae_minmax_ori "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_brits_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_brits_mae_minmax_ori, subgroups, variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USGAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"USGAN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_usgan_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_standard = toolkits.create_table(testing_mae_saits_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_standard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_usgan_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_usgan_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"USGAN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_usgan_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_standard_ori = toolkits.create_table(testing_mae_usgan_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_standard_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_usgan_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_usgan_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"USGAN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_usgan_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_minmax = toolkits.create_table(testing_mae_usgan_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_usgan_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_usgan_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"USGAN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_usgan_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_minmax_ori = toolkits.create_table(testing_mae_usgan_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usgan_mae_minmax_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_usgan_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_usgan_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPVAE - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_gpvae_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_standard = toolkits.create_table(testing_mae_gpvae_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_standard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_gpvae_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_gpvae_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPVAE - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_gpvae_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_standard_ori = toolkits.create_table(testing_mae_gpvae_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_standard_ori "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_gpvae_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_gpvae_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPVAE - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_gpvae_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_minmax = toolkits.create_table(testing_mae_gpvae_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_gpvae_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_gpvae_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPVAE - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_gpvae_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_minmax_ori = toolkits.create_table(testing_mae_gpvae_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpvae_mae_minmax_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_gpvae_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_gpvae_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MRNN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_mrnn_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_standard = toolkits.create_table(testing_mae_mrnn_variables_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_standard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_mrnn_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_mrnn_mae_standard, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Standard Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MRNN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_mrnn_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_standard_ori = toolkits.create_table(testing_mae_mrnn_variables_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_standard_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_mrnn_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_mrnn_mae_standard_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (C/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MRNN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_mrnn_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_minmax = toolkits.create_table(testing_mae_mrnn_variables_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_mrnn_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_mrnn_mae_minmax, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>MinMax Scaler (S/Normalização)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MRNN - MAE\")\n",
    "print(\"************\")\n",
    "toolkits.show_mae(testing_mae_mrnn_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_minmax_ori = toolkits.create_table(testing_mae_mrnn_variables_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrnn_mae_minmax_ori "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Minimum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.min_value_in_subgroup(df_mrnn_mae_minmax_ori, subgroups, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Maximum MAE value in each subgroup</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkits.max_value_in_subgroup(df_mrnn_mae_minmax_ori, subgroups, variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
